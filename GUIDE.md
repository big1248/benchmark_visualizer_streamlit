# 📘 LLM 벤치마크 시각화 도구 사용 가이드

## 목차
1. [빠른 시작](#빠른-시작)
2. [주요 기능 상세 설명](#주요-기능-상세-설명)
3. [실전 사용 예시](#실전-사용-예시)
4. [팁과 트릭](#팁과-트릭)
5. [FAQ](#faq)

---

## 빠른 시작

### 1단계: 설치
```bash
# 패키지 설치
pip install -r requirements.txt
```

### 2단계: 데이터 확인
```bash
# 데이터 검증 (선택사항)
python validate_data.py /mnt/project
```

### 3단계: 실행
```bash
# 방법 1
streamlit run llm_benchmark_visualizer.py

# 방법 2
./run.sh
```

### 4단계: 브라우저 열기
자동으로 브라우저가 열리며, 기본 주소는 `http://localhost:8501` 입니다.

---

## 주요 기능 상세 설명

### 📊 1. 전체 요약 탭

#### 주요 메트릭
화면 상단에 4개의 주요 지표가 표시됩니다:
- **총 문제 수**: 현재 필터 조건에 해당하는 전체 문제 수
- **전체 정확도**: 모든 모델의 평균 정확도
- **모델 수**: 분석 중인 모델의 개수
- **테스트 수**: 포함된 테스트의 개수

#### 모델별 정확도 요약 테이블
각 모델의 상세도(detailed/summary)와 프롬프팅 방식별로:
- 정답 수 / 총 문제 수
- 정확도 (%)
- 문제당 평균 처리 시간

**활용법**: 이 테이블을 통해 어떤 모델 설정이 가장 우수한지 한눈에 파악할 수 있습니다.

#### 정확도 비교 막대 그래프
모든 모델 조합의 정확도를 시각적으로 비교합니다.

**팁**: 그래프 위에 마우스를 올리면 정확한 수치를 볼 수 있습니다.

---

### 🏆 2. 모델별 비교 탭

#### 모델×테스트 히트맵
- **색상**: 녹색(높은 정확도) → 노란색(중간) → 빨간색(낮은 정확도)
- **숫자**: 각 셀에 정확도(%) 표시

**활용법**: 
- 특정 모델이 어떤 테스트에 강한지 약한지 파악
- 테스트별 난이도 추정 (모든 모델이 낮은 점수 → 어려운 테스트)

#### 정확도 vs 처리속도 산점도
- **X축**: 문제당 평균 처리 시간 (초)
- **Y축**: 정확도 (%)
- **크기**: 총 문제 수

**해석**:
- 오른쪽 위 = 느리지만 정확
- 왼쪽 위 = 빠르고 정확 (최적!)
- 왼쪽 아래 = 빠르지만 부정확
- 오른쪽 아래 = 느리고 부정확

#### 상세도별 성능 비교
detailed와 summary 방식의 성능 차이를 비교합니다.

**인사이트**: 일반적으로 detailed가 더 정확하지만 시간이 더 걸립니다.

---

### 📚 3. 테스트별 분석 탭

#### 테스트 선택
드롭다운에서 특정 테스트를 선택하면 해당 테스트에 대한 상세 분석이 표시됩니다.

#### 주요 분석 내용
1. **기본 통계**: 총 문제 수, 평균 정확도, 평균 처리 시간
2. **모델별 성능 테이블**: 해당 테스트에서 각 모델의 성능
3. **정확도 막대 그래프**: 모델 간 성능 비교
4. **과목별 성능 추이**: 과목별로 모델 성능 변화 (라인 차트)

**활용법**: 
- "산업안전기사"를 선택하면 해당 시험에서 어떤 모델이 가장 우수한지 확인
- 과목별 성능 추이로 모델의 강점/약점 파악

---

### ⚖️ 4. 법령/비법령 분석 탭

#### 법령/비법령 문제 분포 (파이 차트)
전체 문제 중 법령 문제와 비법령 문제의 비율을 표시합니다.

#### 법령/비법령별 정확도
법령 문제와 비법령 문제의 평균 정확도를 비교합니다.

**인사이트**: 
- 법령 문제가 일반적으로 더 어렵거나 쉬운지 확인
- 모델이 법률 용어 처리에 강한지 약한지 파악

#### 모델별 법령/비법령 성능 비교
각 모델이 법령/비법령 문제를 어떻게 처리하는지 비교합니다.

**활용법**: 법률 관련 업무에 사용할 모델을 선택할 때 참고

#### 테스트별 법령/비법령 성능
각 테스트에서 법령/비법령 문제의 정확도 차이를 확인합니다.

---

### 📖 5. 과목별 분석 탭

#### 과목별 전체 통계 테이블
모든 과목의 성적을 한눈에 볼 수 있습니다:
- 과목명
- 정답 수 / 총 문제 수
- 정확도 (%)
- 평균 처리 시간

#### 과목별 정확도 막대 그래프
과목별 정확도를 색상으로 구분하여 표시합니다.
- 녹색 → 높은 정확도
- 빨간색 → 낮은 정확도

#### 모델×과목 히트맵
모든 모델과 과목의 조합에 대한 성능을 한 화면에 표시합니다.

**활용법**: 특정 과목에 강한 모델을 찾거나, 모든 모델이 어려워하는 과목을 파악

#### 과목별 상세 분석
특정 과목을 선택하면:
1. 해당 과목에서 모델별 정확도
2. 해당 과목의 법령/비법령 문제 정확도

---

## 실전 사용 예시

### 예시 1: 최고 성능 모델 찾기

**목표**: 전체적으로 가장 우수한 모델 조합 찾기

**단계**:
1. 사이드바에서 모든 필터를 기본값(전체 선택)으로 유지
2. "전체 요약" 탭 → "모델별 정확도 요약" 테이블 확인
3. 정확도가 가장 높은 행 확인
4. "모델별 비교" 탭 → "정확도 vs 처리속도" 그래프로 속도도 고려

**결과 해석**: 
- 데이터 검증 결과에 따르면 GPT-4o가 75.18%로 가장 높은 정확도
- Claude-3.5-Sonnet은 67.73%, GPT-4o-Mini는 62.77%

### 예시 2: 법령 문제에 강한 모델 찾기

**목표**: 법률 시험에 최적화된 모델 선택

**단계**:
1. 사이드바에서 "법령 구분" → "법령" 선택
2. "모델별 비교" 탭으로 이동
3. 모델별 성능 지표 확인
4. "법령/비법령 분석" 탭 → "모델별 법령/비법령 성능 비교" 확인

**활용**: 법무 챗봇이나 법률 상담 AI에 사용할 모델 선택

### 예시 3: 특정 과목 전문 모델 찾기

**목표**: "안전관리론" 과목에 가장 강한 모델 찾기

**단계**:
1. "과목별 분석" 탭으로 이동
2. "과목별 상세 분석" 섹션에서 "안전관리론" 선택
3. 모델별 정확도 막대 그래프 확인
4. 필요시 사이드바에서 특정 모델만 선택하여 비교

### 예시 4: 비용 대비 효율 분석

**목표**: 성능과 비용의 균형점 찾기

**단계**:
1. "전체 요약" 탭 → "모델별 정확도 요약" 테이블 확인
2. 정확도와 비용수준 함께 고려
3. "모델별 비교" 탭 → "정확도 vs 처리속도" 그래프 확인
4. 빠르면서도 정확도가 충분한 모델 선택

**예**: 
- GPT-4o-Mini: 중간 정확도(62.77%), 낮은 비용
- Claude-3.5-Haiku: 중간 정확도(60.28%), 낮은 비용
- 대량 처리가 필요하면 Mini 모델 고려

### 예시 5: 테스트 난이도 분석

**목표**: 어떤 테스트가 가장 어려운지 파악

**단계**:
1. "모델별 비교" 탭 → "모델별 테스트별 정확도" 히트맵 확인
2. 모든 모델이 낮은 점수를 받은 테스트 식별
3. "테스트별 분석" 탭에서 해당 테스트 선택
4. 과목별 성능 추이로 어떤 과목이 어려운지 확인

---

## 팁과 트릭

### 💡 팁 1: 필터 조합 활용
여러 필터를 동시에 사용하면 더 구체적인 분석이 가능합니다.

**예시**:
- 테스트: "산업안전기사"
- 모델: "GPT-4o", "Claude-3.5-Sonnet"
- 법령 구분: "법령"
- → 산업안전기사의 법령 문제에서 두 모델 비교

### 💡 팁 2: 그래프 인터랙션
모든 Plotly 그래프는 인터랙티브합니다:
- **확대**: 드래그하여 영역 선택
- **축소**: 더블 클릭
- **패닝**: Shift + 드래그
- **정보**: 마우스 오버로 상세 정보 표시
- **저장**: 카메라 아이콘으로 이미지 저장

### 💡 팁 3: 데이터 정렬
테이블의 컬럼 헤더를 클릭하면 해당 컬럼으로 정렬됩니다.

### 💡 팁 4: 빠른 비교
한 번에 여러 탭을 열어두고 비교하고 싶다면, 브라우저에서 여러 윈도우를 열고 다른 필터 설정으로 비교할 수 있습니다.

### 💡 팁 5: 보고서 작성
차트를 이미지로 저장하여 보고서나 프레젠테이션에 활용할 수 있습니다.

---

## FAQ

### Q1: 데이터가 표시되지 않아요!
**A**: 
1. 사이드바의 데이터 디렉토리 경로 확인
2. CSV 파일명이 규칙에 맞는지 확인 (`testset_*.csv`, `*_detailed_*.csv`, `*_summary_*.csv`)
3. `python validate_data.py [경로]` 실행하여 데이터 확인

### Q2: 일부 차트가 비어있어요!
**A**: 
- 필터 설정으로 모든 데이터가 제외되었을 수 있습니다
- 해당 데이터에 필요한 컬럼(예: `Subject`, `law`)이 없을 수 있습니다
- 필터를 "전체"로 재설정해보세요

### Q3: 속도가 느려요!
**A**: 
- 첫 로딩 시 모든 CSV 파일을 읽기 때문에 시간이 걸릴 수 있습니다
- 이후에는 Streamlit의 캐싱으로 빠르게 동작합니다
- 데이터가 매우 큰 경우, 필터를 사용하여 분석 범위를 줄이세요

### Q4: 새로운 데이터를 추가했는데 반영이 안돼요!
**A**: 
- 브라우저에서 `R` 키를 눌러 페이지 새로고침
- 또는 우측 상단 메뉴 → "Rerun" 클릭
- 또는 `Ctrl + R` (Windows/Linux) / `Cmd + R` (Mac)

### Q5: 여러 사람이 동시에 사용할 수 있나요?
**A**: 
- 네, 가능합니다!
- 서버를 외부에 공개하려면: `streamlit run llm_benchmark_visualizer.py --server.address=0.0.0.0`
- 각 사용자는 독립적인 세션을 가집니다

### Q6: 데이터를 엑셀로 내보낼 수 있나요?
**A**: 
- 테이블 위에 마우스를 올리면 다운로드 옵션이 표시됩니다
- CSV 형식으로 다운로드 가능
- 엑셀로 변환: pandas를 사용하여 별도 스크립트 작성 가능

### Q7: 커스텀 필터를 추가하고 싶어요!
**A**: 
- `llm_benchmark_visualizer.py` 파일을 수정하세요
- 사이드바 섹션에 `st.sidebar.multiselect()` 또는 `st.sidebar.selectbox()` 추가
- 필터링 로직은 `filtered_df` 생성 부분에 추가

### Q8: 다크 모드를 사용하고 싶어요!
**A**: 
- 우측 상단 메뉴 (⋮) → Settings → Theme → Dark
- 또는 시스템 테마 사용 선택

### Q9: 모바일에서도 볼 수 있나요?
**A**: 
- 네! Streamlit은 반응형 디자인을 지원합니다
- 모바일 브라우저에서 접속 가능
- 다만 큰 화면에서 더 편리합니다

### Q10: 오류가 발생했어요!
**A**: 
1. 터미널의 에러 메시지 확인
2. 데이터 파일 인코딩 확인 (UTF-8 또는 CP949)
3. 필수 컬럼이 있는지 확인
4. Python과 패키지 버전 확인
5. `validate_data.py`로 데이터 검증

---

## 추가 개선 아이디어

이 도구를 더 발전시키고 싶다면:

1. **통계적 유의성 검정**: 모델 간 성능 차이가 통계적으로 유의한지 검증
2. **시계열 분석**: 연도별, 세션별 성능 변화 추이
3. **비용 분석**: 토큰 사용량과 비용 상세 분석
4. **오답 분석**: 어떤 유형의 문제를 자주 틀리는지 분석
5. **앙상블 분석**: 여러 모델의 답을 조합했을 때의 성능
6. **A/B 테스트**: 프롬프팅 방식 비교
7. **실시간 업데이트**: 새로운 결과 파일 자동 감지 및 반영
8. **리포트 자동 생성**: PDF 또는 PowerPoint 보고서 자동 작성

---

## 문의 및 지원

문제가 발생하거나 개선 제안이 있으시면:
- GitHub Issues에 등록
- 또는 `validate_data.py`를 실행하여 데이터 상태 먼저 확인

**Happy Analyzing! 📊✨**
