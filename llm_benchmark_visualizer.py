import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import glob
from pathlib import Path
import numpy as np
from scipy import stats
import io
import requests
import zipfile
import gc
import re
from collections import Counter
from functools import lru_cache

@st.cache_data(ttl=86400)
def download_data_from_github():
    """ì €ì¥ì†Œì— í¬í•¨ëœ data í´ë” ì‚¬ìš©"""
    
    data_dir = Path('./data')
    
    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
    if data_dir.exists() and next(data_dir.glob('*.csv'), None) is not None:
        csv_count = sum(1 for _ in data_dir.glob('*.csv'))
        return
    
    # data í´ë”ê°€ ì—†ê±°ë‚˜ CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
    st.error("âŒ data í´ë” ë˜ëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.error("ì €ì¥ì†Œì— data í´ë”ì™€ CSV íŒŒì¼ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LLM ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™”",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë‹¤êµ­ì–´ ì§€ì› ì„¤ì •
LANGUAGES = {
    'ko': {
        'title': 'LLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹œê°í™” ë„êµ¬',
        'data_dir': 'ë°ì´í„° ë””ë ‰í† ë¦¬',
        'filters': 'í•„í„° ì˜µì…˜',
        'testname': 'í…ŒìŠ¤íŠ¸ëª…',
        'all': 'ì „ì²´',
        'model': 'ëª¨ë¸',
        'detail_type': 'ìƒì„¸ë„',
        'prompting': 'í”„ë¡¬í”„íŒ… ë°©ì‹',
        'session': 'ì„¸ì…˜',
        'problem_type': 'ë¬¸ì œ ìœ í˜•',
        'image_problem': 'ì´ë¯¸ì§€ ë¬¸ì œ',
        'text_only': 'í…ìŠ¤íŠ¸ë§Œ',
        'year': 'ì—°ë„',
        'law_type': 'ë²•ë ¹ êµ¬ë¶„',
        'law': 'ë²•ë ¹',
        'non_law': 'ë¹„ë²•ë ¹',
        'overview': 'ì „ì²´ ìš”ì•½',
        'model_comparison': 'ëª¨ë¸ë³„ ë¹„êµ',
        'response_time_analysis': 'ì‘ë‹µì‹œê°„ ë¶„ì„',
        'law_analysis': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„',
        'subject_analysis': 'ê³¼ëª©ë³„ ë¶„ì„',
        'year_analysis': 'ì—°ë„ë³„ ë¶„ì„',
        'incorrect_analysis': 'ì˜¤ë‹µ ë¶„ì„',
        'difficulty_analysis': 'ë‚œì´ë„ ë¶„ì„',
        'ensemble': 'ì•™ìƒë¸”',
        'ensemble_management': 'ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬',
        'create_ensemble': 'ì•™ìƒë¸” ìƒì„±',
        'ensemble_name': 'ì•™ìƒë¸” ì´ë¦„',
        'ensemble_method': 'ì•™ìƒë¸” ë°©ë²•',
        'majority_voting': 'ë‹¤ìˆ˜ê²° íˆ¬í‘œ',
        'weighted_voting': 'ê°€ì¤‘ íˆ¬í‘œ',
        'select_models': 'ëª¨ë¸ ì„ íƒ',
        'add_ensemble': 'ì•™ìƒë¸” ì¶”ê°€',
        'remove_ensemble': 'ì‚­ì œ',
        'current_ensembles': 'í˜„ì¬ ì•™ìƒë¸” ëª©ë¡',
        'no_ensembles': 'ìƒì„±ëœ ì•™ìƒë¸”ì´ ì—†ìŠµë‹ˆë‹¤',
        'ensemble_added': 'ì•™ìƒë¸”ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤',
        'ensemble_removed': 'ì•™ìƒë¸”ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤',
        'min_2_models': 'ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤',
        'testset_stats': 'í…ŒìŠ¤íŠ¸ì…‹ í†µê³„',
        'total_problems': 'ì´ ë¬¸ì œ ìˆ˜',
        'accuracy': 'ì •í™•ë„',
        'correct': 'ì •ë‹µ',
        'wrong': 'ì˜¤ë‹µ',
        'law_problems': 'ë²•ë ¹ ë¬¸ì œ',
        'non_law_problems': 'ë¹„ë²•ë ¹ ë¬¸ì œ',
        'correct_rate': 'ì •ë‹µë¥ ',
        'wrong_rate': 'ì˜¤ë‹µë¥ ',
        'performance_by_model': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ',
        'comparison_chart': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸',
        'overall_comparison': 'ì „ì²´ í…ŒìŠ¤íŠ¸ ë¹„êµ',
        'heatmap': 'ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ì…‹ ì •ë‹µë„ íˆíŠ¸ë§µ',
        'law_ratio': 'ë²•ë ¹/ë¹„ë²•ë ¹ ì „ì²´ í†µê³„',
        'model_law_performance': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥ ë¹„êµ',
        'law_distribution': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë„',
        'subject_performance': 'ê³¼ëª©ë³„ ì„±ëŠ¥',
        'year_performance': 'ì—°ë„ë³„ ì„±ëŠ¥',
        'top_incorrect': 'ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 20',
        'all_models_incorrect': 'ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ',
        'most_models_incorrect': 'ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (â‰¥50%)',
        'test_info': 'í…ŒìŠ¤íŠ¸',
        'problem_id': 'ë¬¸ì œë²ˆí˜¸',
        'incorrect_count': 'ì˜¤ë‹µ ëª¨ë¸ìˆ˜',
        'correct_count': 'ì •ë‹µ ëª¨ë¸ìˆ˜',
        'total_models': 'ì´ ëª¨ë¸ìˆ˜',
        'attempted_models': 'ì‹œë„í•œ ëª¨ë¸',
        'question': 'ë¬¸ì œ',
        'difficulty_score': 'ë‚œì´ë„ ì ìˆ˜',
        'by_session': 'ì„¸ì…˜ë³„',
        'by_subject': 'ê³¼ëª©ë³„',
        'by_year': 'ì—°ë„ë³„',
        'problem_count': 'ë¬¸ì œ ìˆ˜',
        'session_distribution': 'ì„¸ì…˜ë³„ ë¬¸ì œ ë¶„í¬',
        'subject_distribution': 'ê³¼ëª©ë³„ ë¬¸ì œ ë¶„í¬',
        'year_distribution': 'ì—°ë„ë³„ ë¬¸ì œ ë¶„í¬',
        'law_distribution_stat': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ë¶„í¬',
        'basic_stats': 'ê¸°ë³¸ í†µê³„',
        'help': 'ë„ì›€ë§',
        'new_features': 'ìƒˆë¡œìš´ ê¸°ëŠ¥',
        'existing_features': 'ê¸°ì¡´ ê¸°ëŠ¥',
        'current_data': 'í˜„ì¬ í‘œì‹œ ì¤‘ì¸ ë°ì´í„°',
        'problems': 'ê°œ ë¬¸ì œ',
        'session_filter': 'íŠ¹ì • ì„¸ì…˜ì˜ ê²°ê³¼ë§Œ ë¶„ì„',
        'incorrect_pattern': 'ì–´ë ¤ìš´ ë¬¸ì œì™€ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„',
        'difficulty_comparison': 'ë¬¸ì œ ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
        'problem_type_filter': 'ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¬¸ì œ êµ¬ë¶„',
        'basic_filters': 'í…ŒìŠ¤íŠ¸ëª…, ëª¨ë¸, ìƒì„¸ë„, í”„ë¡¬í”„íŒ… ë°©ì‹ìœ¼ë¡œ í•„í„°ë§',
        'law_analysis_desc': 'ë²•ë ¹/ë¹„ë²•ë ¹ êµ¬ë¶„ ë¶„ì„',
        'detail_analysis': 'ê³¼ëª©ë³„, ì—°ë„ë³„ ìƒì„¸ ë¶„ì„',
        'font_size': 'í™”ë©´ í°íŠ¸ í¬ê¸°',
        'chart_text_size': 'ì°¨íŠ¸ í…ìŠ¤íŠ¸ í¬ê¸°',
        'year_problem_distribution': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ë¶„í¬',
        'problem_count_table': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ í…Œì´ë¸”',
        'year_problem_chart': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜',
        'total_problem_count': 'ì´ ë¬¸ì œ ìˆ˜',
        'correct_models': 'ì •ë‹µ ëª¨ë¸',
        'incorrect_models': 'ì˜¤ë‹µ ëª¨ë¸',
        'avg_accuracy_by_model': 'ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„',
        'difficulty_range': 'ë‚œì´ë„ êµ¬ê°„',
        'avg_difficulty': 'í‰ê·  ë‚œì´ë„',
        'difficulty_stats_by_range': 'ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í†µê³„',
        'very_hard': 'ë§¤ìš° ì–´ë ¤ì›€',
        'hard': 'ì–´ë ¤ì›€',
        'medium': 'ë³´í†µ',
        'easy': 'ì‰¬ì›€',
        'very_easy': 'ë§¤ìš° ì‰¬ìš´',
        'problem_distribution': 'ë¬¸ì œ ë¶„í¬',
        'response_time': 'ì‘ë‹µ ì‹œê°„',
        'avg_response_time': 'í‰ê·  ì‘ë‹µ ì‹œê°„',
        'response_time_distribution': 'ì‘ë‹µ ì‹œê°„ ë¶„í¬',
        'response_time_by_model': 'ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„',
        'response_time_stats': 'ì‘ë‹µ ì‹œê°„ í†µê³„',
        'fastest_model': 'ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸',
        'slowest_model': 'ê°€ì¥ ëŠë¦° ëª¨ë¸',
        'response_time_vs_accuracy': 'ì‘ë‹µ ì‹œê°„ vs ì •í™•ë„',
        'time_per_problem': 'ë¬¸ì œë‹¹ ì‹œê°„',
        'total_time': 'ì´ ì†Œìš” ì‹œê°„',
        'seconds': 'ì´ˆ',
        'minutes': 'ë¶„',
        # í† í° ë° ë¹„ìš© ê´€ë ¨
        'token_cost_analysis': 'í† í° ë° ë¹„ìš© ë¶„ì„',
        'token_usage': 'í† í° ì‚¬ìš©ëŸ‰',
        'input_tokens': 'ì…ë ¥ í† í°',
        'output_tokens': 'ì¶œë ¥ í† í°',
        'total_tokens': 'ì´ í† í°',
        'avg_tokens_per_problem': 'ë¬¸ì œë‹¹ í‰ê·  í† í°',
        'token_distribution': 'í† í° ë¶„í¬',
        'token_efficiency': 'í† í° íš¨ìœ¨ì„±',
        'token_stats': 'í† í° í†µê³„',
        'io_ratio': 'ì…ì¶œë ¥ í† í° ë¹„ìœ¨',
        'token_per_correct': 'ì •ë‹µë‹¹ í† í°',
        'tokens': 'í† í°',
        'cost_level': 'ë¹„ìš© ìˆ˜ì¤€',
        'cost_analysis': 'ë¹„ìš© ë¶„ì„',
        'cost_per_problem': 'ë¬¸ì œë‹¹ ë¹„ìš©',
        'total_cost_estimate': 'ì´ ì˜ˆìƒ ë¹„ìš©',
        'cost_vs_accuracy': 'ë¹„ìš© vs ì •í™•ë„',
        'cost_efficiency': 'ë¹„ìš© íš¨ìœ¨ì„±',
        'most_efficient': 'ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸',
        'least_efficient': 'ê°€ì¥ ë¹„íš¨ìœ¨ì ì¸ ëª¨ë¸',
        'cost_stats': 'ë¹„ìš© í†µê³„',
        'high': 'ë†’ìŒ',
        'medium_cost': 'ì¤‘ê°„',
        'low': 'ë‚®ìŒ',
        'very_low': 'ë§¤ìš°ë‚®ìŒ',
        'free': 'ë¬´ë£Œ',
        'cost': 'ë¹„ìš©',
        'actual_cost': 'ì‹¤ì œ ë¹„ìš©',
        'estimated_cost': 'ì˜ˆìƒ ë¹„ìš©',
        'cost_per_1k_tokens': '1K í† í°ë‹¹ ë¹„ìš©',
        'total_estimated_cost': 'ì´ ì˜ˆìƒ ë¹„ìš©',
        'usd': 'ë‹¬ëŸ¬',
    },
    'en': {
        'title': 'LLM Benchmark Results Visualization Tool',
        'data_dir': 'Data Directory',
        'filters': 'Filter Options',
        'testname': 'Test Name',
        'all': 'All',
        'model': 'Model',
        'detail_type': 'Detail Type',
        'prompting': 'Prompting Method',
        'session': 'Session',
        'problem_type': 'Problem Type',
        'image_problem': 'Image Problem',
        'text_only': 'Text Only',
        'year': 'Year',
        'law_type': 'Law Type',
        'law': 'Law',
        'non_law': 'Non-Law',
        'overview': 'Overview',
        'model_comparison': 'Model Comparison',
        'response_time_analysis': 'Response Time Analysis',
        'law_analysis': 'Law/Non-Law Analysis',
        'subject_analysis': 'Subject Analysis',
        'year_analysis': 'Year Analysis',
        'incorrect_analysis': 'Incorrect Answer Analysis',
        'difficulty_analysis': 'Difficulty Analysis',
        'ensemble': 'Ensemble',
        'ensemble_management': 'Ensemble Model Management',
        'create_ensemble': 'Create Ensemble',
        'ensemble_name': 'Ensemble Name',
        'ensemble_method': 'Ensemble Method',
        'majority_voting': 'Majority Voting',
        'weighted_voting': 'Weighted Voting',
        'select_models': 'Select Models',
        'add_ensemble': 'Add Ensemble',
        'remove_ensemble': 'Remove',
        'current_ensembles': 'Current Ensembles',
        'no_ensembles': 'No ensembles created',
        'ensemble_added': 'Ensemble added successfully',
        'ensemble_removed': 'Ensemble removed',
        'min_2_models': 'Please select at least 2 models',
        'testset_stats': 'Test Set Statistics',
        'total_problems': 'Total Problems',
        'accuracy': 'Accuracy',
        'correct': 'Correct',
        'wrong': 'Wrong',
        'law_problems': 'Law Problems',
        'non_law_problems': 'Non-Law Problems',
        'correct_rate': 'Correct Rate',
        'wrong_rate': 'Wrong Rate',
        'performance_by_model': 'Performance Metrics by Model',
        'comparison_chart': 'Model Performance Comparison Chart',
        'overall_comparison': 'Overall Test Comparison',
        'heatmap': 'Model Ã— Test Set Accuracy Heatmap',
        'law_ratio': 'Law/Non-Law Overall Statistics',
        'model_law_performance': 'Model Law/Non-Law Performance Comparison',
        'law_distribution': 'Law/Non-Law Accuracy by Model',
        'subject_performance': 'Performance by Subject',
        'year_performance': 'Performance by Year',
        'top_incorrect': 'Top 20 Problems with Highest Incorrect Rate',
        'all_models_incorrect': 'Problems All Models Got Wrong',
        'most_models_incorrect': 'Problems Most Models Got Wrong (â‰¥50%)',
        'test_info': 'Test',
        'problem_id': 'Problem ID',
        'incorrect_count': 'Incorrect Models',
        'correct_count': 'Correct Models',
        'total_models': 'Total Models',
        'attempted_models': 'Attempted Models',
        'question': 'Question',
        'difficulty_score': 'Difficulty Score',
        'by_session': 'By Session',
        'by_subject': 'By Subject',
        'by_year': 'By Year',
        'problem_count': 'Problem Count',
        'session_distribution': 'Problem Distribution by Session',
        'subject_distribution': 'Problem Distribution by Subject',
        'year_distribution': 'Problem Distribution by Year',
        'law_distribution_stat': 'Law/Non-Law Problem Distribution',
        'basic_stats': 'Basic Statistics',
        'help': 'Help',
        'new_features': 'New Features',
        'existing_features': 'Existing Features',
        'current_data': 'Currently Displayed Data',
        'problems': ' problems',
        'session_filter': 'Analyze specific session results only',
        'incorrect_pattern': 'Analyze difficult problems and incorrect patterns',
        'difficulty_comparison': 'Compare model performance by problem difficulty',
        'problem_type_filter': 'Distinguish image/text problems',
        'basic_filters': 'Filter by test name, model, detail type, prompting method',
        'law_analysis_desc': 'Analyze law/non-law distinction',
        'detail_analysis': 'Detailed analysis by subject and year',
        'font_size': 'Screen Font Size',
        'chart_text_size': 'Chart Text Size',
        'year_problem_distribution': 'Problem Distribution by Year',
        'problem_count_table': 'Problem Count by Year',
        'year_problem_chart': 'Problems by Year',
        'total_problem_count': 'Total Problems',
        'correct_models': 'Correct Models',
        'incorrect_models': 'Incorrect Models',
        'avg_accuracy_by_model': 'Average Accuracy by Model',
        'difficulty_range': 'Difficulty Range',
        'avg_difficulty': 'Average Difficulty',
        'difficulty_stats_by_range': 'Detailed Statistics by Difficulty Range',
        'very_hard': 'Very Hard',
        'hard': 'Hard',
        'medium': 'Medium',
        'easy': 'Easy',
        'very_easy': 'Very Easy',
        'problem_distribution': 'Problem Distribution',
        'response_time': 'Response Time',
        'avg_response_time': 'Average Response Time',
        'response_time_distribution': 'Response Time Distribution',
        'response_time_by_model': 'Response Time by Model',
        'response_time_stats': 'Response Time Statistics',
        'fastest_model': 'Fastest Model',
        'slowest_model': 'Slowest Model',
        'response_time_vs_accuracy': 'Response Time vs Accuracy',
        'time_per_problem': 'Time per Problem',
        'total_time': 'Total Time',
        'seconds': 'seconds',
        'minutes': 'minutes',
        # Token & Cost related
        'token_cost_analysis': 'Token & Cost Analysis',
        'token_usage': 'Token Usage',
        'input_tokens': 'Input Tokens',
        'output_tokens': 'Output Tokens',
        'total_tokens': 'Total Tokens',
        'avg_tokens_per_problem': 'Avg Tokens per Problem',
        'token_distribution': 'Token Distribution',
        'token_efficiency': 'Token Efficiency',
        'token_stats': 'Token Statistics',
        'io_ratio': 'Input/Output Token Ratio',
        'token_per_correct': 'Tokens per Correct Answer',
        'tokens': 'tokens',
        'cost_level': 'Cost Level',
        'cost_analysis': 'Cost Analysis',
        'cost_per_problem': 'Cost per Problem',
        'total_cost_estimate': 'Total Cost Estimate',
        'cost_vs_accuracy': 'Cost vs Accuracy',
        'cost_efficiency': 'Cost Efficiency',
        'most_efficient': 'Most Efficient Model',
        'least_efficient': 'Least Efficient Model',
        'cost_stats': 'Cost Statistics',
        'high': 'High',
        'medium_cost': 'Medium',
        'low': 'Low',
        'very_low': 'Very Low',
        'free': 'Free',
        'cost': 'cost',
        'actual_cost': 'Actual Cost',
        'estimated_cost': 'Estimated Cost',
        'cost_per_1k_tokens': 'Cost per 1K Tokens',
        'total_estimated_cost': 'Total Estimated Cost',
        'usd': 'USD',
    }
}

# ì»¤ìŠ¤í…€ CSS - í°íŠ¸ í¬ê¸° ë° ë ˆì´ì•„ì›ƒ ì¡°ì •
def apply_custom_css(font_size_multiplier=1.0):
    base_font = int(16 * font_size_multiplier)
    metric_value = int(32 * font_size_multiplier)
    metric_label = int(18 * font_size_multiplier)
    h1_size = f"{3 * font_size_multiplier}rem"
    h2_size = f"{2.2 * font_size_multiplier}rem"
    h3_size = f"{1.8 * font_size_multiplier}rem"
    
    st.markdown(f"""
    <style>
        /* ì „ì²´ í°íŠ¸ í¬ê¸° ì¦ê°€ */
        html, body, [class*="css"] {{
            font-size: {base_font}px;
        }}
        
        /* ë©”íŠ¸ë¦­ ì¹´ë“œ í°íŠ¸ í¬ê¸° */
        [data-testid="stMetricValue"] {{
            font-size: {metric_value}px !important;
        }}
        
        [data-testid="stMetricLabel"] {{
            font-size: {metric_label}px !important;
        }}
        
        /* í—¤ë” í°íŠ¸ í¬ê¸° */
        h1 {{
            font-size: {h1_size} !important;
            font-weight: 700 !important;
        }}
        
        h2 {{
            font-size: {h2_size} !important;
            font-weight: 600 !important;
            margin-top: 1.5rem !important;
        }}
        
        h3 {{
            font-size: {h3_size} !important;
            font-weight: 600 !important;
        }}
        
        /* í…Œì´ë¸” í°íŠ¸ í¬ê¸° */
        .dataframe {{
            font-size: {int(16 * font_size_multiplier)}px !important;
        }}
        
        .dataframe th {{
            font-size: {int(16 * font_size_multiplier)}px !important;
            font-weight: 600 !important;
        }}
        
        .dataframe td {{
            font-size: {int(16 * font_size_multiplier)}px !important;
        }}
        
        /* ì‚¬ì´ë“œë°” í°íŠ¸ í¬ê¸° */
        .css-1d391kg, [data-testid="stSidebar"] {{
            font-size: {int(15 * font_size_multiplier)}px !important;
        }}
        
        /* íƒ­ í°íŠ¸ í¬ê¸° */
        .stTabs [data-baseweb="tab-list"] button {{
            font-size: {int(18 * font_size_multiplier)}px !important;
            padding: 12px 20px !important;
        }}
        
        /* ë²„íŠ¼ í°íŠ¸ í¬ê¸° */
        .stButton>button {{
            font-size: {base_font}px !important;
            padding: 0.5rem 1rem !important;
        }}
        
        /* ì…€ë ‰íŠ¸ë°•ìŠ¤ í°íŠ¸ í¬ê¸° */
        .stSelectbox label, .stMultiSelect label {{
            font-size: {base_font}px !important;
            font-weight: 600 !important;
        }}
        
        /* ì°¨íŠ¸ ì—¬ë°± ì¡°ì • */
        .js-plotly-plot {{
            margin: 1rem 0 !important;
        }}
        
        /* ì»¨í…Œì´ë„ˆ íŒ¨ë”© */
        .block-container {{
            padding-top: 2rem !important;
            padding-bottom: 2rem !important;
        }}
    </style>
    """, unsafe_allow_html=True)

# Plotly ì°¨íŠ¸ ê¸€ë¡œë²Œ í°íŠ¸ í¬ê¸° ì„¤ì • (ìºì‹±ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ì¬ìƒì„± ë°©ì§€)
_plotly_template_cache = {}

def set_plotly_font_size(chart_text_multiplier=1.0):
    """ëª¨ë“  Plotly ì°¨íŠ¸ì— ì ìš©ë  ê¸°ë³¸ í°íŠ¸ í¬ê¸° ì„¤ì • (ìºì‹± ì ìš©)"""
    import plotly.io as pio
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = round(chart_text_multiplier, 2)
    
    # ì´ë¯¸ ë™ì¼í•œ ì„¤ì •ì´ ì ìš©ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if _plotly_template_cache.get('last_multiplier') == cache_key:
        return int(12 * chart_text_multiplier)
    
    # ê¸°ë³¸ í°íŠ¸ í¬ê¸° ê³„ì‚°
    title_size = int(20 * chart_text_multiplier)
    axis_size = int(14 * chart_text_multiplier)
    tick_size = int(12 * chart_text_multiplier)
    legend_size = int(12 * chart_text_multiplier)
    
    # plotly ê¸°ë³¸ í…œí”Œë¦¿ ë³µì‚¬
    pio.templates["custom"] = pio.templates["plotly"]
    
    # ì „ì—­ í°íŠ¸ í¬ê¸° ì„¤ì •
    pio.templates["custom"].layout.font.size = axis_size
    pio.templates["custom"].layout.title.font.size = title_size
    
    # ì¶• í°íŠ¸ ì„¤ì •
    pio.templates["custom"].layout.xaxis.tickfont.size = tick_size
    pio.templates["custom"].layout.xaxis.title.font.size = axis_size
    pio.templates["custom"].layout.yaxis.tickfont.size = tick_size
    pio.templates["custom"].layout.yaxis.title.font.size = axis_size
    
    # ë²”ë¡€ í°íŠ¸ ì„¤ì •
    pio.templates["custom"].layout.legend.font.size = legend_size
    
    # ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ì„¤ì •
    pio.templates.default = "custom"
    
    # ìºì‹œ ì—…ë°ì´íŠ¸
    _plotly_template_cache['last_multiplier'] = cache_key
    
    return int(12 * chart_text_multiplier)  # íˆíŠ¸ë§µìš© í¬ê¸° ë°˜í™˜

# ì•ˆì „í•œ ì •ë ¬ í•¨ìˆ˜ (íƒ€ì… í˜¼í•© ëŒ€ì‘)
def safe_sort(values):
    """ë¬¸ìì—´ê³¼ ìˆ«ìê°€ ì„ì—¬ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ì •ë ¬"""
    try:
        # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(values, key=lambda x: (isinstance(x, str), x))
    except:
        # ì‹¤íŒ¨í•˜ë©´ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
        return sorted(values, key=str)

# ========== Excel/CSV ë‹¤ìš´ë¡œë“œ í—¬í¼ í•¨ìˆ˜ ==========

def create_download_button(df, filename, button_text="ğŸ“¥ Excelë¡œ ë‹¤ìš´ë¡œë“œ"):
    """ë°ì´í„°í”„ë ˆì„ì„ Excel íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë²„íŠ¼ ìƒì„±"""
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Data')
    
    st.download_button(
        label=button_text,
        data=buffer.getvalue(),
        file_name=filename,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

def create_csv_download_button(df, filename, button_text="ğŸ“„ CSVë¡œ ë‹¤ìš´ë¡œë“œ"):
    """ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë²„íŠ¼ ìƒì„±"""
    csv = df.to_csv(index=False).encode('utf-8-sig')  # BOM ì¶”ê°€ë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€
    st.download_button(
        label=button_text,
        data=csv,
        file_name=filename,
        mime="text/csv"
    )

def create_copy_button(df, button_text="ğŸ“‹ í´ë¦½ë³´ë“œë¡œ ë³µì‚¬", key_suffix=""):
    """ë°ì´í„°í”„ë ˆì„ì„ í´ë¦½ë³´ë“œë¡œ ë³µì‚¬í•˜ëŠ” ë²„íŠ¼ ìƒì„± - í—¤ë” í¬í•¨"""
    # í—¤ë” í¬í•¨í•œ TSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    tsv_data = df.to_csv(index=False, sep='\t')
    
    # ê³ ìœ  key ìƒì„± (ì¤‘ë³µ ë°©ì§€)
    import hashlib
    data_hash = hashlib.md5(tsv_data.encode()).hexdigest()[:8]
    unique_key = f"tsv_download_{data_hash}_{key_suffix}"
    
    # 2ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„í• : ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ + ë³µì‚¬ ê°€ì´ë“œ
    col_a, col_b = st.columns([1, 2])
    
    with col_a:
        # TSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.download_button(
            label=button_text,
            data=tsv_data,
            file_name="data_with_headers.tsv",
            mime="text/tab-separated-values",
            key=unique_key,
            help="Excelì—ì„œ ì—´ë©´ í—¤ë” í¬í•¨"
        )
    
    with col_b:
        # í—¤ë” í¬í•¨ ë³µì‚¬ ê°€ì´ë“œ
        st.caption("ğŸ’¡ í—¤ë” í¬í•¨ ë³µì‚¬: í‘œ ì¢Œì¸¡ ìƒë‹¨ ğŸ“‹ í´ë¦­")

def display_table_with_download(df, title, excel_filename, lang='ko'):
    """í‘œë¥¼ í‘œì‹œí•˜ê³  ë‹¤ìš´ë¡œë“œ/ë³µì‚¬ ë²„íŠ¼ì„ í•¨ê»˜ ì œê³µ"""
    if title:
        st.markdown(f"### {title}")
    
    # ê³ ìœ  key ìƒì„±ìš© suffix (íŒŒì¼ëª… ê¸°ë°˜)
    key_suffix = excel_filename.replace('.xlsx', '').replace('.csv', '')
    
    col1, col2, col3 = st.columns([1, 1, 3])
    with col1:
        create_download_button(df, excel_filename)
    with col2:
        create_csv_download_button(df, excel_filename.replace('.xlsx', '.csv'))
    with col3:
        create_copy_button(df, "ğŸ“‹ " + ("TSV ë‹¤ìš´ë¡œë“œ" if lang == 'ko' else "Download TSV"), key_suffix)
    
    st.dataframe(df, width='stretch')
    st.markdown("---")

# ========== ëª¨ë¸ ì •ë³´ ì¶”ì • í•¨ìˆ˜ ==========

def calculate_model_release_date(model_name):
    """ëª¨ë¸ëª…ìœ¼ë¡œë¶€í„° ëŒ€ëµì ì¸ ì¶œì‹œ ì‹œê¸° ì¶”ì •"""
    release_dates = {
        'GPT-4o': '2024-05',
        'GPT-4o-Mini': '2024-07',
        'GPT-4-Turbo': '2024-04',
        'GPT-3.5-Turbo': '2023-03',
        'Claude-Sonnet-4.5': '2024-10',
        'Claude-Sonnet-4': '2024-06',
        'Claude-3.5-Sonnet': '2024-06',
        'Claude-3.5-Haiku': '2024-08',
        'Claude-3-Opus': '2024-03',
        'Claude-3-Sonnet': '2024-03',
        'Claude-3-Haiku': '2024-03',
        'Llama-3.3-70b': '2024-12',
        'Llama-3.1-70b': '2024-07',
        'Llama-3.1-8b': '2024-07',
        'Qwen-2.5-72b': '2024-09',
        'Qwen-2.5-32b': '2024-09',
        'EXAONE-3.5-32b': '2024-08',
        'EXAONE-3.0-7.8b': '2024-08',
        'SOLAR-Pro': '2024-05',
        'Gemma-2-27b': '2024-06',
        'ko-gemma-2-9b': '2024-08',
    }
    
    for key, date in release_dates.items():
        if key.replace('-', '').replace('.', '').lower() in model_name.replace('-', '').replace('.', '').lower():
            return date
    
    return '2024-01'  # ê¸°ë³¸ê°’

def calculate_model_parameters(model_name):
    """ëª¨ë¸ëª…ìœ¼ë¡œë¶€í„° íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì • (ì–µ ë‹¨ìœ„)"""
    if '70b' in model_name.lower() or '72b' in model_name.lower():
        return 70
    elif '32b' in model_name.lower():
        return 32
    elif '27b' in model_name.lower():
        return 27
    elif '9b' in model_name.lower():
        return 9
    elif '8b' in model_name.lower():
        return 8
    elif '7.8b' in model_name.lower() or '7b' in model_name.lower():
        return 7.8
    elif 'gpt-4' in model_name.lower():
        return 175  # ì¶”ì •ì¹˜
    elif 'claude' in model_name.lower():
        return 100  # ì¶”ì •ì¹˜
    else:
        return 10  # ê¸°ë³¸ê°’

# ========== ì¶”ê°€ ë¶„ì„ í‘œ ìƒì„± í•¨ìˆ˜ ==========

def create_testset_accuracy_table(filtered_df, testsets, lang='ko'):
    """í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •ë‹µë¥  í‘œ - í…ŒìŠ¤íŠ¸ì…‹ ì›ë³¸ ë°ì´í„° ì‚¬ìš©"""
    if 'í…ŒìŠ¤íŠ¸ëª…' not in filtered_df.columns:
        return None
    
    test_names = filtered_df['í…ŒìŠ¤íŠ¸ëª…'].unique()
    
    data = []
    for test_name in test_names:
        # í•´ë‹¹ í…ŒìŠ¤íŠ¸ì˜ í‰ê°€ ê²°ê³¼
        testset_df = filtered_df[filtered_df['í…ŒìŠ¤íŠ¸ëª…'] == test_name]
        
        # ì‹¤ì œ ë¬¸ì œ ìˆ˜ëŠ” testsetsì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if test_name in testsets:
            actual_problems = len(testsets[test_name])
            
            # ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨ë„ testsetsì—ì„œ ê³„ì‚°
            law_ratio = 0
            if 'law' in testsets[test_name].columns:
                law_count = len(testsets[test_name][testsets[test_name]['law'] == 'O'])
                law_ratio = (law_count / actual_problems * 100) if actual_problems > 0 else 0
        else:
            # testsetsì— ì—†ìœ¼ë©´ ê³ ìœ  ë¬¸ì œ ìˆ˜ë¡œ ì¶”ì •
            actual_problems = testset_df['Question'].nunique() if 'Question' in testset_df.columns else len(testset_df)
            
            # ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨
            law_ratio = 0
            if 'law' in testset_df.columns:
                unique_problems = testset_df.drop_duplicates(subset=['Question'])
                law_count = len(unique_problems[unique_problems['law'] == 'O'])
                law_ratio = (law_count / actual_problems * 100) if actual_problems > 0 else 0
        
        # í‰ê°€ í†µê³„
        num_models = testset_df['ëª¨ë¸'].nunique()
        total_evaluations = len(testset_df)
        
        # ì •í™•ë„ ê³„ì‚°
        if 'ì •ë‹µì—¬ë¶€' in testset_df.columns:
            accuracy = testset_df['ì •ë‹µì—¬ë¶€'].mean() * 100
            correct_count = testset_df['ì •ë‹µì—¬ë¶€'].sum()
        else:
            accuracy = 0
            correct_count = 0
        
        # ê³¼ëª©ë³„ ì •ë‹µë¥  ê³„ì‚°
        best_subject = ""
        best_subject_acc = 0
        worst_subject = ""
        worst_subject_acc = 100
        
        if 'Subject' in testset_df.columns and 'ì •ë‹µì—¬ë¶€' in testset_df.columns:
            subject_acc = testset_df.groupby('Subject')['ì •ë‹µì—¬ë¶€'].mean() * 100
            if len(subject_acc) > 0:
                best_idx = subject_acc.idxmax()
                best_subject_acc = subject_acc[best_idx]
                best_subject = f"{best_idx} ({best_subject_acc:.1f}%)"
                
                worst_idx = subject_acc.idxmin()
                worst_subject_acc = subject_acc[worst_idx]
                worst_subject = f"{worst_idx} ({worst_subject_acc:.1f}%)"
        
        data.append({
            'í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name': test_name,
            'ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Problems': actual_problems,
            'í‰ê°€ ëª¨ë¸ ìˆ˜' if lang == 'ko' else 'Models': num_models,
            'ì´ í‰ê°€ íšŸìˆ˜' if lang == 'ko' else 'Total Evaluations': total_evaluations,
            'í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)': round(accuracy, 2),
            'ìµœê³  ê³¼ëª© (ì •ë‹µë¥ )' if lang == 'ko' else 'Best Subject (Accuracy)': best_subject if best_subject else '-',
            'ìµœì € ê³¼ëª© (ì •ë‹µë¥ )' if lang == 'ko' else 'Worst Subject (Accuracy)': worst_subject if worst_subject else '-',
            'ì •ë‹µ ìˆ˜' if lang == 'ko' else 'Correct': int(correct_count),
            'ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨ (%)' if lang == 'ko' else 'Law Problem Ratio (%)': round(law_ratio, 1)
        })
    
    df = pd.DataFrame(data).sort_values('í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)', ascending=False)
    return df

def create_model_release_performance_table(filtered_df, lang='ko'):
    """í‘œ 3: ëª¨ë¸ ì¶œì‹œ ì‹œê¸°ì™€ SafetyQ&A ì„±ëŠ¥"""
    models = filtered_df['ëª¨ë¸'].unique()
    
    data = []
    for model in models:
        model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
        accuracy = (model_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if 'ì •ë‹µì—¬ë¶€' in model_df.columns else 0
        release_date = calculate_model_release_date(model)
        
        data.append({
            'ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date': release_date,
            'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model,
            'í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)': round(accuracy, 2)
        })
    
    df = pd.DataFrame(data).sort_values('ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date', ascending=False)
    return df

def create_response_time_parameters_table(filtered_df, lang='ko'):
    """í‘œ 4: ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„ ë° ì •ë‹µë¥  (íŒŒë¼ë¯¸í„° ìˆ˜ í¬í•¨)"""
    time_col = None
    for col in ['ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)', 'ì´ì†Œìš”ì‹œê°„(ì´ˆ)', 'question_duration']:
        if col in filtered_df.columns:
            time_col = col
            break
    
    if time_col is None:
        return None
    
    models = filtered_df['ëª¨ë¸'].unique()
    
    data = []
    for model in models:
        model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
        
        avg_time = model_df[time_col].mean() if time_col in model_df.columns else 0
        accuracy = (model_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if 'ì •ë‹µì—¬ë¶€' in model_df.columns else 0
        params = calculate_model_parameters(model)
        
        data.append({
            'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model,
            'íŒŒë¼ë¯¸í„° ìˆ˜ (B)' if lang == 'ko' else 'Parameters (B)': params,
            'í‰ê·  ì‘ë‹µì‹œê°„ (ì´ˆ)' if lang == 'ko' else 'Avg Response Time (s)': round(avg_time, 2),
            'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': round(accuracy, 2)
        })
    
    df = pd.DataFrame(data).sort_values('íŒŒë¼ë¯¸í„° ìˆ˜ (B)' if lang == 'ko' else 'Parameters (B)', ascending=False)
    return df

def create_law_nonlaw_comparison_table(filtered_df, testsets, lang='ko'):
    """í‘œ 2: í…ŒìŠ¤íŠ¸ì…‹ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ"""
    if 'í…ŒìŠ¤íŠ¸ëª…' not in filtered_df.columns or 'law' not in filtered_df.columns:
        return None
    
    test_names = filtered_df['í…ŒìŠ¤íŠ¸ëª…'].unique()
    
    data = []
    for test_name in test_names:
        testset_df = filtered_df[filtered_df['í…ŒìŠ¤íŠ¸ëª…'] == test_name]
        
        # ë²•ë ¹ ë¬¸ì œ í†µê³„
        law_df = testset_df[testset_df['law'] == 'O']
        non_law_df = testset_df[testset_df['law'] != 'O']
        
        # ì‹¤ì œ ë¬¸ì œ ìˆ˜ëŠ” testsetsì—ì„œ
        law_problems = 0
        non_law_problems = 0
        total_problems = 0
        
        if test_name in testsets and 'law' in testsets[test_name].columns:
            test_data = testsets[test_name]
            law_problems = len(test_data[test_data['law'] == 'O'])
            non_law_problems = len(test_data[test_data['law'] != 'O'])
            total_problems = len(test_data)
        else:
            # Fallback: filtered_dfì—ì„œ ê³ ìœ  ë¬¸ì œ ìˆ˜ë¡œ ê³„ì‚°
            unique_law = testset_df[testset_df['law'] == 'O']['Question'].nunique() if 'Question' in testset_df.columns else len(law_df)
            unique_non_law = testset_df[testset_df['law'] != 'O']['Question'].nunique() if 'Question' in testset_df.columns else len(non_law_df)
            law_problems = unique_law
            non_law_problems = unique_non_law
            total_problems = law_problems + non_law_problems
        
        # ì •ë‹µë¥  ê³„ì‚°
        law_acc = 0
        non_law_acc = 0
        
        if 'ì •ë‹µì—¬ë¶€' in testset_df.columns:
            law_acc = (law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(law_df) > 0 else 0
            non_law_acc = (non_law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(non_law_df) > 0 else 0
        
        # ì •ë‹µ ìˆ˜
        law_correct = law_df['ì •ë‹µì—¬ë¶€'].sum() if len(law_df) > 0 and 'ì •ë‹µì—¬ë¶€' in law_df.columns else 0
        non_law_correct = non_law_df['ì •ë‹µì—¬ë¶€'].sum() if len(non_law_df) > 0 and 'ì •ë‹µì—¬ë¶€' in non_law_df.columns else 0
        
        # ì •ë‹µë¥  ì°¨ì´
        diff = law_acc - non_law_acc
        
        # ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨
        law_ratio = (law_problems / total_problems * 100) if total_problems > 0 else 0
        
        data.append({
            'í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name': test_name,
            'ë²•ë ¹ ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Law Problems': law_problems,
            'ë²•ë ¹ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Law Accuracy (%)': round(law_acc, 2),
            'ë²•ë ¹ ì •ë‹µ ìˆ˜' if lang == 'ko' else 'Law Correct': int(law_correct),
            'ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Non-Law Problems': non_law_problems,
            'ë¹„ë²•ë ¹ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Non-Law Accuracy (%)': round(non_law_acc, 2),
            'ë¹„ë²•ë ¹ ì •ë‹µ ìˆ˜' if lang == 'ko' else 'Non-Law Correct': int(non_law_correct),
            'ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹-ë¹„ë²•ë ¹)' if lang == 'ko' else 'Accuracy Diff (Law-NonLaw)': round(diff, 2),
            'ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨ (%)' if lang == 'ko' else 'Law Ratio (%)': round(law_ratio, 1)
        })
    
    df = pd.DataFrame(data).sort_values('ë²•ë ¹ ë¬¸ì œ ë¹„ìœ¨ (%)' if lang == 'ko' else 'Law Ratio (%)', ascending=False)
    return df

def create_year_correlation_table(filtered_df, lang='ko'):
    """í‘œ 6: ì¶œì œ ì—°ë„ë³„ í‰ê·  ì •ë‹µë¥  ë° ë¬¸í•­ ìˆ˜ (ìƒê´€ê³„ìˆ˜ í¬í•¨)"""
    if 'Year' not in filtered_df.columns:
        return None
    
    # ë²¡í„°í™”ëœ ì—°ë„ ë³€í™˜ (copy ìµœì†Œí™”)
    year_int_series = filtered_df['Year'].apply(safe_convert_to_int)
    valid_mask = year_int_series.notna()
    
    if not valid_mask.any():
        return None
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    year_df = filtered_df.loc[valid_mask, ['Question', 'ì •ë‹µì—¬ë¶€']].assign(Year_Int=year_int_series[valid_mask])
    
    # ì—°ë„ë³„ í†µê³„
    year_stats = year_df.groupby('Year_Int').agg({
        'Question': 'count',
        'ì •ë‹µì—¬ë¶€': ['mean', 'std']
    }).reset_index()
    
    # ë‹¤êµ­ì–´ ì»¬ëŸ¼ëª…
    if lang == 'ko':
        year_stats.columns = ['ì—°ë„', 'ë¬¸í•­ ìˆ˜', 'í‰ê·  ì •ë‹µë¥ ', 'í‘œì¤€í¸ì°¨']
        year_col = 'ì—°ë„'
        count_col = 'ë¬¸í•­ ìˆ˜'
        acc_col = 'í‰ê·  ì •ë‹µë¥ '
        std_col = 'í‘œì¤€í¸ì°¨'
        law_ratio_col = 'ë²•ë ¹ ë¬¸í•­ ë¹„ìœ¨ (%)'
        corr_label = 'ìƒê´€ê³„ìˆ˜ (r)'
        p_label = 'p-value'
    else:
        year_stats.columns = ['Year', 'Problem Count', 'Avg Accuracy', 'Std Dev']
        year_col = 'Year'
        count_col = 'Problem Count'
        acc_col = 'Avg Accuracy'
        std_col = 'Std Dev'
        law_ratio_col = 'Law Ratio (%)'
        corr_label = 'Correlation (r)'
        p_label = 'p-value'
    
    year_stats[acc_col] = year_stats[acc_col] * 100
    year_stats[std_col] = year_stats[std_col] * 100
    year_stats[year_col] = year_stats[year_col].astype(int)
    
    # ë²•ë ¹ ë¬¸í•­ ë¹„ìœ¨
    if 'law' in year_df.columns:
        law_ratio = year_df.groupby('Year_Int').apply(
            lambda x: (x['law'] == 'O').sum() / len(x) * 100
        ).reset_index()
        law_ratio.columns = [year_col, law_ratio_col]
        year_stats = year_stats.merge(law_ratio, on=year_col, how='left')
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    if len(year_stats) > 1:
        correlation, p_value = stats.pearsonr(year_stats[year_col], year_stats[acc_col])
        
        # ìƒê´€ê³„ìˆ˜ ì •ë³´ë¥¼ ë³„ë„ í–‰ìœ¼ë¡œ ì¶”ê°€
        corr_row = pd.DataFrame({
            year_col: [corr_label],
            count_col: ['-'],
            acc_col: [f"{correlation:.4f}"],
            std_col: ['-']
        })
        
        if law_ratio_col in year_stats.columns:
            corr_row[law_ratio_col] = ['-']
        
        p_row = pd.DataFrame({
            year_col: [p_label],
            count_col: ['-'],
            acc_col: [f"{p_value:.4f}"],
            std_col: ['-']
        })
        
        if law_ratio_col in year_stats.columns:
            p_row[law_ratio_col] = ['-']
        
        year_stats = pd.concat([year_stats, corr_row, p_row], ignore_index=True)
    
    return year_stats

def create_difficulty_distribution_table(filtered_df, lang='ko'):
    """í‘œ 7: ë‚œì´ë„ êµ¬ê°„ë³„ ë¬¸í•­ ë¶„í¬ ë° ì •ë‹µë¥ """
    # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚°
    difficulty = filtered_df.groupby('Question').agg({
        'ì •ë‹µì—¬ë¶€': ['mean', 'count']
    }).reset_index()
    difficulty.columns = ['Question', 'difficulty_score', 'attempt_count']
    difficulty['difficulty_score'] = difficulty['difficulty_score'] * 100
    
    # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
    def classify_difficulty(score, lang='ko'):
        if lang == 'ko':
            if score < 20:
                return 'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)'
            elif score < 40:
                return 'ì–´ë ¤ì›€ (20-40%)'
            elif score < 60:
                return 'ë³´í†µ (40-60%)'
            elif score < 80:
                return 'ì‰¬ì›€ (60-80%)'
            else:
                return 'ë§¤ìš° ì‰¬ì›€ (80-100%)'
        else:
            if score < 20:
                return 'Very Hard (0-20%)'
            elif score < 40:
                return 'Hard (20-40%)'
            elif score < 60:
                return 'Medium (40-60%)'
            elif score < 80:
                return 'Easy (60-80%)'
            else:
                return 'Very Easy (80-100%)'
    
    difficulty['ë‚œì´ë„_êµ¬ê°„'] = difficulty['difficulty_score'].apply(lambda x: classify_difficulty(x, lang))
    
    # êµ¬ê°„ë³„ í†µê³„
    difficulty_dist = difficulty.groupby('ë‚œì´ë„_êµ¬ê°„').agg({
        'Question': 'count',
        'difficulty_score': 'mean'
    }).reset_index()
    
    difficulty_dist.columns = [
        'ë‚œì´ë„ êµ¬ê°„' if lang == 'ko' else 'Difficulty Range',
        'ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count',
        'í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)'
    ]
    
    difficulty_dist['ë¹„ìœ¨ (%)'] = (difficulty_dist['ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count'] / 
                                    difficulty_dist['ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count'].sum() * 100)
    
    # ë‚œì´ë„ ìˆœì„œ ì •ì˜
    if lang == 'ko':
        order = ['ë§¤ìš° ì–´ë ¤ì›€ (0-20%)', 'ì–´ë ¤ì›€ (20-40%)', 'ë³´í†µ (40-60%)', 'ì‰¬ì›€ (60-80%)', 'ë§¤ìš° ì‰¬ì›€ (80-100%)']
    else:
        order = ['Very Hard (0-20%)', 'Hard (20-40%)', 'Medium (40-60%)', 'Easy (60-80%)', 'Very Easy (80-100%)']
    
    difficulty_dist['ë‚œì´ë„ êµ¬ê°„' if lang == 'ko' else 'Difficulty Range'] = pd.Categorical(
        difficulty_dist['ë‚œì´ë„ êµ¬ê°„' if lang == 'ko' else 'Difficulty Range'],
        categories=order,
        ordered=True
    )
    
    difficulty_dist = difficulty_dist.sort_values('ë‚œì´ë„ êµ¬ê°„' if lang == 'ko' else 'Difficulty Range')
    
    return difficulty_dist

def create_incorrect_pattern_table(filtered_df, lang='ko'):
    """í‘œ 10: ì£¼ìš” ì˜¤ë‹µ íŒ¨í„´ ë° ë¹ˆë„ ë¶„ì„"""
    # ë¬¸ì œë³„ ì˜¤ë‹µ ë¶„ì„
    problem_analysis = filtered_df.groupby('Question').agg({
        'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
    }).reset_index()
    problem_analysis.columns = ['Question', 'correct_count', 'total_count', 'correct_rate']
    problem_analysis['incorrect_rate'] = 1 - problem_analysis['correct_rate']
    
    # ì˜¤ë‹µ íŒ¨í„´ ë¶„ë¥˜
    patterns = []
    
    # ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ
    all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
    patterns.append({
        'ì˜¤ë‹µ íŒ¨í„´ ìœ í˜•' if lang == 'ko' else 'Error Pattern Type': 'ì „ì²´ ëª¨ë¸ ì˜¤ë‹µ' if lang == 'ko' else 'All Models Incorrect',
        'ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count': len(all_wrong),
        'ëª¨ë¸ ì¼ì¹˜ë„ (%)' if lang == 'ko' else 'Model Agreement (%)': 100.0
    })
    
    # ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (70% ì´ìƒ)
    most_wrong = problem_analysis[(problem_analysis['incorrect_rate'] >= 0.7) & (problem_analysis['incorrect_rate'] < 1.0)]
    if len(most_wrong) > 0:
        avg_agreement = most_wrong['incorrect_rate'].mean() * 100
        patterns.append({
            'ì˜¤ë‹µ íŒ¨í„´ ìœ í˜•' if lang == 'ko' else 'Error Pattern Type': 'ëŒ€ë¶€ë¶„ ëª¨ë¸ ì˜¤ë‹µ (â‰¥70%)' if lang == 'ko' else 'Most Models Incorrect (â‰¥70%)',
            'ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count': len(most_wrong),
            'ëª¨ë¸ ì¼ì¹˜ë„ (%)' if lang == 'ko' else 'Model Agreement (%)': round(avg_agreement, 1)
        })
    
    # ì ˆë°˜ ì •ë„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ
    half_wrong = problem_analysis[(problem_analysis['incorrect_rate'] >= 0.4) & (problem_analysis['incorrect_rate'] < 0.7)]
    if len(half_wrong) > 0:
        avg_agreement = half_wrong['incorrect_rate'].mean() * 100
        patterns.append({
            'ì˜¤ë‹µ íŒ¨í„´ ìœ í˜•' if lang == 'ko' else 'Error Pattern Type': 'ì ˆë°˜ ì •ë„ ì˜¤ë‹µ (40-70%)' if lang == 'ko' else 'About Half Incorrect (40-70%)',
            'ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count': len(half_wrong),
            'ëª¨ë¸ ì¼ì¹˜ë„ (%)' if lang == 'ko' else 'Model Agreement (%)': round(avg_agreement, 1)
        })
    
    # ì¼ë¶€ ëª¨ë¸ë§Œ í‹€ë¦° ë¬¸ì œ
    some_wrong = problem_analysis[(problem_analysis['incorrect_rate'] > 0) & (problem_analysis['incorrect_rate'] < 0.4)]
    if len(some_wrong) > 0:
        avg_agreement = some_wrong['incorrect_rate'].mean() * 100
        patterns.append({
            'ì˜¤ë‹µ íŒ¨í„´ ìœ í˜•' if lang == 'ko' else 'Error Pattern Type': 'ì¼ë¶€ ëª¨ë¸ ì˜¤ë‹µ (<40%)' if lang == 'ko' else 'Some Models Incorrect (<40%)',
            'ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count': len(some_wrong),
            'ëª¨ë¸ ì¼ì¹˜ë„ (%)' if lang == 'ko' else 'Model Agreement (%)': round(avg_agreement, 1)
        })
    
    return pd.DataFrame(patterns)

def create_model_law_performance_table(filtered_df, lang='ko'):
    """
    í‘œ 5: ëª¨ë¸ë³„ ë²•ë ¹ ë¬¸í•­ vs ë¹„ë²•ë ¹ ë¬¸í•­ ì„±ëŠ¥ ë¹„êµ
    """
    if 'law' not in filtered_df.columns or 'ì •ë‹µì—¬ë¶€' not in filtered_df.columns:
        return None
    
    results = []
    
    for model in filtered_df['ëª¨ë¸'].unique():
        model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
        
        law_df = model_df[model_df['law'] == 'O']
        non_law_df = model_df[model_df['law'] != 'O']
        
        law_acc = (law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(law_df) > 0 else 0
        non_law_acc = (non_law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(non_law_df) > 0 else 0
        diff = law_acc - non_law_acc
        
        results.append({
            'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model,
            'ë²•ë ¹ ë¬¸í•­ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Law Accuracy (%)': round(law_acc, 2),
            'ë¹„ë²•ë ¹ ë¬¸í•­ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Non-Law Accuracy (%)': round(non_law_acc, 2),
            'ê²©ì°¨ (%p)' if lang == 'ko' else 'Gap (%p)': round(diff, 2),
            'ë²•ë ¹ ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Law Count': len(law_df),
            'ë¹„ë²•ë ¹ ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Non-Law Count': len(non_law_df)
        })
    
    df = pd.DataFrame(results)
    
    if len(df) > 0:
        # ê²©ì°¨ ì ˆëŒ€ê°’ ìˆœìœ¼ë¡œ ì •ë ¬
        df['abs_gap'] = df['ê²©ì°¨ (%p)' if lang == 'ko' else 'Gap (%p)'].abs()
        df = df.sort_values('abs_gap')
        df = df.drop('abs_gap', axis=1)
        
        # ìˆœìœ„ ì¶”ê°€
        df.insert(0, 'ê²©ì°¨ ìˆœìœ„' if lang == 'ko' else 'Gap Rank', range(1, len(df) + 1))
    
    return df

def create_difficulty_model_performance_table(filtered_df, lang='ko'):
    """
    í‘œ 8: ì£¼ìš” ëª¨ë¸ì˜ ë‚œì´ë„ êµ¬ê°„ë³„ ì •ë‹µë¥ 
    """
    if 'ì •ë‹µì—¬ë¶€' not in filtered_df.columns:
        return None
    
    # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚°
    difficulty = filtered_df.groupby('Question')['ì •ë‹µì—¬ë¶€'].mean() * 100
    
    # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
    def classify_difficulty(score):
        if score < 20:
            return 'ë§¤ìš° ì–´ë ¤ì›€' if lang == 'ko' else 'Very Hard'
        elif score < 40:
            return 'ì–´ë ¤ì›€' if lang == 'ko' else 'Hard'
        elif score < 60:
            return 'ë³´í†µ' if lang == 'ko' else 'Medium'
        elif score < 80:
            return 'ì‰¬ì›€' if lang == 'ko' else 'Easy'
        else:
            return 'ë§¤ìš° ì‰¬ì›€' if lang == 'ko' else 'Very Easy'
    
    # ë‚œì´ë„ ë ˆë²¨ì„ Seriesë¡œ ìƒì„± (copy ì—†ì´)
    difficulty_levels = filtered_df['Question'].map(
        lambda q: classify_difficulty(difficulty.get(q, 50))
    )
    
    # ë‚œì´ë„ ìˆœì„œ
    difficulty_order = [
        'ë§¤ìš° ì‰¬ì›€' if lang == 'ko' else 'Very Easy',
        'ì‰¬ì›€' if lang == 'ko' else 'Easy',
        'ë³´í†µ' if lang == 'ko' else 'Medium',
        'ì–´ë ¤ì›€' if lang == 'ko' else 'Hard',
        'ë§¤ìš° ì–´ë ¤ì›€' if lang == 'ko' else 'Very Hard'
    ]
    
    # ìƒìœ„ ëª¨ë¸ ì„ íƒ (ì •ë‹µë¥  ê¸°ì¤€)
    top_models = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().nlargest(10).index.tolist()
    
    results = []
    for model in top_models:
        model_mask = filtered_df['ëª¨ë¸'] == model
        model_difficulty = difficulty_levels[model_mask]
        model_correct = filtered_df.loc[model_mask, 'ì •ë‹µì—¬ë¶€']
        
        row = {'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model}
        
        for diff_level in difficulty_order:
            diff_mask = model_difficulty == diff_level
            diff_correct = model_correct[diff_mask]
            acc = (diff_correct.mean() * 100) if len(diff_correct) > 0 else 0
            row[diff_level] = round(acc, 1)
        
        results.append(row)
    
    return pd.DataFrame(results)

def create_cost_efficiency_table(filtered_df, lang='ko'):
    """
    í‘œ 9: ì£¼ìš” ìƒì—…ìš© ëª¨ë¸ì˜ ë¹„ìš© íš¨ìœ¨ì„± ë¹„êµ
    """
    # í† í° ì»¬ëŸ¼ í™•ì¸
    token_columns = {
        'input': ['ì…ë ¥í† í°', 'input_tokens', 'Input Tokens'],
        'output': ['ì¶œë ¥í† í°', 'output_tokens', 'Output Tokens'],
        'total': ['ì´í† í°', 'total_tokens', 'Total Tokens']
    }
    
    available_cols = {}
    for key, possible_names in token_columns.items():
        for col_name in possible_names:
            if col_name in filtered_df.columns:
                available_cols[key] = col_name
                break
    
    if not available_cols or 'ì •ë‹µì—¬ë¶€' not in filtered_df.columns:
        return None
    
    # ìƒì—…ìš© ëª¨ë¸ë§Œ í•„í„°ë§
    commercial_models = ['GPT-4o', 'GPT-4o-Mini', 'Claude-Sonnet-4.5', 'Claude-Sonnet-4', 'Claude-3.5-Sonnet', 'Claude-3.5-Haiku', 'Claude-Haiku-4.5']
    commercial_df = filtered_df[filtered_df['ëª¨ë¸'].str.contains('|'.join(commercial_models), case=False, na=False)]
    
    if len(commercial_df) == 0:
        return None
    
    # ëª¨ë¸ ê°€ê²© (per 1M tokens)
    MODEL_PRICING = {
        'GPT-4o': {'input': 5.00, 'output': 15.00},
        'GPT-4o-Mini': {'input': 0.150, 'output': 0.600},
        'Claude-Sonnet-4.5': {'input': 3.00, 'output': 15.00},
        'Claude-Sonnet-4': {'input': 3.00, 'output': 15.00},
        'Claude-3.5-Sonnet': {'input': 3.00, 'output': 15.00},
        'Claude-Haiku-4.5': {'input': 1.00, 'output': 5.00},
        'Claude-3.5-Haiku': {'input': 0.80, 'output': 4.00}
    }
    
    results = []
    
    for model in commercial_df['ëª¨ë¸'].unique():
        model_df = commercial_df[commercial_df['ëª¨ë¸'] == model]
        
        # ì •ë‹µë¥ 
        acc = model_df['ì •ë‹µì—¬ë¶€'].mean() * 100
        correct_count = model_df['ì •ë‹µì—¬ë¶€'].sum()
        
        # í‰ê·  í† í°
        if 'input' in available_cols and 'output' in available_cols:
            avg_input = model_df[available_cols['input']].mean()
            avg_output = model_df[available_cols['output']].mean()
        elif 'total' in available_cols:
            avg_total = model_df[available_cols['total']].mean()
            avg_input = avg_total * 0.6  # ì¶”ì •
            avg_output = avg_total * 0.4  # ì¶”ì •
        else:
            continue
        
        # ë¹„ìš© ê³„ì‚°
        matched_pricing = None
        for price_model, pricing in MODEL_PRICING.items():
            if price_model.replace('-', '').replace('.', '').lower() in model.replace('-', '').replace('.', '').lower():
                matched_pricing = pricing
                break
        
        if matched_pricing:
            # ë¬¸ì œë‹¹ ë¹„ìš©
            cost_per_problem = (avg_input / 1_000_000) * matched_pricing['input'] + \
                              (avg_output / 1_000_000) * matched_pricing['output']
            
            # ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš©
            if correct_count > 0:
                cost_per_1000_correct = (cost_per_problem * len(model_df) / correct_count) * 1000
            else:
                cost_per_1000_correct = float('inf')
            
            results.append({
                'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model,
                'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': round(acc, 2),
                'í‰ê·  ì…ë ¥ í† í°' if lang == 'ko' else 'Avg Input Tokens': int(avg_input),
                'í‰ê·  ì¶œë ¥ í† í°' if lang == 'ko' else 'Avg Output Tokens': int(avg_output),
                'ì´ ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)': round(cost_per_problem * len(model_df), 4),
                'ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ($)' if lang == 'ko' else 'Cost per 1K Correct ($)': round(cost_per_1000_correct, 2) if cost_per_1000_correct != float('inf') else 0
            })
    
    df = pd.DataFrame(results)
    
    if len(df) > 0:
        # ë¹„ìš© íš¨ìœ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬ (ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ë‚®ì€ ìˆœ)
        df = df.sort_values('ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ($)' if lang == 'ko' else 'Cost per 1K Correct ($)')
    
    return df

def create_benchmark_comparison_table(filtered_df, lang='ko'):
    """
    í‘œ 11: SafetyQ&Aì™€ ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¹„êµ
    (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ì˜ˆì‹œ ë°ì´í„° ìƒì„±)
    """
    if 'ì •ë‹µì—¬ë¶€' not in filtered_df.columns:
        return None
    
    # SafetyQ&A ì„±ëŠ¥ ê³„ì‚°
    safetyqa_performance = {}
    for model in filtered_df['ëª¨ë¸'].unique():
        model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
        safetyqa_performance[model] = model_df['ì •ë‹µì—¬ë¶€'].mean() * 100
    
    # ì˜ˆì‹œ: ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ (ì‹¤ì œ ë°ì´í„°ëŠ” ì™¸ë¶€ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
    # ì—¬ê¸°ì„œëŠ” SafetyQ&A ì„±ëŠ¥ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •ì¹˜ ìƒì„±
    benchmark_data = []
    
    for model, safetyqa_score in safetyqa_performance.items():
        # ë²”ìš© ë²¤ì¹˜ë§ˆí¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ SafetyQ&Aë³´ë‹¤ ë†’ìŒ
        mmlu_score = min(safetyqa_score * 1.2 + 10, 95)
        gpqa_score = min(safetyqa_score * 0.8 + 5, 70)
        mmlu_pro_score = min(safetyqa_score * 0.9 + 8, 80)
        
        benchmark_data.append({
            'ëª¨ë¸ëª…' if lang == 'ko' else 'Model': model,
            'SafetyQ&A': round(safetyqa_score, 1),
            'MMLU': round(mmlu_score, 1),
            'GPQA': round(gpqa_score, 1),
            'MMLU-Pro': round(mmlu_pro_score, 1)
        })
    
    df = pd.DataFrame(benchmark_data)
    
    if len(df) > 0:
        # SafetyQ&A ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
        df = df.sort_values('SafetyQ&A', ascending=False)
    
    return df

# ì•™ìƒë¸” ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (ìµœì í™” ë²„ì „)
def create_ensemble_model(base_df, ensemble_name, selected_model_names, method='majority'):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì•™ìƒë¸” ëª¨ë¸ ë°ì´í„° ìƒì„± (ìµœì í™” ë²„ì „)
    
    Args:
        base_df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        ensemble_name: ì•™ìƒë¸” ëª¨ë¸ ì´ë¦„
        selected_model_names: ì•™ìƒë¸”ì— í¬í•¨í•  ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        method: ì•™ìƒë¸” ë°©ë²• ('majority' ë˜ëŠ” 'weighted')
    
    Returns:
        ensemble_df: ì•™ìƒë¸” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    # ì„ íƒëœ ëª¨ë¸ë§Œ í•„í„°ë§ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    filtered_df = base_df[base_df['ëª¨ë¸'].isin(selected_model_names)]
    
    if filtered_df.empty:
        return pd.DataFrame()
    
    # ëª¨ë¸ë³„ ì „ì²´ ì •í™•ë„ ê³„ì‚° (ê°€ì¤‘ íˆ¬í‘œìš©) - ë¯¸ë¦¬ ê³„ì‚°
    model_accuracy = {}
    if method == 'weighted':
        model_accuracy = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().to_dict()
    
    # ë¬¸ì œë³„ ëª¨ë¸ ìˆ˜ ê³„ì‚°í•˜ì—¬ ëª¨ë“  ëª¨ë¸ì´ í‘¼ ë¬¸ì œë§Œ ì„ íƒ
    question_model_counts = filtered_df.groupby('Question')['ëª¨ë¸'].nunique()
    valid_questions = question_model_counts[question_model_counts == len(selected_model_names)].index
    
    if len(valid_questions) == 0:
        return pd.DataFrame()
    
    # ìœ íš¨í•œ ë¬¸ì œë§Œ í•„í„°ë§
    valid_df = filtered_df[filtered_df['Question'].isin(valid_questions)]
    
    ensemble_rows = []
    
    # ë¬¸ì œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬ (groupby ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”)
    for question, q_df in valid_df.groupby('Question'):
        # ëŒ€í‘œ í–‰ ê°€ì ¸ì˜¤ê¸°
        base_row = q_df.iloc[0].to_dict()
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
        predictions = q_df['ì˜ˆì¸¡ë‹µ'].tolist()
        
        if method == 'majority':
            # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
            counter = Counter(predictions)
            ensemble_answer = counter.most_common(1)[0][0]
        else:  # weighted
            # ê°€ì¤‘ íˆ¬í‘œ - ë²¡í„°í™” ì²˜ë¦¬ (iterrows ëŒ€ì‹  zip ì‚¬ìš©)
            answer_weights = {}
            for answer, model in zip(q_df['ì˜ˆì¸¡ë‹µ'].values, q_df['ëª¨ë¸'].values):
                weight = model_accuracy.get(model, 0)
                answer_weights[answer] = answer_weights.get(answer, 0) + weight
            
            ensemble_answer = max(answer_weights, key=answer_weights.get) if answer_weights else None
        
        # ì•™ìƒë¸” ê²°ê³¼ í–‰ ìƒì„±
        if ensemble_answer is not None:
            base_row['ëª¨ë¸'] = ensemble_name
            base_row['ì˜ˆì¸¡ë‹µ'] = ensemble_answer
            base_row['ì •ë‹µì—¬ë¶€'] = (base_row.get('Answer') == ensemble_answer) if pd.notna(base_row.get('Answer')) else False
            base_row['ìƒì„¸ë„'] = 'ensemble'
            base_row['í”„ë¡¬í”„íŒ…'] = method
            ensemble_rows.append(base_row)
    
    if ensemble_rows:
        return pd.DataFrame(ensemble_rows)
    return pd.DataFrame()


# ëª¨ë¸ëª… í¬ë§·íŒ… í•¨ìˆ˜ (ìºì‹±ìœ¼ë¡œ ë°˜ë³µ í˜¸ì¶œ ìµœì í™”)
@lru_cache(maxsize=256)
def format_model_name(model_str):
    """
    ëª¨ë¸ëª…ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ìºì‹± ì ìš©)
    ì˜ˆ: claude-sonnet-4-5-20250929 â†’ Claude-Sonnet-4.5
        gpt-4o-mini â†’ GPT-4o-Mini
        llama-3-3-70b â†’ Llama-3.3-70b
    """
    # ë‚ ì§œ íŒ¨í„´ ì œê±° (8ìë¦¬ ìˆ«ì)
    model_str = re.sub(r'-\d{8}$', '', model_str)
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: GPT ëª¨ë¸
    if model_str.startswith('gpt-'):
        parts = model_str.split('-')
        formatted_parts = ['GPT']
        
        for i in range(1, len(parts)):
            part = parts[i]
            if part == '4o' or part == '3.5':
                formatted_parts.append(part)
            elif part.isdigit():
                formatted_parts.append(part)
            else:
                formatted_parts.append(part.capitalize())
        
        return '-'.join(formatted_parts)
    
    # Claude ëª¨ë¸ ì²˜ë¦¬
    if model_str.startswith('claude-'):
        parts = model_str.split('-')
        formatted_parts = ['Claude']
        
        i = 1
        while i < len(parts):
            part = parts[i]
            
            if i + 1 < len(parts) and part.isdigit() and parts[i+1].isdigit():
                formatted_parts.append(f"{part}.{parts[i+1]}")
                i += 2
            elif part in ['sonnet', 'haiku', 'opus']:
                formatted_parts.append(part.capitalize())
                i += 1
            elif part.isdigit():
                formatted_parts.append(part)
                i += 1
            else:
                formatted_parts.append(part.capitalize())
                i += 1
        
        return '-'.join(formatted_parts)
    
    # ê¸°íƒ€ ëª¨ë¸: ìŠ¤ë§ˆíŠ¸ ë²„ì „ ë²ˆí˜¸ ì²˜ë¦¬
    parts = model_str.split('-')
    formatted_parts = []
    
    i = 0
    while i < len(parts):
        part = parts[i]
        
        if i == 0:
            if len(part) > 1 and part[:-1].isalpha() and part[-1].isdigit():
                if i + 1 < len(parts) and parts[i+1].isdigit() and len(parts[i+1]) == 1:
                    formatted_parts.append(f"{part.capitalize()}.{parts[i+1]}")
                    i += 2
                    continue
            formatted_parts.append(part.capitalize())
            i += 1
        elif (i + 1 < len(parts) and 
              part.isdigit() and len(part) == 1 and 
              parts[i+1].isdigit() and len(parts[i+1]) == 1):
            formatted_parts.append(f"{part}.{parts[i+1]}")
            i += 2
        elif not part.isdigit() and not any(c.isdigit() for c in part):
            formatted_parts.append(part.capitalize())
            i += 1
        else:
            formatted_parts.append(part)
            i += 1
    
    return '-'.join(formatted_parts)


# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_data(data_dir):
    """ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í†µí•©"""
    
    # testset íŒŒì¼ë“¤ ë¡œë“œ
    testset_files = glob.glob(os.path.join(data_dir, "testset_*.csv"))
    testsets = {}
    for file in testset_files:
        test_name = os.path.basename(file).replace("testset_", "").replace(".csv", "")
        try:
            df = pd.read_csv(file, encoding='utf-8')
            testsets[test_name] = df
        except:
            try:
                df = pd.read_csv(file, encoding='cp949')
                testsets[test_name] = df
            except:
                continue
    
    # testsetì—ì„œ í…ŒìŠ¤íŠ¸ëª… ëª©ë¡ ì¶”ì¶œ (ìë™ ê°ì§€)
    available_test_names = list(testsets.keys())
    
    # ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ (detailedë§Œ - summaryëŠ” ì»¬ëŸ¼ êµ¬ì¡° ë¶ˆì¼ì¹˜ë¡œ ì œì™¸)
    result_files = glob.glob(os.path.join(data_dir, "*_detailed_*.csv"))
    
    results = []
    for file in result_files:
        filename = os.path.basename(file)
        
        try:
            # íŒŒì¼ëª… í˜•ì‹: {ëª¨ë¸ëª…}_{ìƒì„¸ë„}_{í”„ë¡¬í”„íŒ…}_{í…ŒìŠ¤íŠ¸ëª…}.csv
            # ì˜ˆ: llama-3-3-70b_detailed_noprompting_ì‚°ì—…ì•ˆì „ê¸°ì‚¬.csv
            
            # í…ŒìŠ¤íŠ¸ëª… ì°¾ê¸° ë° ì œê±° (testsetì—ì„œ ì¶”ì¶œí•œ ëª©ë¡ ì‚¬ìš©)
            test_name = None
            filename_without_csv = filename.replace('.csv', '')
            
            # ê°€ì¥ ê¸´ í…ŒìŠ¤íŠ¸ëª…ë¶€í„° ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
            sorted_test_names = sorted(available_test_names, key=len, reverse=True)
            
            for tn in sorted_test_names:
                if filename_without_csv.endswith('_' + tn):
                    test_name = tn
                    # í…ŒìŠ¤íŠ¸ëª… ì œê±°
                    filename_without_test = filename_without_csv[:-len('_' + tn)]
                    break
            
            if test_name is None:
                continue
            
            # ë‚¨ì€ ë¶€ë¶„ì„ '_'ë¡œ ë¶„ë¦¬
            parts = filename_without_test.split('_')
            
            if len(parts) < 3:
                continue
            
            # ìƒì„¸ë„ ì°¾ê¸° (detailed ë˜ëŠ” summary)
            detail_type = None
            detail_idx = -1
            for i, part in enumerate(parts):
                if part in ['detailed', 'summary']:
                    detail_type = part
                    detail_idx = i
                    break
            
            if detail_type is None or detail_idx == -1:
                continue
            
            # ëª¨ë¸ëª… ì¶”ì¶œ (ìƒì„¸ë„ ì´ì „ê¹Œì§€ì˜ ëª¨ë“  ë¶€ë¶„ì„ ê²°í•©)
            model_parts = parts[:detail_idx]
            model_raw = '_'.join(model_parts)
            
            # í”„ë¡¬í”„íŒ… ë°©ì‹ ì¶”ì¶œ (ìƒì„¸ë„ ë‹¤ìŒë¶€í„° ëê¹Œì§€)
            prompt_parts = parts[detail_idx + 1:]
            prompt_raw = '_'.join(prompt_parts)
            
            # í”„ë¡¬í”„íŒ… ë°©ì‹ ì •ê·œí™”
            if "noprompting" in prompt_raw.lower() or "no-prompting" in prompt_raw.lower() or "no_prompting" in prompt_raw.lower():
                prompt_type = "no-prompting"
            elif "few-shot" in prompt_raw.lower() or "few_shot" in prompt_raw.lower() or "fewshot" in prompt_raw.lower():
                prompt_type = "few-shot"
            elif "cot" in prompt_raw.lower() or "chain-of-thought" in prompt_raw.lower():
                prompt_type = "cot"
            else:
                prompt_type = prompt_raw if prompt_raw else "unknown"
            
            # ğŸ”¥ ëª¨ë¸ëª… ìë™ íŒŒì‹± ë° ì •ê·œí™” (í•˜ë“œì½”ë”© ì œê±°)
            # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€í™˜í•˜ê³  ì†Œë¬¸ìë¡œ ì •ê·œí™”
            model_normalized = model_raw.lower().replace('_', '-')
            
            # ì™¸ë¶€ì— ì •ì˜ëœ ìºì‹±ëœ format_model_name í•¨ìˆ˜ ì‚¬ìš©
            model = format_model_name(model_normalized)
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except:
                try:
                    df = pd.read_csv(file, encoding='cp949')
                except:
                    continue
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            df['ëª¨ë¸'] = model
            df['ìƒì„¸ë„'] = detail_type
            df['í”„ë¡¬í”„íŒ…'] = prompt_type
            df['í…ŒìŠ¤íŠ¸ëª…'] = test_name
            
            results.append(df)
            
        except Exception as e:
            st.sidebar.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(file)}")
            continue
    
    if results:
        results_df = pd.concat(results, ignore_index=True)
    else:
        results_df = pd.DataFrame()
    
    return testsets, results_df

def safe_convert_to_int(value):
    """ì•ˆì „í•˜ê²Œ ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ - ì‰¼í‘œ êµ¬ë¶„ì ì²˜ë¦¬ ê°œì„ """
    try:
        # Noneì´ë‚˜ NaN ì²˜ë¦¬
        if pd.isna(value):
            return None
            
        # ë¬¸ìì—´ì¸ ê²½ìš° ì‰¼í‘œ ì œê±° (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì)
        if isinstance(value, str):
            # ì‰¼í‘œëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ìì´ë¯€ë¡œ ê·¸ëƒ¥ ì œê±°
            value = value.replace(',', '')
        
        # floatë¡œ ë³€í™˜ í›„ intë¡œ ë³€í™˜
        return int(float(value))
    except (ValueError, TypeError):
        return None

def get_available_sessions(df, test_names):
    """íŠ¹ì • í…ŒìŠ¤íŠ¸ë“¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ì…˜ ëª©ë¡ ë°˜í™˜ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)"""
    if df is None or len(df) == 0:
        return []
    
    # ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì„ íƒ ì‹œ í•„í„°ë§
    if test_names:
        test_df = df[df['í…ŒìŠ¤íŠ¸ëª…'].isin(test_names)] if 'í…ŒìŠ¤íŠ¸ëª…' in df.columns else df
    else:
        test_df = df
    
    if 'Session' in test_df.columns:
        sessions_raw = test_df['Session'].dropna().unique().tolist()
        sessions_clean = []
        
        for s in sessions_raw:
            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ ë¨¼ì € ì‹œë„
            s_int = safe_convert_to_int(s)
            if s_int is not None:
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•˜ë©´ ì •ìˆ˜ë¡œ ì €ì¥
                if s_int not in sessions_clean:
                    sessions_clean.append(s_int)
            else:
                # ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•˜ë©´ ë¬¸ìì—´ë¡œ ì €ì¥
                if isinstance(s, str):
                    s_clean = s.strip()
                    if s_clean and s_clean not in sessions_clean:
                        sessions_clean.append(s_clean)
        
        # ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(sessions_clean, key=lambda x: (isinstance(x, str), x))
    return []

def create_problem_identifier(row, lang='ko'):
    """ë¬¸ì œ ì‹ë³„ì ìƒì„± (í…ŒìŠ¤íŠ¸ëª…/ì—°ë„/ì„¸ì…˜/ê³¼ëª©/ë¬¸ì œë²ˆí˜¸)"""
    parts = []
    
    if 'Test Name' in row and pd.notna(row['Test Name']):
        parts.append(str(row['Test Name']))
    elif 'í…ŒìŠ¤íŠ¸ëª…' in row and pd.notna(row['í…ŒìŠ¤íŠ¸ëª…']):
        parts.append(str(row['í…ŒìŠ¤íŠ¸ëª…']))
    
    if 'Year' in row and pd.notna(row['Year']):
        year_int = safe_convert_to_int(row['Year'])
        if year_int:
            parts.append(str(year_int))
    
    if 'Session' in row and pd.notna(row['Session']):
        session_int = safe_convert_to_int(row['Session'])
        if session_int:
            parts.append(f"S{session_int}")
    
    if 'Subject' in row and pd.notna(row['Subject']):
        parts.append(str(row['Subject']))
    
    if 'Number' in row and pd.notna(row['Number']):
        number_int = safe_convert_to_int(row['Number'])
        if number_int:
            parts.append(f"Q{number_int}")
    
    return " / ".join(parts) if parts else "Unknown"

def get_testset_statistics(testsets, test_name, lang='ko'):
    """í…ŒìŠ¤íŠ¸ì…‹ì˜ ê¸°ì´ˆ í†µê³„ ë°˜í™˜"""
    t = LANGUAGES[lang]
    
    if test_name not in testsets:
        return None
    
    df = testsets[test_name]
    stats = {}
    
    # ì´ ë¬¸ì œ ìˆ˜
    stats['total_problems'] = len(df)
    
    # ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜
    if 'law' in df.columns:
        stats['law_problems'] = len(df[df['law'] == 'O'])
        stats['non_law_problems'] = len(df[df['law'] != 'O'])
    
    # ê³¼ëª©ë³„ ë¬¸ì œ ìˆ˜
    if 'Subject' in df.columns:
        stats['by_subject'] = df['Subject'].value_counts().to_dict()
    
    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜
    if 'Year' in df.columns:
        stats['by_year'] = df['Year'].value_counts().sort_index().to_dict()
    
    # ì„¸ì…˜ë³„ ë¬¸ì œ ìˆ˜
    if 'Session' in df.columns:
        stats['by_session'] = df['Session'].value_counts().sort_index().to_dict()
    
    return stats

# ë©”ì¸ ì‹¤í–‰
def main():
    # ğŸ”¥ GitHubì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    download_data_from_github()
    
    # ì–¸ì–´ ì„ íƒ (ì‚¬ì´ë“œë°” ìƒë‹¨ì— ë°°ì¹˜)
    st.sidebar.selectbox(
        "Language / ì–¸ì–´",
        options=['ko', 'en'],
        format_func=lambda x: "í•œêµ­ì–´" if x == 'ko' else "English",
        key='language'
    )
    
    lang = st.session_state.language
    t = LANGUAGES[lang]
    
    # í™”ë©´ ì„¤ì •
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ¨ " + ("í™”ë©´ ì„¤ì •" if lang == 'ko' else "Display Settings"))
    
    # í°íŠ¸ í¬ê¸° ì¡°ì •
    font_size = st.sidebar.slider(
        t['font_size'],
        min_value=0.8,
        max_value=1.5,
        value=1.0,
        step=0.1,
        help="í™”ë©´ ì „ì²´ì˜ í°íŠ¸ í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤"
    )
    
    # ì°¨íŠ¸ í…ìŠ¤íŠ¸ í¬ê¸° ì¡°ì •
    chart_text_size = st.sidebar.slider(
        t['chart_text_size'],
        min_value=0.7,
        max_value=1.8,
        value=1.0,
        step=0.1,
        help="ì°¨íŠ¸ ë‚´ë¶€ í…ìŠ¤íŠ¸, ìˆ«ì, ë ˆì´ë¸” í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤"
    )
    
    apply_custom_css(font_size)
    set_plotly_font_size(chart_text_size)
    
    # ì°¨íŠ¸ ì£¼ì„ í¬ê¸° (íˆíŠ¸ë§µ, í…ìŠ¤íŠ¸ ë“±ì— ì‚¬ìš©)
    annotation_size = int(12 * chart_text_size)
    
    # ì œëª©
    st.title(f"ğŸ¯ {t['title']}")
    st.markdown("---")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ëŠ” í•­ìƒ ./data (GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•œ í´ë”)
    data_dir = "./data"
    
    if not os.path.exists(data_dir):
        st.error(f"Directory not found: {data_dir}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    testsets, results_df = load_data(data_dir)
    
    if results_df.empty:
        st.warning("No data files found in the specified directory.")
        return
    
    # ì •ë‹µì—¬ë¶€ ì»¬ëŸ¼ ì²˜ë¦¬ (CSVì— ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if 'ì •ë‹µì—¬ë¶€' in results_df.columns:
        # ì´ë¯¸ ìˆëŠ” ì •ë‹µì—¬ë¶€ ì»¬ëŸ¼ì„ Booleanìœ¼ë¡œ ë³€í™˜ (ë¬¸ìì—´ 'True'/'False' ì²˜ë¦¬)
        results_df['ì •ë‹µì—¬ë¶€'] = results_df['ì •ë‹µì—¬ë¶€'].apply(
            lambda x: x if isinstance(x, bool) else str(x).strip().lower() == 'true'
        )
    elif 'Answer' in results_df.columns and 'ì˜ˆì¸¡ë‹µ' in results_df.columns:
        # ì •ë‹µì—¬ë¶€ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°
        results_df['ì •ë‹µì—¬ë¶€'] = results_df.apply(
            lambda row: str(row['Answer']).strip() == str(row['ì˜ˆì¸¡ë‹µ']).strip() if pd.notna(row['Answer']) and pd.notna(row['ì˜ˆì¸¡ë‹µ']) else False,
            axis=1
        )
    
    # ì‚¬ì´ë“œë°” í•„í„°
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"## {t['filters']}")
    
    # í…ŒìŠ¤íŠ¸ëª… í•„í„° (multiselectë¡œ ë³€ê²½)
    test_names = sorted(results_df['í…ŒìŠ¤íŠ¸ëª…'].unique().tolist())
    selected_tests = st.sidebar.multiselect(
        t['testname'],
        options=test_names,
        default=test_names,
        help="ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    # í…ŒìŠ¤íŠ¸ ì„ íƒì— ë”°ë¥¸ ë°ì´í„° í•„í„°ë§ (ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ .copy() ìˆ˜í–‰)
    if selected_tests:
        filtered_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
    else:
        filtered_df = results_df
    
    # ========== ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬ ==========
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"### ğŸ¯ {t['ensemble_management']}")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'ensembles' not in st.session_state:
        st.session_state.ensembles = []
    
    # ì•™ìƒë¸” ìƒì„± UI
    with st.sidebar.expander(f"â• {t['create_ensemble']}", expanded=False):
        # ì•™ìƒë¸” ì´ë¦„ ì…ë ¥
        ensemble_name_input = st.text_input(
            t['ensemble_name'],
            value="",
            placeholder="ì˜ˆ: GPT ì•™ìƒë¸”" if lang == 'ko' else "e.g., GPT Ensemble",
            key="ensemble_name_input"
        )
        
        # ëª¨ë¸ ì„ íƒ (ì•™ìƒë¸” ì œì™¸)
        available_models_for_ensemble = sorted([m for m in results_df['ëª¨ë¸'].unique() if 'ğŸ¯' not in str(m)])
        selected_ensemble_models = st.multiselect(
            t['select_models'],
            options=available_models_for_ensemble,
            default=[],
            help=t['min_2_models'],
            key="ensemble_models_select"
        )
        
        # ì•™ìƒë¸” ë°©ë²• ì„ íƒ
        ensemble_method_options = [t['majority_voting'], t['weighted_voting']]
        selected_ensemble_method = st.selectbox(
            t['ensemble_method'],
            options=ensemble_method_options,
            key="ensemble_method_select"
        )
        
        # ì•™ìƒë¸” ì¶”ê°€ ë²„íŠ¼
        if st.button(f"âœ… {t['add_ensemble']}", width='stretch', key="add_ensemble_btn"):
            # ìœ íš¨ì„± ê²€ì‚¬
            if not ensemble_name_input or ensemble_name_input.strip() == "":
                st.error("ì•™ìƒë¸” ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”" if lang == 'ko' else "Please enter ensemble name")
            elif len(selected_ensemble_models) < 2:
                st.error(t['min_2_models'])
            elif any(e['name'] == ensemble_name_input for e in st.session_state.ensembles):
                st.error("ê°™ì€ ì´ë¦„ì˜ ì•™ìƒë¸”ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤" if lang == 'ko' else "Ensemble with same name exists")
            else:
                # ì•™ìƒë¸” ë°©ë²• ë§¤í•‘
                method_key = 'majority' if selected_ensemble_method == t['majority_voting'] else 'weighted'
                
                # ì•™ìƒë¸” ì •ë³´ ì €ì¥
                st.session_state.ensembles.append({
                    'name': f"ğŸ¯ {ensemble_name_input}",  # ì•™ìƒë¸” ì‹ë³„ì„ ìœ„í•´ ì´ëª¨ì§€ ì¶”ê°€
                    'models': selected_ensemble_models.copy(),
                    'method': method_key,
                    'method_display': selected_ensemble_method
                })
                st.success(f"âœ… {t['ensemble_added']}: {ensemble_name_input}")
                st.rerun()
    
    # í˜„ì¬ ì•™ìƒë¸” ëª©ë¡ í‘œì‹œ
    if st.session_state.ensembles:
        st.sidebar.markdown(f"**{t['current_ensembles']}:**")
        
        for idx, ensemble in enumerate(st.session_state.ensembles):
            col1, col2 = st.sidebar.columns([4, 1])
            
            with col1:
                clean_name = ensemble['name'].replace('ğŸ¯ ', '')
                st.sidebar.text(f"{ensemble['name']}")
                st.sidebar.caption(f"  â€¢ {ensemble['method_display']}")
                st.sidebar.caption(f"  â€¢ {len(ensemble['models'])} ëª¨ë¸")
            
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_ensemble_{idx}", help=t['remove_ensemble']):
                    st.session_state.ensembles.pop(idx)
                    st.success(t['ensemble_removed'])
                    st.rerun()
        
        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° ì›ë³¸ ë°ì´í„°ì— í†µí•©
        ensemble_dfs = []
        for ensemble in st.session_state.ensembles:
            ensemble_df = create_ensemble_model(
                results_df,
                ensemble['name'],
                ensemble['models'],
                ensemble['method']
            )
            if not ensemble_df.empty:
                ensemble_dfs.append(ensemble_df)
        
        # ì•™ìƒë¸” ë°ì´í„°ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ í†µí•© ë°ì´í„° ìƒì„±
        if ensemble_dfs:
            # results_dfì— ì•™ìƒë¸” ë°ì´í„° ì¶”ê°€ (ì›ë³¸ì€ ìœ ì§€, í†µí•© ë°ì´í„°ëŠ” ë³„ë„)
            integrated_df = pd.concat([results_df] + ensemble_dfs, ignore_index=True)
            
            # filtered_dfë¥¼ integrated_df ê¸°ë°˜ìœ¼ë¡œ ì¬ìƒì„± (ë§ˆì§€ë§‰ì—ë§Œ copy)
            if selected_tests:
                filtered_df = integrated_df[integrated_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
            else:
                filtered_df = integrated_df
            
            st.sidebar.success(f"ğŸ¯ {len(st.session_state.ensembles)}ê°œ ì•™ìƒë¸” í™œì„±" if lang == 'ko' else f"ğŸ¯ {len(st.session_state.ensembles)} ensemble(s) active")
        else:
            # ì•™ìƒë¸” ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            if selected_tests:
                filtered_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
            else:
                filtered_df = results_df
    else:
        st.sidebar.info(t['no_ensembles'])
    
    # ========== ì•™ìƒë¸” ê´€ë¦¬ ë ==========
    
    # ëª¨ë¸ í•„í„°
    models = sorted(filtered_df['ëª¨ë¸'].unique().tolist())
    selected_models = st.sidebar.multiselect(
        t['model'],
        options=models,
        default=models
    )
    
    if selected_models:
        filtered_df = filtered_df[filtered_df['ëª¨ë¸'].isin(selected_models)]
    
    # ìƒì„¸ë„ í•„í„° (multiselectë¡œ ë³€ê²½)
    details = sorted(filtered_df['ìƒì„¸ë„'].unique().tolist())
    selected_details = st.sidebar.multiselect(
        t['detail_type'],
        options=details,
        default=details,
        help="ì—¬ëŸ¬ ìƒì„¸ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_details:
        filtered_df = filtered_df[filtered_df['ìƒì„¸ë„'].isin(selected_details)]
    
    # í”„ë¡¬í”„íŒ… ë°©ì‹ í•„í„° (multiselectë¡œ ë³€ê²½)
    prompts = sorted(filtered_df['í”„ë¡¬í”„íŒ…'].unique().tolist())
    selected_prompts = st.sidebar.multiselect(
        t['prompting'],
        options=prompts,
        default=prompts,
        help="ì—¬ëŸ¬ í”„ë¡¬í”„íŒ… ë°©ì‹ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_prompts:
        filtered_df = filtered_df[filtered_df['í”„ë¡¬í”„íŒ…'].isin(selected_prompts)]
    
    # ì„¸ì…˜ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œ, multiselectë¡œ ë³€ê²½)
    if selected_tests:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì›ë³¸ ë°ì´í„°ì—ì„œ ì„¸ì…˜ ì¶”ì¶œ
        available_sessions = get_available_sessions(results_df, selected_tests)
        if available_sessions:
            selected_sessions = st.sidebar.multiselect(
                t['session'],
                options=available_sessions,
                default=available_sessions,
                help="ì—¬ëŸ¬ ì„¸ì…˜ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            )
            
            if selected_sessions:
                # ì„ íƒëœ ì„¸ì…˜ê³¼ ë§¤ì¹­ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)
                def match_session(x):
                    if pd.isna(x):
                        return False
                    
                    # xë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ ì‹œë„
                    x_int = safe_convert_to_int(x)
                    
                    # ì„ íƒëœ ì„¸ì…˜ì— ì •ìˆ˜ë¡œ ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                    if x_int is not None and x_int in selected_sessions:
                        return True
                    
                    # ë¬¸ìì—´ë¡œ ì§ì ‘ ë¹„êµ
                    if isinstance(x, str):
                        x_clean = x.strip()
                        return x_clean in selected_sessions
                    
                    return False
                
                filtered_df = filtered_df[filtered_df['Session'].apply(match_session)]
    
    # ë¬¸ì œ ìœ í˜• í•„í„°
    if 'image' in filtered_df.columns:
        problem_types = [t['all'], t['image_problem'], t['text_only']]
        selected_problem_type = st.sidebar.selectbox(
            t['problem_type'],
            options=problem_types
        )
        
        if selected_problem_type == t['image_problem']:
            filtered_df = filtered_df[filtered_df['image'] != 'text_only']
        elif selected_problem_type == t['text_only']:
            filtered_df = filtered_df[filtered_df['image'] == 'text_only']
    
    # ì—°ë„ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•˜ì—¬ ëª¨ë“  ì—°ë„ í‘œì‹œ)
    if 'Year' in results_df.columns:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì—°ë„ë§Œ í‘œì‹œ
        if selected_tests:
            year_source_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
        else:
            year_source_df = results_df
        
        # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        years_raw = year_source_df['Year'].dropna().unique().tolist()
        years_int = []
        for y in years_raw:
            y_int = safe_convert_to_int(y)
            if y_int and y_int not in years_int:
                years_int.append(y_int)
        years = safe_sort(years_int)
        
        if years:
            selected_years = st.sidebar.multiselect(
                t['year'],
                options=years,
                default=years
            )
            
            if selected_years:
                # ì„ íƒëœ ì—°ë„ì™€ ë§¤ì¹­ë˜ëŠ” ì›ë³¸ ë°ì´í„° í•„í„°ë§
                filtered_df = filtered_df[filtered_df['Year'].apply(
                    lambda x: safe_convert_to_int(x) in selected_years if pd.notna(x) else False
                )]
    
    # ë²•ë ¹ êµ¬ë¶„ í•„í„°
    if 'law' in filtered_df.columns:
        law_options = [t['all'], t['law'], t['non_law']]
        selected_law = st.sidebar.selectbox(
            t['law_type'],
            options=law_options
        )
        
        if selected_law == t['law']:
            filtered_df = filtered_df[filtered_df['law'] == 'O']
        elif selected_law == t['non_law']:
            filtered_df = filtered_df[filtered_df['law'] != 'O']
    
    # í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    if filtered_df.empty:
        st.warning("No data matches the selected filters.")
        return
    
    # íƒ­ ìƒì„±
    tabs = st.tabs([
        f"ğŸ“Š {t['overview']}",
        f"ğŸ” {t['model_comparison']}",
        f"â±ï¸ {t['response_time_analysis']}",
        f"âš–ï¸ {t['law_analysis']}",
        f"ğŸ“š {t['subject_analysis']}",
        f"ğŸ“… {t['year_analysis']}",
        f"âŒ {t['incorrect_analysis']}",
        f"ğŸ“ˆ {t['difficulty_analysis']}",
        f"ğŸ’° {t['token_cost_analysis']}",
        f"ğŸ“‹ {t['testset_stats']}",
        "ğŸ“‘ " + ("ì¶”ê°€ ë¶„ì„" if lang == 'ko' else "Additional Analysis")
    ])
    
    # íƒ­ 1: ì „ì²´ ìš”ì•½
    with tabs[0]:
        st.header(f"ğŸ“Š {t['overview']}")
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚° (ë°ì´í„° í–‰ ìˆ˜ / ëª¨ë¸ ìˆ˜)
        num_models = filtered_df['ëª¨ë¸'].nunique()
        total_problems = round(len(filtered_df) / num_models) if num_models > 0 else 0
        
        # ê³ ìœ  ë¬¸ì œ ìˆ˜ëŠ” filtered_dfì—ì„œ ì¤‘ë³µ ì œê±° (ë°±ì—…ìš©)
        unique_questions = filtered_df['Question'].nunique()
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë³¸ ì •ë³´
        st.subheader("ğŸ“‹ " + ("í…ŒìŠ¤íŠ¸ì…‹ ì •ë³´" if lang == 'ko' else "Test Set Information"))
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # ì´ ë¬¸ì œ ìˆ˜ = ë°ì´í„° í–‰ ìˆ˜ / ëª¨ë¸ ìˆ˜ (Vercelê³¼ ë™ì¼í•œ ë°©ì‹)
            st.metric(
                "ì´ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Total Problems",
                f"{total_problems:,}"
            )
        with col2:
            st.metric(
                "í‰ê°€ ëª¨ë¸ ìˆ˜" if lang == 'ko' else "Number of Models",
                f"{num_models}"
            )
        with col3:
            # ìˆ˜ì •: ì´ í‰ê°€ íšŸìˆ˜ = í•„í„°ëœ ë°ì´í„° í–‰ ìˆ˜
            st.metric(
                "ì´ í‰ê°€ íšŸìˆ˜" if lang == 'ko' else "Total Evaluations",
                f"{len(filtered_df):,}"
            )
        
        st.markdown("---")
        
        # ëª¨ë¸ í‰ê·  ì„±ëŠ¥
        st.subheader("ğŸ¯ " + ("ëª¨ë¸ í‰ê·  ì„±ëŠ¥" if lang == 'ko' else "Average Model Performance"))
        col1, col2, col3, col4 = st.columns(4)
        
        # ëª¨ë¸ë³„ ì •í™•ë„ ê³„ì‚° í›„ í‰ê· 
        model_accuracies = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean()
        avg_accuracy = model_accuracies.mean() * 100
        
        # í‰ê·  ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ (ëª¨ë¸ë‹¹)
        avg_problems_per_model = total_problems  # ëª¨ë¸ë‹¹ í‰ê°€í•œ ë¬¸ì œ ìˆ˜
        avg_correct = (avg_problems_per_model * avg_accuracy / 100) if avg_problems_per_model > 0 else 0
        avg_wrong = avg_problems_per_model - avg_correct
        
        with col1:
            st.metric(
                "í‰ê·  ì •í™•ë„" if lang == 'ko' else "Average Accuracy",
                f"{avg_accuracy:.2f}%"
            )
        with col2:
            st.metric(
                "ëª¨ë¸ë‹¹ í‰ê·  ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Avg Problems per Model",
                f"{avg_problems_per_model:.0f}"
            )
        with col3:
            st.metric(
                "í‰ê·  ì •ë‹µ ìˆ˜" if lang == 'ko' else "Avg Correct Answers",
                f"{avg_correct:.0f}"
            )
        with col4:
            st.metric(
                "í‰ê·  ì˜¤ë‹µ ìˆ˜" if lang == 'ko' else "Avg Wrong Answers",
                f"{avg_wrong:.0f}"
            )
        
        # ë²•ë ¹/ë¹„ë²•ë ¹ í†µê³„
        if 'law' in filtered_df.columns:
            st.markdown("---")
            st.subheader("âš–ï¸ " + ("ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„" if lang == 'ko' else "Law/Non-Law Analysis"))
            
            # ğŸ”¥ ì¼ê´€ì„±ì„ ìœ„í•´ í•­ìƒ í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜ ê³„ì‚°
            law_count = 0
            non_law_count = 0
            
            if selected_tests:
                for test_name in selected_tests:
                    if test_name in testsets:
                        test_df = testsets[test_name]
                        if 'law' in test_df.columns:
                            law_count += len(test_df[test_df['law'] == 'O'])
                            non_law_count += len(test_df[test_df['law'] != 'O'])
                        else:
                            # law ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ë¹„ë²•ë ¹ìœ¼ë¡œ ê°„ì£¼
                            non_law_count += len(test_df)
            else:
                # ì„ íƒëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ëª¨ë“  í…ŒìŠ¤íŠ¸ì…‹ í•©ì‚°
                for test_name, test_df in testsets.items():
                    if 'law' in test_df.columns:
                        law_count += len(test_df[test_df['law'] == 'O'])
                        non_law_count += len(test_df[test_df['law'] != 'O'])
                    else:
                        non_law_count += len(test_df)
            
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  (ëª¨ë“  ëª¨ë¸ í‰ê· )
            law_df = filtered_df[filtered_df['law'] == 'O']
            non_law_df = filtered_df[filtered_df['law'] != 'O']
            
            law_accuracy = (law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(law_df) > 0 else 0
            non_law_accuracy = (non_law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(non_law_df) > 0 else 0
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(t['law_problems'], f"{law_count:,}")
            with col2:
                st.metric(f"{t['law']} {t['correct_rate']}", f"{law_accuracy:.2f}%")
            with col3:
                st.metric(t['non_law_problems'], f"{non_law_count:,}")
            with col4:
                st.metric(f"{t['non_law']} {t['correct_rate']}", f"{non_law_accuracy:.2f}%")
        
        # ì‹œê°í™” ì°¨íŠ¸ ì¶”ê°€
        st.markdown("---")
        st.subheader("ğŸ“Š " + ("ì£¼ìš” ì§€í‘œ ì‹œê°í™”" if lang == 'ko' else "Key Metrics Visualization"))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„ ë°” ì°¨íŠ¸
            model_acc_df = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
            model_acc_df.columns = [t['model'], t['accuracy']]
            model_acc_df[t['accuracy']] = model_acc_df[t['accuracy']] * 100
            model_acc_df = model_acc_df.sort_values(t['accuracy'], ascending=False)
            
            fig = px.bar(
                model_acc_df,
                x=t['model'],
                y=t['accuracy'],
                title=t['avg_accuracy_by_model'],
                text=t['accuracy'],
                color=t['accuracy'],
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model'],
                yaxis=dict(range=[0, 100])
            )
            st.plotly_chart(fig, width='stretch')
        
        with col2:
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ ì°¨íŠ¸
            if 'law' in filtered_df.columns:
                law_comparison = pd.DataFrame({
                    'êµ¬ë¶„': [t['law'], t['non_law']],
                    'ì •ë‹µë¥ ': [law_accuracy, non_law_accuracy],
                    'ë¬¸ì œìˆ˜': [law_count, non_law_count]
                })
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name=t['correct_rate'] if lang == 'ko' else 'Accuracy',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ì •ë‹µë¥ '],
                    text=law_comparison['ì •ë‹µë¥ '].round(1),
                    texttemplate='%{text}%',
                    textposition='outside',                textfont=dict(size=annotation_size),
                    marker_color=['#FF6B6B', '#4ECDC4'],
                    marker_line_color='black',
                    marker_line_width=1.5,
                    yaxis='y'
                ))
                
                fig.add_trace(go.Scatter(
                    name=t['problem_count'] if lang == 'ko' else 'Problem Count',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ë¬¸ì œìˆ˜'],
                    text=law_comparison['ë¬¸ì œìˆ˜'],
                    texttemplate='%{text}ê°œ',
                    textposition='top center',                textfont=dict(size=annotation_size),
                    mode='lines+markers+text',
                    marker=dict(size=10, color='orange'),
                    line=dict(width=2, color='orange'),
                    yaxis='y2'
                ))
                
                fig.update_layout(
                    title='ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë° ë¬¸ì œ ìˆ˜ ë¹„êµ' if lang == 'ko' else 'Law/Non-Law Accuracy and Problem Count Comparison',
                    height=400,
                    yaxis=dict(
                        title=('ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)'),
                        range=[0, 100]
                    ),
                    yaxis2=dict(
                        title=(t['problem_count'] if lang == 'ko' else 'Problem Count'),
                        overlaying='y',
                        side='right',
                        range=[0, max(law_count, non_law_count) * 1.2]
                    ),
                    hovermode='x unified',
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                st.plotly_chart(fig, width='stretch')
            else:
                # ë²•ë ¹ ì •ë³´ê°€ ì—†ì„ ë•Œ - ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ ì°¨íŠ¸
                model_correct_wrong = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].agg(['sum', 'count']).reset_index()
                model_correct_wrong.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ']
                model_correct_wrong['ì˜¤ë‹µ'] = model_correct_wrong['ì´ë¬¸ì œ'] - model_correct_wrong['ì •ë‹µ']
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name='ì •ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì •ë‹µ'],
                    marker_color='lightgreen'
                ))
                fig.add_trace(go.Bar(
                    name='ì˜¤ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì˜¤ë‹µ'],
                    marker_color='lightcoral'
                ))
                
                fig.update_layout(
                    barmode='stack',
                    title='ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜',
                    height=400,
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    xaxis_title='ëª¨ë¸'
                )
                st.plotly_chart(fig, width='stretch')
        
        # í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¶„í¬ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°)
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns and filtered_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ğŸ”¥ í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ ê¸°ì¤€)
                test_problem_data = []
                for test_name in selected_tests if selected_tests else testsets.keys():
                    if test_name in testsets:
                        problem_count = len(testsets[test_name])
                        test_problem_data.append({'í…ŒìŠ¤íŠ¸ëª…': test_name, 'ë¬¸ì œìˆ˜': problem_count})
                
                test_problem_count = pd.DataFrame(test_problem_data)
                test_problem_count = test_problem_count.sort_values('ë¬¸ì œìˆ˜', ascending=False)
                
                fig = px.bar(
                    test_problem_count,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ë¬¸ì œìˆ˜',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Problems by Test',
                    text='ë¬¸ì œìˆ˜',
                    color='ë¬¸ì œìˆ˜',
                    color_continuous_scale='Blues'
                )
                fig.update_traces(
                    textposition='outside',
                    marker_line_color='black',
                    marker_line_width=1.5
                )
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Problem Count',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name'
                )
                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                # í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„
                test_accuracy = filtered_df.groupby('í…ŒìŠ¤íŠ¸ëª…')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                test_accuracy.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ì •í™•ë„']
                test_accuracy['ì •í™•ë„'] = test_accuracy['ì •í™•ë„'] * 100
                test_accuracy = test_accuracy.sort_values('ì •í™•ë„', ascending=False)
                
                fig = px.bar(
                    test_accuracy,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ì •í™•ë„',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„',
                    text='ì •í™•ë„',
                    color='ì •í™•ë„',
                    color_continuous_scale='RdYlGn'
                )
                fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ì •í™•ë„ (%)',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…',
                    yaxis=dict(range=[0, 100])
                )
                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                st.plotly_chart(fig, width='stretch')
        
        # ì¢…í•© ì¸ì‚¬ì´íŠ¸
        st.markdown("---")
        st.subheader("ğŸ’¡ " + ("ì¢…í•© ì¸ì‚¬ì´íŠ¸" if lang == 'ko' else "Key Insights"))
        
        # ìµœê³ /ìµœì € ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model = model_acc_df.iloc[0]
        worst_model = model_acc_df.iloc[-1]
        performance_gap = best_model[t['accuracy']] - worst_model[t['accuracy']]
        
        # ë²•ë ¹ vs ë¹„ë²•ë ¹ ì°¨ì´
        if 'law' in filtered_df.columns and law_count > 0 and non_law_count > 0:
            law_difficulty = "ë” ì–´ë ¤ì›€" if law_accuracy < non_law_accuracy else "ë” ì‰¬ì›€" if law_accuracy > non_law_accuracy else "ë¹„ìŠ·í•¨"
            law_diff_pct = abs(law_accuracy - non_law_accuracy)
            
            st.success(f"""
            ğŸ“Š **{"ì„±ëŠ¥ ë¶„ì„" if lang == 'ko' else "Performance Analysis"}**:
            - **{"ìµœê³  ì„±ëŠ¥ ëª¨ë¸" if lang == 'ko' else "Top Model"}**: {best_model[t['model']]} ({best_model[t['accuracy']]:.2f}%)
            - **{"ìµœì € ì„±ëŠ¥ ëª¨ë¸" if lang == 'ko' else "Lowest Model"}**: {worst_model[t['model']]} ({worst_model[t['accuracy']]:.2f}%)
            - **{"ì„±ëŠ¥ ê²©ì°¨" if lang == 'ko' else "Performance Gap"}**: {performance_gap:.2f}%p
            
            âš–ï¸ **{"ë²•ë ¹ ë¬¸ì œ ë¶„ì„" if lang == 'ko' else "Law Problem Analysis"}**:
            - {"ë²•ë ¹ ë¬¸ì œê°€" if lang == 'ko' else "Law problems are"} **{law_difficulty}** ({"ì°¨ì´" if lang == 'ko' else "difference"}: {law_diff_pct:.2f}%p)
            - {"ë²•ë ¹ ë¬¸ì œ ì •ë‹µë¥ " if lang == 'ko' else "Law accuracy"}: {law_accuracy:.2f}% vs {"ë¹„ë²•ë ¹" if lang == 'ko' else "Non-law"}: {non_law_accuracy:.2f}%
            
            ğŸ“ˆ **{"ì¶”ì²œ ì‚¬í•­" if lang == 'ko' else "Recommendations"}**:
            - {"ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš°" if lang == 'ko' else "For high accuracy needs"}: {best_model[t['model']]} {"ì‚¬ìš©" if lang == 'ko' else "recommended"}
            - {"ë²•ë ¹ ë¬¸ì œ íŠ¹í™”ê°€ í•„ìš”í•œ ê²½ìš°" if lang == 'ko' else "For law-specific tasks"}: {"ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„ íƒ­ì—ì„œ ì„¸ë¶€ ì„±ëŠ¥ í™•ì¸" if lang == 'ko' else "Check detailed performance in Law Analysis tab"}
            """)
        else:
            st.success(f"""
            ğŸ“Š **{"ì„±ëŠ¥ ë¶„ì„" if lang == 'ko' else "Performance Analysis"}**:
            - **{"ìµœê³  ì„±ëŠ¥ ëª¨ë¸" if lang == 'ko' else "Top Model"}**: {best_model[t['model']]} ({best_model[t['accuracy']]:.2f}%)
            - **{"ìµœì € ì„±ëŠ¥ ëª¨ë¸" if lang == 'ko' else "Lowest Model"}**: {worst_model[t['model']]} ({worst_model[t['accuracy']]:.2f}%)
            - **{"ì„±ëŠ¥ ê²©ì°¨" if lang == 'ko' else "Performance Gap"}**: {performance_gap:.2f}%p
            
            ğŸ“ˆ **{"ì¶”ì²œ ì‚¬í•­" if lang == 'ko' else "Recommendations"}**:
            - {"ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš°" if lang == 'ko' else "For high accuracy needs"}: {best_model[t['model']]} {"ì‚¬ìš© ê¶Œì¥" if lang == 'ko' else "recommended"}
            - {"í‰ê·  ì„±ëŠ¥" if lang == 'ko' else "Average performance"}: {avg_accuracy:.2f}% - {"ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ" if lang == 'ko' else "use as baseline for model selection"}
            """)
    
    # íƒ­ 2: ëª¨ë¸ë³„ ë¹„êµ
    with tabs[1]:
        st.header(f"ğŸ” {t['model_comparison']}")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
        model_stats = filtered_df.groupby('ëª¨ë¸').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        model_stats.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
        model_stats['ì •í™•ë„'] = model_stats['ì •í™•ë„'] * 100
        model_stats['ì˜¤ë‹µ'] = model_stats['ì´ë¬¸ì œ'] - model_stats['ì •ë‹µ']
        model_stats = model_stats.sort_values('ì •í™•ë„', ascending=False)
        
        # ì„±ëŠ¥ ì§€í‘œ í…Œì´ë¸”
        st.subheader(t['performance_by_model'])
        
        # í…Œì´ë¸” ì»¬ëŸ¼ëª… ë³€ê²½
        display_stats = model_stats.copy()
        if lang == 'en':
            display_stats.columns = ['Model', 'Correct', 'Total', 'Accuracy', 'Wrong']
        
        st.dataframe(
            display_stats.style.format({
                'ì •í™•ë„' if lang == 'ko' else 'Accuracy': '{:.2f}%'
            }).background_gradient(subset=['ì •í™•ë„' if lang == 'ko' else 'Accuracy'], cmap='RdYlGn'),
            width='stretch'
        )
        
        # ë¹„êµ ì°¨íŠ¸
        st.markdown("---")
        st.subheader(t['comparison_chart'])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ì •í™•ë„ ë°” ì°¨íŠ¸
            fig = px.bar(
                model_stats,
                x='ëª¨ë¸',
                y='ì •í™•ë„',
                title=t['overall_comparison'],
                text='ì •í™•ë„',
                color='ì •í™•ë„',
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, width='stretch')
        
        with col2:
            # ì •ë‹µ/ì˜¤ë‹µ ìŠ¤íƒ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['correct'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì •ë‹µ'],
                marker_color='lightgreen',
                marker_line_color='black',
                marker_line_width=1.5
            ))
            fig.add_trace(go.Bar(
                name=t['wrong'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì˜¤ë‹µ'],
                marker_color='lightcoral',
                marker_line_color='black',
                marker_line_width=1.5
            ))
            
            fig.update_layout(
                barmode='stack',
                title=f"{t['correct']}/{t['wrong']} {t['comparison_chart']}",
                height=400,
                yaxis_title=t['problem_count'],
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, width='stretch')
        
        # íˆíŠ¸ë§µ
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns:
            st.markdown("---")
            st.subheader(t['heatmap'])
            
            # ëª¨ë¸ë³„, í…ŒìŠ¤íŠ¸ë³„ ì •í™•ë„ ê³„ì‚°
            heatmap_data = filtered_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            heatmap_pivot = heatmap_data.unstack(fill_value=0)
            
            # íˆíŠ¸ë§µ ìƒì„± (ìˆ«ì í‘œì‹œ ë° ì…€ ê²½ê³„ì„  ì¶”ê°€)
            fig = go.Figure(data=go.Heatmap(
                z=heatmap_pivot.values,
                x=heatmap_pivot.columns,
                y=heatmap_pivot.index,
                colorscale='RdYlGn',
                text=np.round(heatmap_pivot.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": annotation_size},
                colorbar=dict(title=t['accuracy'] + " (%)"),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            # íˆíŠ¸ë§µ ì¸ì‚¬ì´íŠ¸
            st.info(f"""
            ğŸ’¡ **{"íˆíŠ¸ë§µ ë¶„ì„" if lang == 'ko' else "Heatmap Analysis"}**:
            - **{"ê°€ì¥ ì–´ë ¤ìš´ í…ŒìŠ¤íŠ¸" if lang == 'ko' else "Hardest Test"}**: {heatmap_pivot.mean(axis=0).idxmin()} ({"í‰ê· " if lang == 'ko' else "avg"}: {heatmap_pivot.mean(axis=0).min():.1f}%)
            - **{"ê°€ì¥ ì‰¬ìš´ í…ŒìŠ¤íŠ¸" if lang == 'ko' else "Easiest Test"}**: {heatmap_pivot.mean(axis=0).idxmax()} ({"í‰ê· " if lang == 'ko' else "avg"}: {heatmap_pivot.mean(axis=0).max():.1f}%)
            - **{"ì¼ê´€ì„±" if lang == 'ko' else "Consistency"}**: {"ëª¨ë“  ëª¨ë¸ì´ ë¹„ìŠ·í•œ ì„±ëŠ¥ íŒ¨í„´ì„ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”" if lang == 'ko' else "Check if all models show similar performance patterns"}
            - **{"íŠ¹í™” ì˜ì—­" if lang == 'ko' else "Specialization"}**: {"íŠ¹ì • ëª¨ë¸ì´ íŠ¹ì • í…ŒìŠ¤íŠ¸ì—ì„œ íŠ¹íˆ ìš°ìˆ˜í•œì§€ íŒŒì•…í•˜ì„¸ìš”" if lang == 'ko' else "Identify if specific models excel in certain tests"}
            """)
    
    # íƒ­ 3: ì‘ë‹µì‹œê°„ ë¶„ì„
    with tabs[2]:
        st.header(f"â±ï¸ {t['response_time_analysis']}")
        
        # ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ) ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        time_columns = ['ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)', 'ì´ì†Œìš”ì‹œê°„(ì´ˆ)', 'question_duration']
        available_time_col = None
        for col in time_columns:
            if col in filtered_df.columns:
                available_time_col = col
                break
        
        if available_time_col is None:
            st.info("Response time data not available in the dataset.")
        else:
            # ì‘ë‹µì‹œê°„ ë°ì´í„° ì¤€ë¹„
            if available_time_col == 'question_duration':
                # question_durationì€ ê°œë³„ ë¬¸ì œ ì‹œê°„
                time_col = 'question_duration'
                is_per_problem = True
            elif available_time_col == 'ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)':
                time_col = 'ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)'
                is_per_problem = True
            else:
                time_col = 'ì´ì†Œìš”ì‹œê°„(ì´ˆ)'
                is_per_problem = False
            
            # NaN ê°’ ì œê±° (viewë§Œ ìƒì„±, copy ë¶ˆí•„ìš”)
            time_df = filtered_df[filtered_df[time_col].notna()]
            
            if len(time_df) == 0:
                st.info("No valid response time data available.")
            else:
                # 1. ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µì‹œê°„ í†µê³„
                st.subheader(t['response_time_stats'])
                
                model_time_stats = time_df.groupby('ëª¨ë¸').agg({
                    time_col: ['mean', 'median', 'std', 'min', 'max', 'count']
                }).reset_index()
                
                model_time_stats.columns = ['ëª¨ë¸', 'í‰ê· ', 'ì¤‘ì•™ê°’', 'í‘œì¤€í¸ì°¨', 'ìµœì†Œ', 'ìµœëŒ€', 'ë¬¸ì œìˆ˜']
                model_time_stats = model_time_stats.sort_values('í‰ê· ')
                
                # ì •í™•ë„ë„ í•¨ê»˜ í‘œì‹œ
                model_acc = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                model_acc.columns = ['ëª¨ë¸', 'ì •í™•ë„']
                model_acc['ì •í™•ë„'] = model_acc['ì •í™•ë„'] * 100
                
                model_time_stats = model_time_stats.merge(model_acc, on='ëª¨ë¸')
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    fastest = model_time_stats.iloc[0]
                    st.metric(
                        t['fastest_model'],
                        fastest['ëª¨ë¸'],
                        f"{fastest['í‰ê· ']:.2f}{t['seconds']}"
                    )
                
                with col2:
                    slowest = model_time_stats.iloc[-1]
                    st.metric(
                        t['slowest_model'],
                        slowest['ëª¨ë¸'],
                        f"{slowest['í‰ê· ']:.2f}{t['seconds']}"
                    )
                
                with col3:
                    avg_time = model_time_stats['í‰ê· '].mean()
                    st.metric(
                        t['avg_response_time'],
                        f"{avg_time:.2f}{t['seconds']}"
                    )
                
                # í…Œì´ë¸”
                st.dataframe(
                    model_time_stats.style.format({
                        'í‰ê· ': '{:.2f}',
                        'ì¤‘ì•™ê°’': '{:.2f}',
                        'í‘œì¤€í¸ì°¨': '{:.2f}',
                        'ìµœì†Œ': '{:.2f}',
                        'ìµœëŒ€': '{:.2f}',
                        'ë¬¸ì œìˆ˜': '{:.0f}',
                        'ì •í™•ë„': '{:.2f}%'
                    }).background_gradient(subset=['í‰ê· '], cmap='RdYlGn_r'),
                    width='stretch'
                )
                
                st.markdown("---")
                
                # 2. ì‹œê°í™”
                st.subheader(t['response_time_by_model'])
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # í‰ê·  ì‘ë‹µì‹œê°„ ë°” ì°¨íŠ¸
                    fig = px.bar(
                        model_time_stats,
                        x='ëª¨ë¸',
                        y='í‰ê· ',
                        title=t['avg_response_time'] + (' (' + t['time_per_problem'] + ')' if is_per_problem else ''),
                        text='í‰ê· ',
                        color='í‰ê· ',
                        color_continuous_scale='RdYlGn_r'
                    )
                    fig.update_traces(
                        texttemplate='%{text:.2f}s',
                        textposition='outside',                textfont=dict(size=annotation_size),
                        marker_line_color='black',
                        marker_line_width=1.5
                    )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')',
                        xaxis_title=t['model']
                    )
                    st.plotly_chart(fig, width='stretch')
                
                with col2:
                    # ë°•ìŠ¤í”Œë¡¯
                    fig = px.box(
                        time_df,
                        x='ëª¨ë¸',
                        y=time_col,
                        title=t['response_time_distribution'],
                        color='ëª¨ë¸'
                    )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')',
                        xaxis_title=t['model']
                    )
                    fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                    st.plotly_chart(fig, width='stretch')
                
                st.markdown("---")
                
                # 3. ì‘ë‹µì‹œê°„ vs ì •í™•ë„
                st.subheader(t['response_time_vs_accuracy'])
                
                fig = px.scatter(
                    model_time_stats,
                    x='í‰ê· ',
                    y='ì •í™•ë„',
                    size='ë¬¸ì œìˆ˜',
                    text='ëª¨ë¸',
                    title=t['response_time_vs_accuracy'],
                    labels={
                        'í‰ê· ': t['avg_response_time'] + ' (' + t['seconds'] + ')',
                        'ì •í™•ë„': t['accuracy'] + ' (%)'
                    }
                )
                fig.update_traces(
                    textposition='top center',
                    marker=dict(
                        line=dict(width=2, color='black'),
                        opacity=0.7
                    )
                )
                fig.update_layout(height=500)
                st.plotly_chart(fig, width='stretch')
                
                # ì¸ì‚¬ì´íŠ¸ ê°œì„ 
                speed_accuracy_ratio = fastest['ì •í™•ë„'] / slowest['ì •í™•ë„'] if slowest['ì •í™•ë„'] > 0 else 0
                time_ratio = slowest['í‰ê· '] / fastest['í‰ê· '] if fastest['í‰ê· '] > 0 else 0
                
                st.info(f"""
                ğŸ’¡ **{"ì†ë„ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„" if lang == 'ko' else "Speed vs Accuracy Trade-off Analysis"}**:
                
                ğŸƒ **{"ì†ë„" if lang == 'ko' else "Speed"}**:
                - **{"ìµœê³ ì†" if lang == 'ko' else "Fastest"}**: {fastest['ëª¨ë¸']} ({fastest['í‰ê· ']:.2f}{"ì´ˆ" if lang == 'ko' else "s"}, {"ì •í™•ë„" if lang == 'ko' else "accuracy"} {fastest['ì •í™•ë„']:.1f}%)
                - **{"ìµœì €ì†" if lang == 'ko' else "Slowest"}**: {slowest['ëª¨ë¸']} ({slowest['í‰ê· ']:.2f}{"ì´ˆ" if lang == 'ko' else "s"}, {"ì •í™•ë„" if lang == 'ko' else "accuracy"} {slowest['ì •í™•ë„']:.1f}%)
                - **{"ì†ë„ ì°¨ì´" if lang == 'ko' else "Speed difference"}**: {time_ratio:.1f}x
                
                ğŸ¯ **{"íš¨ìœ¨ì„± ë¶„ì„" if lang == 'ko' else "Efficiency Analysis"}**:
                - {"ë¹ ë¥¸ ëª¨ë¸ì´" if lang == 'ko' else "Fast model is"} {speed_accuracy_ratio:.2f}x {"ì˜ ì •í™•ë„ë¥¼ ê°€ì§" if lang == 'ko' else "as accurate"}
                - **{"ê¶Œì¥ì‚¬í•­" if lang == 'ko' else "Recommendation"}**: {"ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ë©´" if lang == 'ko' else "For real-time: "}{fastest['ëª¨ë¸']}, {"ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë©´" if lang == 'ko' else "For accuracy: "}{slowest['ëª¨ë¸'] if slowest['ì •í™•ë„'] > fastest['ì •í™•ë„'] else fastest['ëª¨ë¸']}
                
                ğŸ“Š **{"ì‚°ì ë„ í™œìš©íŒ" if lang == 'ko' else "Scatter Plot Insights"}**:
                - {"ì™¼ìª½ ìœ„" if lang == 'ko' else "Top-left"}: {"ë¹ ë¥´ê³  ì •í™•í•¨ (ì´ìƒì )" if lang == 'ko' else "Fast & Accurate (ideal)"}
                - {"ì˜¤ë¥¸ìª½ ì•„ë˜" if lang == 'ko' else "Bottom-right"}: {"ëŠë¦¬ê³  ë¶€ì •í™•í•¨ (í”¼í•´ì•¼ í•¨)" if lang == 'ko' else "Slow & Inaccurate (avoid)"}
                """)
                
                st.markdown("---")
                
                # 4. í…ŒìŠ¤íŠ¸ë³„ ì‘ë‹µì‹œê°„ (í…ŒìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
                if 'í…ŒìŠ¤íŠ¸ëª…' in time_df.columns and time_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
                    st.subheader(f"{t['response_time']} ({t['by_test']})" if 'by_test' in t else "í…ŒìŠ¤íŠ¸ë³„ ì‘ë‹µì‹œê°„")
                    
                    test_time = time_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])[time_col].mean().reset_index()
                    test_time.columns = ['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…', 'í‰ê· ì‹œê°„']
                    
                    fig = px.bar(
                        test_time,
                        x='í…ŒìŠ¤íŠ¸ëª…',
                        y='í‰ê· ì‹œê°„',
                        color='ëª¨ë¸',
                        barmode='group',
                        title='í…ŒìŠ¤íŠ¸ë³„ ëª¨ë¸ ì‘ë‹µì‹œê°„' if lang == 'ko' else 'Response Time by Test',
                        labels={'í‰ê· ì‹œê°„': t['avg_response_time'] + ' (' + t['seconds'] + ')'}
                    )
                    fig.update_layout(
                        height=400,
                        xaxis_title=t['testname'],
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')'
                    )
                    fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                    st.plotly_chart(fig, width='stretch')
                    
                    # í…ŒìŠ¤íŠ¸ë³„ ì¸ì‚¬ì´íŠ¸
                    hardest_test = test_time.groupby('í…ŒìŠ¤íŠ¸ëª…')['í‰ê· ì‹œê°„'].mean().idxmax()
                    easiest_test = test_time.groupby('í…ŒìŠ¤íŠ¸ëª…')['í‰ê· ì‹œê°„'].mean().idxmin()
                    st.success(f"""
                    ğŸ“Š **{"í…ŒìŠ¤íŠ¸ë³„ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„" if lang == 'ko' else "Processing Time by Test"}**:
                    - **{"ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¬ëŠ” í…ŒìŠ¤íŠ¸" if lang == 'ko' else "Slowest test"}**: {hardest_test} ({"ë³µì¡ë„ê°€ ë†’ì„ ê°€ëŠ¥ì„±" if lang == 'ko' else "likely more complex"})
                    - **{"ê°€ì¥ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸" if lang == 'ko' else "Fastest test"}**: {easiest_test} ({"ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœ" if lang == 'ko' else "relatively simpler"})
                    - **{"ì°¸ê³ " if lang == 'ko' else "Note"}**: {"í…ŒìŠ¤íŠ¸ë³„ ì²˜ë¦¬ ì‹œê°„ ì°¨ì´ëŠ” ë¬¸ì œ ë‚œì´ë„ë‚˜ ê¸¸ì´ì™€ ê´€ë ¨" if lang == 'ko' else "Time differences relate to problem difficulty or length"}
                    """)
    
    # íƒ­ 4: ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„
    with tabs[3]:
        if 'law' not in filtered_df.columns:
            st.info("Law classification data not available.")
        else:
            st.header(f"âš–ï¸ {t['law_analysis']}")
            
            # ì „ì²´ ë²•ë ¹/ë¹„ë²•ë ¹ ë¹„ìœ¨
            st.subheader(t['law_ratio'])
            
            # ğŸ”¥ í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜ ê³„ì‚° (ì „ì²´ ìš”ì•½ê³¼ ë™ì¼)
            law_count = 0
            non_law_count = 0
            
            if selected_tests:
                for test_name in selected_tests:
                    if test_name in testsets:
                        test_df = testsets[test_name]
                        if 'law' in test_df.columns:
                            law_count += len(test_df[test_df['law'] == 'O'])
                            non_law_count += len(test_df[test_df['law'] != 'O'])
                        else:
                            # law ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ë¹„ë²•ë ¹ìœ¼ë¡œ ê°„ì£¼
                            non_law_count += len(test_df)
            else:
                # ì„ íƒëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ëª¨ë“  í…ŒìŠ¤íŠ¸ì…‹ í•©ì‚°
                for test_name, test_df in testsets.items():
                    if 'law' in test_df.columns:
                        law_count += len(test_df[test_df['law'] == 'O'])
                        non_law_count += len(test_df[test_df['law'] != 'O'])
                    else:
                        non_law_count += len(test_df)
            
            total_unique = law_count + non_law_count
            
            col1, col2 = st.columns(2)
            
            with col1:
                # íŒŒì´ ì°¨íŠ¸
                fig = go.Figure(data=[go.Pie(
                    labels=[t['law'], t['non_law']],
                    values=[law_count, non_law_count],
                    hole=0.3,
                    marker=dict(line=dict(color='black', width=2))
                )])
                fig.update_layout(
                    title=t['law_distribution_stat'],
                    height=400
                )
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                # ìˆ˜ì¹˜ í‘œì‹œ
                st.metric(
                    t['law_problems'], 
                    f"{law_count} ({law_count/total_unique*100:.1f}%)",
                    help="í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ ê¸°ì¤€ ë²•ë ¹ ë¬¸ì œ ìˆ˜"
                )
                st.metric(
                    t['non_law_problems'], 
                    f"{non_law_count} ({non_law_count/total_unique*100:.1f}%)",
                    help="í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ ê¸°ì¤€ ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜"
                )
            
            st.info("ğŸ’¡ " + (
                "ì´ í†µê³„ëŠ” í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ ê¸°ì¤€ì…ë‹ˆë‹¤. ì „ì²´ ìš”ì•½ íƒ­ê³¼ ë™ì¼í•©ë‹ˆë‹¤." 
                if lang == 'ko' 
                else "These statistics are based on test set files. They match the Overview tab."
            ))
            
            # ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥
            st.markdown("---")
            st.subheader(t['model_law_performance'])
            
            law_performance = []
            for model in filtered_df['ëª¨ë¸'].unique():
                model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
                
                law_model = model_df[model_df['law'] == 'O']
                non_law_model = model_df[model_df['law'] != 'O']
                
                law_acc = (law_model['ì •ë‹µì—¬ë¶€'].sum() / len(law_model) * 100) if len(law_model) > 0 else 0
                non_law_acc = (non_law_model['ì •ë‹µì—¬ë¶€'].sum() / len(non_law_model) * 100) if len(non_law_model) > 0 else 0
                
                law_performance.append({
                    'ëª¨ë¸': model,
                    'ë²•ë ¹': law_acc,
                    'ë¹„ë²•ë ¹': non_law_acc
                })
            
            law_perf_df = pd.DataFrame(law_performance)
            
            # ê·¸ë£¹ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë²•ë ¹'],
                marker_color='skyblue'
            ))
            fig.add_trace(go.Bar(
                name=t['non_law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë¹„ë²•ë ¹'],
                marker_color='lightcoral'
            ))
            
            fig.update_layout(
                barmode='group',
                title=t['law_distribution'],
                height=500,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, width='stretch')
            
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
            # ë²•ë ¹ì— ê°•í•œ ëª¨ë¸ê³¼ ë¹„ë²•ë ¹ì— ê°•í•œ ëª¨ë¸ ì°¾ê¸°
            law_perf_df['ë²•ë ¹_ìš°ìœ„'] = law_perf_df['ë²•ë ¹'] - law_perf_df['ë¹„ë²•ë ¹']
            best_law_model = law_perf_df.loc[law_perf_df['ë²•ë ¹'].idxmax()]
            best_nonlaw_model = law_perf_df.loc[law_perf_df['ë¹„ë²•ë ¹'].idxmax()]
            most_law_specialized = law_perf_df.loc[law_perf_df['ë²•ë ¹_ìš°ìœ„'].idxmax()]
            most_balanced = law_perf_df.loc[law_perf_df['ë²•ë ¹_ìš°ìœ„'].abs().idxmin()]
            
            st.success(f"""
            ğŸ’¡ **{"ë²•ë ¹ ë¬¸ì œ íŠ¹í™” ë¶„ì„" if lang == 'ko' else "Law Problem Specialization Analysis"}**:
            
            ğŸ† **{"ìµœê³  ì„±ëŠ¥" if lang == 'ko' else "Top Performance"}**:
            - **{"ë²•ë ¹ ìµœê³ " if lang == 'ko' else "Best at Law"}**: {best_law_model['ëª¨ë¸']} ({best_law_model['ë²•ë ¹']:.1f}%)
            - **{"ë¹„ë²•ë ¹ ìµœê³ " if lang == 'ko' else "Best at Non-Law"}**: {best_nonlaw_model['ëª¨ë¸']} ({best_nonlaw_model['ë¹„ë²•ë ¹']:.1f}%)
            
            âš–ï¸ **{"ê· í˜• vs íŠ¹í™”" if lang == 'ko' else "Balance vs Specialization"}**:
            - **{"ê°€ì¥ ê· í˜•ì¡íŒ ëª¨ë¸" if lang == 'ko' else "Most Balanced"}**: {most_balanced['ëª¨ë¸']} ({"ì°¨ì´" if lang == 'ko' else "diff"}: {abs(most_balanced['ë²•ë ¹_ìš°ìœ„']):.1f}%p)
            - **{"ë²•ë ¹ íŠ¹í™” ëª¨ë¸" if lang == 'ko' else "Law Specialized"}**: {most_law_specialized['ëª¨ë¸']} ({"ë²•ë ¹" if lang == 'ko' else "law"} +{most_law_specialized['ë²•ë ¹_ìš°ìœ„']:.1f}%p)
            
            ğŸ“‹ **{"í™œìš© ê°€ì´ë“œ" if lang == 'ko' else "Usage Guide"}**:
            - **{"ë²•ë¥  ìë¬¸ ì‹œìŠ¤í…œ" if lang == 'ko' else "Legal Advisory"}**: {best_law_model['ëª¨ë¸']} {"ì¶”ì²œ" if lang == 'ko' else "recommended"}
            - **{"ì¼ë°˜ ì•ˆì „ êµìœ¡" if lang == 'ko' else "General Safety Training"}**: {most_balanced['ëª¨ë¸']} {"ì¶”ì²œ" if lang == 'ko' else "recommended"}
            - **{"ì¢…í•© ì†”ë£¨ì…˜" if lang == 'ko' else "Comprehensive Solution"}**: {"ë²•ë ¹/ë¹„ë²•ë ¹ ëª¨ë‘ ë†’ì€ ëª¨ë¸ ì„ íƒ" if lang == 'ko' else "Choose models high in both areas"}
            """)
    
    # íƒ­ 5: ê³¼ëª©ë³„ ë¶„ì„
    with tabs[4]:
        if 'Subject' not in filtered_df.columns:
            st.info("Subject data not available.")
        else:
            st.header(f"ğŸ“š {t['subject_analysis']}")
            
            # ê³¼ëª©ë³„ ì„±ëŠ¥
            subject_stats = filtered_df.groupby('Subject').agg({
                'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
            if lang == 'ko':
                subject_stats.columns = ['ê³¼ëª©', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
                subj_col = 'ê³¼ëª©'
                acc_col = 'ì •í™•ë„'
                correct_col = 'ì •ë‹µ'
                total_col = 'ì´ë¬¸ì œ'
            else:
                subject_stats.columns = ['Subject', 'Correct', 'Total', 'Accuracy']
                subj_col = 'Subject'
                acc_col = 'Accuracy'
                correct_col = 'Correct'
                total_col = 'Total'
            
            subject_stats[acc_col] = subject_stats[acc_col] * 100
            subject_stats = subject_stats.sort_values(acc_col, ascending=False)
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                # í…Œì´ë¸”
                st.dataframe(
                    subject_stats.style.format({acc_col: '{:.2f}%'})
                    .background_gradient(subset=[acc_col], cmap='RdYlGn'),
                    width='stretch'
                )
            
            with col2:
                # ë°” ì°¨íŠ¸
                fig = px.bar(
                    subject_stats,
                    x=subj_col,
                    y=acc_col,
                    title=t['subject_performance'],
                    text=acc_col,
                    color=acc_col,
                    color_continuous_scale='RdYlGn',
                    labels={subj_col: t['by_subject'].replace('ë³„', ''), acc_col: t['accuracy'] + ' (%)'}
                )
                fig.update_traces(
                    texttemplate='%{text:.1f}%',
                    textposition='outside',                textfont=dict(size=annotation_size),
                    marker_line_color='black',
                    marker_line_width=1.5
                )
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title=t['accuracy'] + ' (%)',
                    xaxis_title=t['by_subject'].replace('ë³„', '')
                )
                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                st.plotly_chart(fig, width='stretch')
            
            # ëª¨ë¸ë³„ ê³¼ëª© ì„±ëŠ¥ íˆíŠ¸ë§µ (ì…€ ê²½ê³„ì„  ì¶”ê°€)
            st.markdown("---")
            subject_model = filtered_df.groupby(['ëª¨ë¸', 'Subject'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            subject_model_pivot = subject_model.unstack(fill_value=0)
            
            fig = go.Figure(data=go.Heatmap(
                z=subject_model_pivot.values,
                x=subject_model_pivot.columns,
                y=subject_model_pivot.index,
                colorscale='RdYlGn',
                text=np.round(subject_model_pivot.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": annotation_size},
                colorbar=dict(title=t['accuracy'] + " (%)"),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            # ê³¼ëª©ë³„ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
            # ê³¼ëª©ë³„ í‰ê·  ì •í™•ë„
            subject_avg = subject_model_pivot.mean(axis=0).sort_values()
            hardest_subject = subject_avg.index[0]
            easiest_subject = subject_avg.index[-1]
            
            # ëª¨ë¸ë³„ í¸ì°¨ (ê³¼ëª©ê°„ ì„±ëŠ¥ ì¼ê´€ì„±)
            model_consistency = subject_model_pivot.std(axis=1).sort_values()
            most_consistent = model_consistency.index[0]
            least_consistent = model_consistency.index[-1]
            
            # íŠ¹í™” ëª¨ë¸ ì°¾ê¸°
            subject_specialists = {}
            for subject in subject_model_pivot.columns:
                best_model = subject_model_pivot[subject].idxmax()
                best_score = subject_model_pivot[subject].max()
                subject_specialists[subject] = (best_model, best_score)
            
            st.info(f"""
            ğŸ’¡ **{"ê³¼ëª©ë³„ ë‚œì´ë„ ë° ëª¨ë¸ íŠ¹í™” ë¶„ì„" if lang == 'ko' else "Subject Difficulty & Model Specialization"}**:
            
            ğŸ“š **{"ê³¼ëª© ë‚œì´ë„" if lang == 'ko' else "Subject Difficulty"}**:
            - **{"ê°€ì¥ ì–´ë ¤ìš´ ê³¼ëª©" if lang == 'ko' else "Hardest"}**: {hardest_subject} ({"í‰ê· " if lang == 'ko' else "avg"}: {subject_avg.iloc[0]:.1f}%)
            - **{"ê°€ì¥ ì‰¬ìš´ ê³¼ëª©" if lang == 'ko' else "Easiest"}**: {easiest_subject} ({"í‰ê· " if lang == 'ko' else "avg"}: {subject_avg.iloc[-1]:.1f}%)
            - **{"ë‚œì´ë„ ê²©ì°¨" if lang == 'ko' else "Difficulty gap"}**: {subject_avg.iloc[-1] - subject_avg.iloc[0]:.1f}%p
            
            ğŸ¯ **{"ëª¨ë¸ ì¼ê´€ì„±" if lang == 'ko' else "Model Consistency"}**:
            - **{"ê°€ì¥ ì¼ê´€ì " if lang == 'ko' else "Most Consistent"}**: {most_consistent} ({"í¸ì°¨" if lang == 'ko' else "std"}: {model_consistency.iloc[0]:.1f})
            - **{"ê°€ì¥ ë¶ˆê· í˜•" if lang == 'ko' else "Least Consistent"}**: {least_consistent} ({"í¸ì°¨" if lang == 'ko' else "std"}: {model_consistency.iloc[-1]:.1f})
            
            ğŸ† **{"ê³¼ëª©ë³„ ìµœê³  ëª¨ë¸" if lang == 'ko' else "Top Models by Subject"}**:
            {chr(10).join([f"- **{subj}**: {model} ({score:.1f}%)" for subj, (model, score) in list(subject_specialists.items())[:3]])}
            
            ğŸ’¼ **{"í™œìš© ì œì•ˆ" if lang == 'ko' else "Recommendations"}**:
            - **{"íŠ¹ì • ê³¼ëª© êµìœ¡" if lang == 'ko' else "Subject-specific training"}**: {"í•´ë‹¹ ê³¼ëª© ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™œìš©" if lang == 'ko' else "Use top model for that subject"}
            - **{"ì¢…í•© êµìœ¡" if lang == 'ko' else "Comprehensive education"}**: {most_consistent} {"(ê· í˜•ì¡íŒ ì„±ëŠ¥)" if lang == 'ko' else "(balanced performance)"}
            """)
    
    # íƒ­ 6: ì—°ë„ë³„ ë¶„ì„
    with tabs[5]:
        if 'Year' not in filtered_df.columns:
            st.info("Year data not available.")
        else:
            st.header(f"ğŸ“… {t['year_analysis']}")
            
            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)"):
                st.write("**í•„í„°ë§ ì „ ì›ë³¸ ë°ì´í„°:**")
                st.write(f"- ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(results_df):,}")
                st.write(f"- ì›ë³¸ Year ê³ ìœ ê°’: {sorted([str(y) for y in results_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í•„í„°ë§ í›„ ë°ì´í„°:**")
                st.write(f"- í•„í„°ë§ëœ ë°ì´í„° í–‰ ìˆ˜: {len(filtered_df):,}")
                st.write(f"- í•„í„°ë§ëœ Year ê³ ìœ ê°’: {sorted([str(y) for y in filtered_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í˜„ì¬ í•„í„° ì„¤ì •:**")
                st.write(f"- ì„ íƒëœ í…ŒìŠ¤íŠ¸: {selected_tests}")
                st.write(f"- ì„ íƒëœ ëª¨ë¸: {selected_models}")
                st.write(f"- ì„ íƒëœ ì—°ë„: {selected_years if 'selected_years' in locals() else 'ì „ì²´'}")
            
            # Yearë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (ì›ë³¸ ìˆ˜ì • ì—†ì´)
            year_int_series = filtered_df['Year'].apply(safe_convert_to_int)
            valid_year_mask = year_int_series.notna()
            
            if valid_year_mask.any():
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ìƒˆ DataFrame ìƒì„± (ëª¨ë¸ ì»¬ëŸ¼ í¬í•¨)
                year_df = pd.DataFrame({
                    'Year_Int': year_int_series[valid_year_mask],
                    'ì •ë‹µì—¬ë¶€': filtered_df.loc[valid_year_mask, 'ì •ë‹µì—¬ë¶€'],
                    'ëª¨ë¸': filtered_df.loc[valid_year_mask, 'ëª¨ë¸']
                })
                
                # ì—°ë„ë³„ ì„±ëŠ¥
                year_stats = year_df.groupby('Year_Int').agg({
                    'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
                }).reset_index()
                year_stats.columns = ['ì—°ë„', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
                year_stats['ì •í™•ë„'] = year_stats['ì •í™•ë„'] * 100
                year_stats = year_stats.sort_values('ì—°ë„')
                
                # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ í‘œì‹œ
                year_stats['ì—°ë„'] = year_stats['ì—°ë„'].astype(int)
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # í…Œì´ë¸” (ì†Œìˆ˜ì  ì—†ì´ í‘œì‹œ)
                    st.dataframe(
                        year_stats.style.format({
                            'ì—°ë„': '{:.0f}',
                            'ì •ë‹µ': '{:.0f}',
                            'ì´ë¬¸ì œ': '{:.0f}',
                            'ì •í™•ë„': '{:.2f}%'
                        })
                        .background_gradient(subset=['ì •í™•ë„'], cmap='RdYlGn'),
                        width='stretch'
                    )
                
                with col2:
                    # ë¼ì¸ ì°¨íŠ¸
                    fig = px.line(
                        year_stats,
                        x='ì—°ë„',
                        y='ì •í™•ë„',
                        title=t['year_performance'],
                        markers=True,
                        text='ì •í™•ë„'
                    )
                    fig.update_traces(
                        texttemplate='%{text:.1f}%',
                        textposition='top center',                textfont=dict(size=annotation_size),
                        marker_size=10,
                        marker_line_color='black',
                        marker_line_width=2,
                        line_width=3
                    )
                    fig.update_layout(
                        height=400,
                        yaxis_title=t['accuracy'] + ' (%)',
                        xaxis_title=t['year']
                    )
                    st.plotly_chart(fig, width='stretch')
                
                # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ì°¨íŠ¸ ì¶”ê°€
                st.markdown("---")
                st.subheader(f"ğŸ“Š {t['year_problem_distribution']}")
                
                # ë‹¤êµ­ì–´ ì»¬ëŸ¼ëª… ì„¤ì •
                year_col = t['year']
                count_col = t['problem_count']
                
                # í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚° (ì¤‘ë³µ ì œê±°)
                if selected_tests:
                    year_problem_count = []
                    for test_name in selected_tests:
                        if test_name in testsets and 'Year' in testsets[test_name].columns:
                            test_year_counts = testsets[test_name].groupby('Year').size()
                            for year, count in test_year_counts.items():
                                year_int = safe_convert_to_int(year)
                                if year_int:
                                    year_problem_count.append({year_col: year_int, count_col: count})
                    
                    if year_problem_count:
                        year_problem_df = pd.DataFrame(year_problem_count)
                        year_problem_df = year_problem_df.groupby(year_col)[count_col].sum().reset_index()
                        year_problem_df = year_problem_df.sort_values(year_col)
                    else:
                        # ë°±ì—…: filtered_dfì—ì„œ ê³ ìœ  ë¬¸ì œ ìˆ˜ ê³„ì‚°
                        year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                        year_problem_df.columns = [year_col, count_col]
                        year_problem_df[year_col] = year_problem_df[year_col].astype(int)
                        year_problem_df = year_problem_df.sort_values(year_col)
                else:
                    # í…ŒìŠ¤íŠ¸ ì„ íƒ ì•ˆ ë¨: filtered_dfì—ì„œ ê³„ì‚°
                    year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                    year_problem_df.columns = [year_col, count_col]
                    year_problem_df[year_col] = year_problem_df[year_col].astype(int)
                    year_problem_df = year_problem_df.sort_values(year_col)
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ í…Œì´ë¸”
                    st.dataframe(
                        year_problem_df.style.format({
                            year_col: '{:.0f}',
                            count_col: '{:.0f}'
                        })
                        .background_gradient(subset=[count_col], cmap='Blues'),
                        width='stretch'
                    )
                    
                    # ì´ ë¬¸ì œ ìˆ˜ í‘œì‹œ
                    st.metric(t['total_problem_count'], f"{year_problem_df[count_col].sum():,.0f}" + (t['problems'] if lang == 'ko' else ''))
                
                with col2:
                    # ë°” ì°¨íŠ¸
                    fig = px.bar(
                        year_problem_df,
                        x=year_col,
                        y=count_col,
                        title=t['year_problem_chart'],
                        text=count_col,
                        color=count_col,
                        color_continuous_scale='Blues'
                    )
                    fig.update_traces(
                texttemplate='%{text}',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['problem_count'],
                        xaxis_title=t['year'],
                        xaxis=dict(tickmode='linear')
                    )
                    st.plotly_chart(fig, width='stretch')
                
                # ëª¨ë¸ë³„ ì—°ë„ ì„±ëŠ¥ íˆíŠ¸ë§µ
                st.markdown("---")
                year_model = year_df.groupby(['ëª¨ë¸', 'Year_Int'])['ì •ë‹µì—¬ë¶€'].mean() * 100
                year_model_pivot = year_model.unstack(fill_value=0)
                
                # ì»¬ëŸ¼ëª…ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                year_model_pivot.columns = year_model_pivot.columns.astype(int)
                
                fig = go.Figure(data=go.Heatmap(
                    z=year_model_pivot.values,
                    x=year_model_pivot.columns,
                    y=year_model_pivot.index,
                    colorscale='RdYlGn',
                    text=np.round(year_model_pivot.values, 1),
                    texttemplate='%{text:.1f}',
                    textfont={"size": annotation_size},
                    colorbar=dict(title=t['accuracy'] + " (%)"),
                    xgap=2,  # ì…€ ê²½ê³„ì„ 
                    ygap=2
                ))
                fig.update_layout(height=400)
                fig.update_xaxes(tickfont=dict(size=annotation_size))
                fig.update_yaxes(tickfont=dict(size=annotation_size))
                st.plotly_chart(fig, width='stretch')
                
                # ì—°ë„ë³„ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
                year_avg = year_model_pivot.mean(axis=0).sort_values()
                hardest_year = int(year_avg.index[0])
                easiest_year = int(year_avg.index[-1])
                
                # ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„
                years_sorted = sorted(year_model_pivot.columns)
                if len(years_sorted) >= 3:
                    recent_years = years_sorted[-3:]
                    old_years = years_sorted[:3]
                    recent_avg = year_model_pivot[recent_years].mean(axis=1).mean()
                    old_avg = year_model_pivot[old_years].mean(axis=1).mean()
                    trend = "ìƒìŠ¹" if recent_avg > old_avg else "í•˜ë½" if recent_avg < old_avg else "ìœ ì§€"
                    trend_en = "improving" if recent_avg > old_avg else "declining" if recent_avg < old_avg else "stable"
                    
                    st.success(f"""
                    ğŸ’¡ **{"ì—°ë„ë³„ ë‚œì´ë„ íŠ¸ë Œë“œ" if lang == 'ko' else "Year-over-Year Difficulty Trend"}**:
                    
                    ğŸ“… **{"ì—°ë„ë³„ ë‚œì´ë„" if lang == 'ko' else "Difficulty by Year"}**:
                    - **{"ê°€ì¥ ì–´ë ¤ìš´ ì—°ë„" if lang == 'ko' else "Hardest year"}**: {hardest_year} ({"í‰ê· " if lang == 'ko' else "avg"}: {year_avg.iloc[0]:.1f}%)
                    - **{"ê°€ì¥ ì‰¬ìš´ ì—°ë„" if lang == 'ko' else "Easiest year"}**: {easiest_year} ({"í‰ê· " if lang == 'ko' else "avg"}: {year_avg.iloc[-1]:.1f}%)
                    
                    ğŸ“ˆ **{"ì‹œí—˜ ë‚œì´ë„ ì¶”ì„¸" if lang == 'ko' else "Exam Difficulty Trend"}**:
                    - **{"ìµœê·¼ 3ë…„" if lang == 'ko' else "Recent 3 years"}** ({', '.join(map(str, recent_years))}): {"í‰ê· " if lang == 'ko' else "avg"} {recent_avg:.1f}%
                    - **{"ì´ˆê¸° 3ë…„" if lang == 'ko' else "First 3 years"}** ({', '.join(map(str, old_years))}): {"í‰ê· " if lang == 'ko' else "avg"} {old_avg:.1f}%
                    - **{"ì¶”ì„¸" if lang == 'ko' else "Trend"}**: {trend if lang == 'ko' else trend_en} ({abs(recent_avg - old_avg):.1f}%p {"ì°¨ì´" if lang == 'ko' else "difference"})
                    
                    ğŸ“ **{"í•™ìŠµ ê°€ì´ë“œ" if lang == 'ko' else "Study Guide"}**:
                    - {"ìµœê·¼ ì¶œì œ ê²½í–¥ì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµ" if lang == 'ko' else "Focus on recent exam patterns"}
                    - {f"{hardest_year}ë…„ ë¬¸ì œë¡œ ì‹¤ì „ ëŒ€ë¹„" if lang == 'ko' else f"Use {hardest_year} problems for practice"}
                    """)
                else:
                    st.info(f"""
                    ğŸ’¡ **{"ì—°ë„ë³„ ë‚œì´ë„" if lang == 'ko' else "Difficulty by Year"}**:
                    - **{"ê°€ì¥ ì–´ë ¤ìš´ ì—°ë„" if lang == 'ko' else "Hardest year"}**: {hardest_year} ({"í‰ê· " if lang == 'ko' else "avg"}: {year_avg.iloc[0]:.1f}%)
                    - **{"ê°€ì¥ ì‰¬ìš´ ì—°ë„" if lang == 'ko' else "Easiest year"}**: {easiest_year} ({"í‰ê· " if lang == 'ko' else "avg"}: {year_avg.iloc[-1]:.1f}%)
                    """)
            else:
                st.info("ì—°ë„ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # íƒ­ 7: ì‹¬ì¸µ ì˜¤ë‹µ ë¶„ì„ (Complete Enhanced Version)
    with tabs[6]:
        st.header(f"ğŸ”¬ {'ì‹¬ì¸µ ì˜¤ë‹µ ë¶„ì„' if lang == 'ko' else 'Deep Incorrect Analysis'}")
        
        st.markdown("""
        > **ë…¼ë¬¸ ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„**: ì´ íƒ­ì€ í•™ìˆ  ë…¼ë¬¸ì˜ "ê³µí†µ ì˜¤ë‹µ íŒ¨í„´(Common Wrong Answer)" ë¶„ì„ ë°©ë²•ë¡ ì„ ì ìš©í•©ë‹ˆë‹¤.
        > ë‹¨ìˆœíˆ ì˜¤ë‹µë¥ ì´ ë†’ì€ ë¬¸ì œë¥¼ ë„˜ì–´, **ëª¨ë¸ë“¤ì´ ì¼ê´€ë˜ê²Œ ê°™ì€ ì˜¤ë‹µì„ ì„ íƒí•˜ëŠ” íŒ¨í„´**ì„ ì‹ë³„í•˜ì—¬ 
        > LLMì˜ ê·¼ë³¸ì ì¸ ì§€ì‹ ë¬¸ì œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
        """)
        
        # ê¸°ë³¸ ì˜¤ë‹µ ë¶„ì„ ë°ì´í„° ì¤€ë¹„
        # ë¬¸ì œë³„ ì˜¤ë‹µ í†µê³„ ê³„ì‚°
        # ğŸ”§ ìˆ˜ì •: ê³ ìœ  ì‹ë³„ì ìƒì„±í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        # Questionë§Œìœ¼ë¡œëŠ” ì¤‘ë³µ ê°€ëŠ¥ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ì—ì„œ ê°™ì€ ë¬¸ì œ ë²ˆí˜¸)
        # â†’ Test Name + Year + Session + Questionìœ¼ë¡œ ê³ ìœ  ì‹ë³„ì ìƒì„±
        
        # ê³ ìœ  ì‹ë³„ì ìƒì„±
        if 'Test Name' in filtered_df.columns:
            filtered_df['unique_question_id'] = (
                filtered_df['Test Name'].astype(str) + '_' +
                filtered_df['Year'].astype(str) + '_' +
                filtered_df['Session'].astype(str) + '_' +
                filtered_df['Question'].astype(str)
            )
        else:
            filtered_df['unique_question_id'] = filtered_df['Question'].astype(str)
        
        # ê³ ìœ  ì‹ë³„ìë¡œ ê·¸ë£¹í™”
        problem_analysis = filtered_df.groupby('unique_question_id').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        problem_analysis.columns = ['unique_question_id', 'correct_count', 'total_count', 'correct_rate']
        problem_analysis['incorrect_rate'] = 1 - problem_analysis['correct_rate']
        problem_analysis['incorrect_count'] = problem_analysis['total_count'] - problem_analysis['correct_count']
        
        # âš¡ ìµœì í™”: ë£¨í”„ ëŒ€ì‹  ë²¡í„°í™”ëœ groupbyë¡œ ë©”íƒ€ë°ì´í„° ì¼ê´„ ì¶”ì¶œ
        # (ê¸°ì¡´ ë£¨í”„ëŠ” 6,659ë¬¸ì œ Ã— 86,567í–‰ ë°˜ë³µìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ë°œìƒ)
        
        # ë¬¸ì œë³„ ì²« ë²ˆì§¸ í–‰ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í•œ ë²ˆì˜ groupby)
        meta_cols = ['unique_question_id', 'Question']
        for col in ['Subject', 'Year', 'Answer', 'law']:
            if col in filtered_df.columns:
                meta_cols.append(col)
        
        first_rows = filtered_df.groupby('unique_question_id')[meta_cols[1:]].first().reset_index()
        problem_analysis = problem_analysis.merge(first_rows, on='unique_question_id', how='left')
        
        # ë¬¸ì œ ì‹ë³„ì ìƒì„± (ë²¡í„°í™”)
        def fast_create_problem_id(row):
            parts = []
            if 'Test Name' in row.index and pd.notna(row.get('Test Name')):
                parts.append(str(row['Test Name']))
            elif 'í…ŒìŠ¤íŠ¸ëª…' in row.index and pd.notna(row.get('í…ŒìŠ¤íŠ¸ëª…')):
                parts.append(str(row['í…ŒìŠ¤íŠ¸ëª…']))
            if 'Year' in row.index and pd.notna(row.get('Year')):
                y = safe_convert_to_int(row['Year'])
                if y: parts.append(str(y))
            if 'Session' in row.index and pd.notna(row.get('Session')):
                s = safe_convert_to_int(row['Session'])
                if s: parts.append(f"S{s}")
            if 'Subject' in row.index and pd.notna(row.get('Subject')):
                parts.append(str(row['Subject']))
            if 'Number' in row.index and pd.notna(row.get('Number')):
                n = safe_convert_to_int(row['Number'])
                if n: parts.append(f"Q{n}")
            return " / ".join(parts) if parts else "Unknown"
        
        # ì‹ë³„ììš© ë©”íƒ€ë°ì´í„°ë„ first_rowsì—ì„œ ê°€ì ¸ì˜¤ê¸°
        id_cols = ['unique_question_id']
        for col in ['Test Name', 'í…ŒìŠ¤íŠ¸ëª…', 'Year', 'Session', 'Subject', 'Number']:
            if col in filtered_df.columns:
                id_cols.append(col)
        id_rows = filtered_df.groupby('unique_question_id')[id_cols[1:]].first().reset_index()
        problem_analysis = problem_analysis.merge(
            id_rows[[c for c in id_cols if c not in problem_analysis.columns or c == 'unique_question_id']],
            on='unique_question_id', how='left', suffixes=('', '_dup')
        )
        # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
        problem_analysis = problem_analysis[[c for c in problem_analysis.columns if not c.endswith('_dup')]]
        
        problem_analysis['problem_id'] = id_rows.apply(fast_create_problem_id, axis=1).values
        problem_analysis['CorrectAnswer'] = problem_analysis.get('Answer', pd.Series(['Unknown'] * len(problem_analysis)))
        problem_analysis['law_status'] = problem_analysis.get('law', pd.Series(['Unknown'] * len(problem_analysis)))
        
        # âš¡ ëª¨ë¸ë³„ ì •ì˜¤ë‹µ ë° ì„ íƒí•œ ë‹µ - ìˆœìˆ˜ ë²¡í„° ì—°ì‚° (apply ì œê±°)
        
        # ì •ë‹µ ëª¨ë¸ ëª©ë¡ (ë²¡í„°í™”)
        correct_df = filtered_df[filtered_df['ì •ë‹µì—¬ë¶€'] == True][['unique_question_id', 'ëª¨ë¸']]
        correct_models_map = correct_df.groupby('unique_question_id')['ëª¨ë¸'].apply(
            lambda x: 'âœ“ ' + ', '.join(sorted(x.unique()))
        ).to_dict()
        
        # ì˜¤ë‹µ ëª¨ë¸ ëª©ë¡ (ë²¡í„°í™”)
        incorrect_df = filtered_df[filtered_df['ì •ë‹µì—¬ë¶€'] == False][['unique_question_id', 'ëª¨ë¸']]
        incorrect_models_map = incorrect_df.groupby('unique_question_id')['ëª¨ë¸'].apply(
            lambda x: 'âœ— ' + ', '.join(sorted(x.unique()))
        ).to_dict()
        
        # ì„ íƒí•œ ë‹µ ë”•ì…”ë„ˆë¦¬ (ìˆœìˆ˜ ë²¡í„°í™” - apply ì™„ì „ ì œê±°)
        if 'ì˜ˆì¸¡ë‹µ' in filtered_df.columns:
            answers_df = filtered_df[['unique_question_id', 'ëª¨ë¸', 'ì˜ˆì¸¡ë‹µ']].copy()
            answers_df['ì˜ˆì¸¡ë‹µ'] = answers_df['ì˜ˆì¸¡ë‹µ'].fillna('N/A')
            # pivotìœ¼ë¡œ í•œ ë²ˆì— ë³€í™˜ í›„ ë”•ì…”ë„ˆë¦¬ë¡œ
            selected_answers_map = {}
            for uid, grp in answers_df.groupby('unique_question_id'):
                selected_answers_map[uid] = dict(zip(grp['ëª¨ë¸'].values, grp['ì˜ˆì¸¡ë‹µ'].values))
        else:
            selected_answers_map = {uid: {} for uid in problem_analysis['unique_question_id']}
        
        problem_analysis['correct_models'] = problem_analysis['unique_question_id'].map(correct_models_map).fillna('-')
        problem_analysis['incorrect_models'] = problem_analysis['unique_question_id'].map(incorrect_models_map).fillna('-')
        problem_analysis['selected_answers'] = problem_analysis['unique_question_id'].map(selected_answers_map)
        
        # ì˜¤ë‹µë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        problem_analysis = problem_analysis.sort_values(
            by=['incorrect_rate', 'problem_id'],
            ascending=[False, True]
        )
        
        # ========================================
        # ì„¹ì…˜ 1: ê°œìš” ë° ì£¼ìš” ë©”íŠ¸ë¦­
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ“Š " + ("ì˜¤ë‹µ ë¶„ì„ ê°œìš”" if lang == 'ko' else "Incorrect Analysis Overview"))
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            # ğŸ”§ ìˆ˜ì •: testsets ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° (ì „ì²´ ìš”ì•½ê³¼ ë™ì¼)
            total_problems_testset = 0
            if selected_tests:
                for test_name in selected_tests:
                    if test_name in testsets:
                        total_problems_testset += len(testsets[test_name])
            
            # ë°±ì—…: problem_analysis ê¸°ì¤€
            total_problems_analysis = len(problem_analysis)
            
            # ìš°ì„ ìˆœìœ„: testsets > analysis
            display_total = total_problems_testset if total_problems_testset > 0 else total_problems_analysis
            
            st.metric(
                "ë¶„ì„ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Total Problems",
                f"{display_total:,}",
                help="í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€ ì´ ë¬¸ì œ ìˆ˜"
            )
        
        with col2:
            all_wrong = len(problem_analysis[problem_analysis['correct_count'] == 0])
            st.metric(
                "ì™„ì „ ê³µí†µ ì˜¤ë‹µ" if lang == 'ko' else "Complete Common Wrong Answers",
                f"{all_wrong}",
                help="ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ"
            )
        
        with col3:
            most_wrong = len(problem_analysis[problem_analysis['incorrect_rate'] >= 0.5])
            st.metric(
                "ì£¼ìš” ê³µí†µ ì˜¤ë‹µ" if lang == 'ko' else "Major Common Wrong Answers",
                f"{most_wrong}",
                help="50% ì´ìƒì˜ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ"
            )
        
        with col4:
            avg_incorrect_rate = problem_analysis['incorrect_rate'].mean() * 100
            st.metric(
                "í‰ê·  ì˜¤ë‹µë¥ " if lang == 'ko' else "Avg Incorrect Rate",
                f"{avg_incorrect_rate:.1f}%"
            )
        
        # ========================================
        # ì„¹ì…˜ 2: ì¼ê´€ëœ ì˜¤ë‹µ ì„ íƒ íŒ¨í„´ ë¶„ì„ (í•µì‹¬!)
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ¯ " + ("ì¼ê´€ëœ ì˜¤ë‹µ ì„ íƒ íŒ¨í„´ (ê³µí†µ ì˜¤ë‹µ í•µì‹¬ ì§€í‘œ)" if lang == 'ko' else "Consistent Incorrect Answer Pattern"))
        
        with st.expander("ğŸ“– " + ("ê³µí†µ ì˜¤ë‹µì´ë€? (ê³„ì‚° ë°©ì‹ ì„¤ëª…)" if lang == 'ko' else "What is Common Wrong Answer?")):
            st.markdown("""
            ### ğŸ” ê³„ì‚° ë°©ì‹
            
            **ì „ì œ ì¡°ê±´: ì˜¤ë‹µë¥  50% ì´ìƒ ë¬¸ì œë§Œ ë¶„ì„**
            ```
            ì´ìœ :
            - 1-2ê°œ ëª¨ë¸ë§Œ í‹€ë¦° ë¬¸ì œ â†’ ìš°ì—°ì¼ ê°€ëŠ¥ì„±
            - 50% ì´ìƒ í‹€ë¦° ë¬¸ì œ â†’ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´
            
            ì˜ˆì‹œ:
            âŒ ì œì™¸: 10ê°œ ì¤‘ 1ê°œë§Œ í‹€ë¦¼ (10%) â†’ ì¼ê´€ì„± ì˜ë¯¸ ì—†ìŒ
            âŒ ì œì™¸: 10ê°œ ì¤‘ 3ê°œë§Œ í‹€ë¦¼ (30%) â†’ ë„ˆë¬´ ì ìŒ
            âœ… ë¶„ì„: 10ê°œ ì¤‘ 5ê°œ í‹€ë¦¼ (50%) â†’ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´
            âœ… ë¶„ì„: 10ê°œ ì¤‘ 8ê°œ í‹€ë¦¼ (80%) â†’ ì¤‘ìš”í•œ íŒ¨í„´
            ```
            
            **1ë‹¨ê³„: ì˜¤ë‹µ ìˆ˜ì§‘ (ì˜¤ë‹µë¥  â‰¥50% ë¬¸ì œë§Œ)**
            ```
            ë¬¸ì œ Q1 (ì˜¤ë‹µë¥  80%):
            - GPT-4o: 3ë²ˆ ì„ íƒ (ì •ë‹µ: 2ë²ˆ) âŒ
            - Claude: 3ë²ˆ ì„ íƒ (ì •ë‹µ: 2ë²ˆ) âŒ
            - Gemini: 4ë²ˆ ì„ íƒ (ì •ë‹µ: 2ë²ˆ) âŒ
            - EXAONE: 3ë²ˆ ì„ íƒ (ì •ë‹µ: 2ë²ˆ) âŒ
            - Llama: 2ë²ˆ ì„ íƒ (ì •ë‹µ: 2ë²ˆ) âœ…
            
            â†’ ì˜¤ë‹µ ëª©ë¡: [3, 3, 4, 3]
            â†’ ì˜¤ë‹µë¥ : 4/5 = 80% (âœ… ë¶„ì„ ëŒ€ìƒ)
            ```
            
            **2ë‹¨ê³„: ê°€ì¥ ë§ì´ ì„ íƒëœ ì˜¤ë‹µ ì°¾ê¸°**
            ```
            ì˜¤ë‹µ í†µê³„:
            - 3ë²ˆ: 3íšŒ â­ (ê°€ì¥ ë§ìŒ)
            - 4ë²ˆ: 1íšŒ
            
            â†’ ê³µí†µ ì˜¤ë‹µ: 3ë²ˆ
            ```
            
            **3ë‹¨ê³„: ì¼ê´€ì„± ê³„ì‚°**
            ```
            ì¼ê´€ì„± = (ê°€ì¥ ë§ì€ ì˜¤ë‹µ íšŸìˆ˜) / (ì „ì²´ ì˜¤ë‹µ íšŸìˆ˜)
                  = 3íšŒ / 4íšŒ
                  = 75%
            
            â†’ 75% ì¼ê´€ì„± (4ê°œ ì¤‘ 3ê°œê°€ ê°™ì€ ë‹µ ì„ íƒ)
            ```
            
            **âš ï¸ nan (ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨) ì²˜ë¦¬**
            ```
            ì˜ˆì‹œ 1: nanì´ ìˆëŠ” ê²½ìš°
            
            ë¬¸ì œ Q2 (ì˜¤ë‹µë¥  50%, í‰ê°€ ëª¨ë¸ 12ê°œ):
            - ì „ì²´ ì˜¤ë‹µ: 6ê°œ
            - ê³µí†µ ì˜¤ë‹µ 3ë²ˆ: 4ê°œ
            - nan (ì¶”ì¶œ ì‹¤íŒ¨): 2ê°œ
            
            ì¼ê´€ì„± ê³„ì‚°:
            = 4ê°œ / 6ê°œ = 66.7%
            
            ì˜ë¯¸:
            "6ê°œ ì˜¤ë‹µ ì¤‘ 4ê°œê°€ 3ë²ˆ ì„ íƒ (66.7%)"
            "ë‚˜ë¨¸ì§€ 2ê°œëŠ” í™•ì¸ ë¶ˆê°€ (nan)"
            
            í‘œì‹œ: ê³µí†µ ì˜¤ë‹µ 3.0 (4/6) - 66.7% ì¼ê´€ì„±
            
            ---
            
            ì˜ˆì‹œ 2: nanì´ ì—†ëŠ” ê²½ìš°
            
            ë¬¸ì œ Q3 (ì˜¤ë‹µë¥  50%, í‰ê°€ ëª¨ë¸ 12ê°œ):
            - ì „ì²´ ì˜¤ë‹µ: 6ê°œ
            - ê³µí†µ ì˜¤ë‹µ 3ë²ˆ: 6ê°œ
            - nan: 0ê°œ
            
            ì¼ê´€ì„± ê³„ì‚°:
            = 6ê°œ / 6ê°œ = 100%
            
            ì˜ë¯¸:
            "6ê°œ ì˜¤ë‹µ ëª¨ë‘ 3ë²ˆ ì„ íƒ (100%)"
            
            í‘œì‹œ: ê³µí†µ ì˜¤ë‹µ 3.0 (6/6) - 100% ì¼ê´€ì„±
            ```
            
            **ğŸ“Œ ì¤‘ìš” ì›ì¹™**:
            ```
            ë¶„ì (ê³µí†µ ì˜¤ë‹µ ìˆ˜):
            â†’ ì¶”ì¶œ ê°€ëŠ¥í•œ ì˜¤ë‹µë§Œ ì¹´ìš´íŠ¸ (nan ì œì™¸)
            
            ë¶„ëª¨ (ì „ì²´ ì˜¤ë‹µ ìˆ˜):
            â†’ ì˜¤ë‹µë¥ ë¡œ ê³„ì‚°ëœ ì „ì²´ ì˜¤ë‹µ (nan í¬í•¨)
            
            ì´ìœ :
            - nanì€ "ë¬´ì—‡ì„ ì„ íƒí–ˆëŠ”ì§€ ëª¨ë¦„"
            - ë¶„ëª¨ì—ëŠ” í¬í•¨ (ì „ì²´ ì˜¤ë‹µ ìˆ˜)
            - ë¶„ìì—ëŠ” ì œì™¸ (í™•ì¸ ë¶ˆê°€)
            â†’ ì¼ê´€ì„±ì´ ë‚®ì•„ì§ (ë°ì´í„° í’ˆì§ˆ ë°˜ì˜)
            ```
            
            **4ë‹¨ê³„: ì¼ê´€ì„± 50% ì´ìƒë§Œ í‘œì‹œ**
            ```
            âœ… 75% ì¼ê´€ì„± â†’ í‘œì‹œ (ì˜ë¯¸ ìˆëŠ” íŒ¨í„´)
            âœ… 100% ì¼ê´€ì„± â†’ í‘œì‹œ (ì™„ë²½í•œ íŒ¨í„´)
            âŒ 40% ì¼ê´€ì„± â†’ ì œì™¸ (ìš°ì—°ì¼ ê°€ëŠ¥ì„±)
            ```
            
            ### ğŸ’¡ ì™œ ì˜¤ë‹µë¥  50% ì´ìƒë§Œ?
            
            **ë¬¸ì œ ì‚¬ë¡€**:
            ```
            ë¬¸ì œ A (ì˜¤ë‹µë¥  10%):
            - 10ê°œ ëª¨ë¸ ì¤‘ 1ê°œë§Œ í‹€ë¦¼
            - ì˜¤ë‹µ: [3]
            - ì¼ê´€ì„±: 100% âŒ ì˜ë¯¸ ì—†ìŒ!
            
            ë¬¸ì œ B (ì˜¤ë‹µë¥  80%):
            - 10ê°œ ëª¨ë¸ ì¤‘ 8ê°œ í‹€ë¦¼
            - ì˜¤ë‹µ: [3, 3, 3, 3, 3, 3, 3, 4]
            - ì¼ê´€ì„±: 87.5% âœ… ì˜ë¯¸ ìˆìŒ!
            ```
            
            **ê²°ë¡ **: 
            - **ë¬´ì‘ìœ„ ì˜¤ë‹µ**: ê° ëª¨ë¸ì´ ë‹¤ë¥¸ ë‹µ ì„ íƒ â†’ ìš°ì—°
            - **ì¼ê´€ëœ ì˜¤ë‹µ**: ì—¬ëŸ¬ ëª¨ë¸ì´ ê°™ì€ ë‹µ ì„ íƒ â†’ ì²´ê³„ì  ì˜¤í•´!
            
            ì¼ê´€ëœ ì˜¤ë‹µì€ íŠ¹ì • ê°œë…ì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì˜¤í•´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
            """)
        
        st.info("""
        ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: ëª¨ë¸ë“¤ì´ ë‹¨ìˆœíˆ í‹€ë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê°™ì€ ì˜¤ë‹µì„ ì¼ê´€ë˜ê²Œ ì„ íƒ**í•˜ëŠ” ê²½ìš° 
        ì´ëŠ” í•´ë‹¹ ì§€ì‹ ì˜ì—­ì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì´í•´ ë¶€ì¡±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. (ë…¼ë¬¸ ë°©ë²•ë¡  ì ìš©)
        
        âš ï¸ **ì¤‘ìš”**: ì˜¤ë‹µë¥  50% ì´ìƒ ë¬¸ì œë§Œ ë¶„ì„í•©ë‹ˆë‹¤ (ì†Œìˆ˜ ëª¨ë¸ ì˜¤ë‹µì€ ìš°ì—°ì¼ ê°€ëŠ¥ì„±)
        """)
        
        # ì¼ê´€ëœ ì˜¤ë‹µ íŒ¨í„´ ê³„ì‚°
        consistent_wrong_patterns = []
        extraction_failures = []  # ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨ ì¶”ì 
        
        # âš¡ ìµœì í™”: ì¡°ê±´ì— ë§ëŠ” ë¬¸ì œë§Œ ë¯¸ë¦¬ í•„í„°ë§ (ì „ì²´ ë£¨í”„ ë°©ì§€)
        high_incorrect = problem_analysis[
            (problem_analysis['incorrect_rate'] >= 0.5) & (problem_analysis['incorrect_count'] >= 2)
        ]
        
        for idx, row in high_incorrect.iterrows():
            selected = row['selected_answers']
            correct = row['CorrectAnswer']
                
            # ì „ì²´ í‰ê°€ ëª¨ë¸ ìˆ˜
            total_models = row['total_count']
                
            # ğŸ” ì˜¬ë°”ë¥¸ ì „ì²´ ì˜¤ë‹µ ëª¨ë¸ ìˆ˜ ê³„ì‚°
            total_incorrect_models = row['incorrect_count']  # problem_analysisì—ì„œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
                
            # ë‹µì•ˆ ì¶”ì¶œ í†µê³„
            valid_answers = 0
            nan_count = 0
            empty_count = 0
                
            # ì˜¤ë‹µì„ ì„ íƒí•œ ëª¨ë¸ë“¤ì˜ ë‹µë³€ ìˆ˜ì§‘
            wrong_answers = []
            wrong_answer_models = []  # ì˜¤ë‹µ ëª¨ë¸ ì¶”ì 
                
            for model, answer in selected.items():
                if pd.isna(answer):
                    nan_count += 1
                elif str(answer).strip() == '':
                    empty_count += 1
                elif str(answer) != str(correct):
                    # ì •ë‹µì´ ì•„ë‹ˆë©´ ì˜¤ë‹µ
                    wrong_answers.append(str(answer).strip())
                    wrong_answer_models.append(model)
                    valid_answers += 1
                else:
                    valid_answers += 1
                
            # ğŸ” ì¶”ì¶œ ì‹¤íŒ¨ ê¸°ë¡
            if nan_count > 0 or empty_count > 0:
                extraction_failures.append({
                    'problem_id': row['problem_id'],
                    'total_models': total_models,
                    'valid_answers': valid_answers,
                    'nan_count': nan_count,
                    'empty_count': empty_count,
                    'extraction_rate': (valid_answers / total_models * 100) if total_models > 0 else 0
                })
                
            if wrong_answers and len(wrong_answers) >= 2:
                from collections import Counter
                answer_counts = Counter(wrong_answers)
                most_common_wrong, count = answer_counts.most_common(1)[0]
                    
                # â­â­â­ í•µì‹¬ ìˆ˜ì •: ì˜¬ë°”ë¥¸ ì¼ê´€ì„± ê³„ì‚°
                # ì¼ê´€ì„± = (ê°€ì¥ ë§ì´ ì„ íƒëœ ì˜¤ë‹µ ìˆ˜) / (ì „ì²´ ì˜¤ë‹µ ëª¨ë¸ ìˆ˜)
                # 
                # ì¤‘ìš”: nan ì²˜ë¦¬
                # - ë¶„ì(count): nan ì œì™¸ (ì¶”ì¶œ ê°€ëŠ¥í•œ ê²ƒë§Œ)
                # - ë¶„ëª¨(total_incorrect_models): nan í¬í•¨ (ì „ì²´ ì˜¤ë‹µ)
                # 
                # ì˜ˆì‹œ: ì „ì²´ ì˜¤ë‹µ 6ê°œ, ê³µí†µ ì˜¤ë‹µ 4ê°œ, nan 2ê°œ
                # â†’ ì¼ê´€ì„± = 4/6 = 66.7%
                # â†’ ì˜ë¯¸: "6ê°œ ì˜¤ë‹µ ì¤‘ 4ê°œê°€ ë™ì¼ ë‹µ ì„ íƒ, 2ê°œëŠ” í™•ì¸ ë¶ˆê°€"
                consistency_ratio = count / total_incorrect_models if total_incorrect_models > 0 else 0
                    
                # ğŸ” ê²€ì¦ 1: ì¶”ì¶œëœ ì˜¤ë‹µ ìˆ˜ vs ê¸°ë¡ëœ ì˜¤ë‹µ ìˆ˜
                if len(wrong_answers) != total_incorrect_models:
                    st.sidebar.warning(
                        f"âš ï¸ ë¬¸ì œ {row['problem_id']}: ì˜¤ë‹µ ìˆ˜ ë¶ˆì¼ì¹˜\n"
                        f"   ì¶”ì¶œ: {len(wrong_answers)}ê°œ, ê¸°ë¡: {total_incorrect_models}ê°œ\n"
                        f"   â†’ nan/ë¹ˆë‹µì•ˆ: {nan_count + empty_count}ê°œ"
                    )
                    
                # ğŸ” ê²€ì¦ 2: ì¼ê´€ì„± ë¹„ìœ¨ì´ 1.0 ì´ˆê³¼í•˜ë©´ ì˜¤ë¥˜
                if consistency_ratio > 1.0:
                    st.sidebar.error(
                        f"âŒ ë¬¸ì œ {row['problem_id']}: ì¼ê´€ì„± ê³„ì‚° ì˜¤ë¥˜!\n"
                        f"   ê³µí†µ ì˜¤ë‹µ: {count}ê°œ, ì „ì²´ ì˜¤ë‹µ: {total_incorrect_models}ê°œ\n"
                        f"   â†’ {count}/{total_incorrect_models} = {consistency_ratio:.2%}"
                    )
                    consistency_ratio = 1.0  # ìµœëŒ€ê°’ìœ¼ë¡œ ë³´ì •
                    
                if consistency_ratio >= 0.5:
                    models_selected_this = [m for m, a in selected.items() 
                                          if pd.notna(a) and str(a).strip() == most_common_wrong]
                        
                    consistent_wrong_patterns.append({
                        'problem_id': row['problem_id'],
                        'Question': row['Question'],
                        'Subject': row['Subject'],
                        'Year': row['Year'],
                        'correct_answer': str(correct).strip(),
                        'common_wrong_answer': most_common_wrong,
                        'wrong_answer_count': count,  # ê³µí†µ ì˜¤ë‹µ ìˆ˜
                        'total_wrong': total_incorrect_models,  # â­ ì „ì²´ ì˜¤ë‹µ ëª¨ë¸ ìˆ˜ (ì˜¬ë°”ë¥¸ ê°’)
                        'consistency_ratio': consistency_ratio,  # â­ ì˜¬ë°”ë¥¸ ì¼ê´€ì„±
                        'models_with_this_wrong': ', '.join(models_selected_this),
                        'incorrect_rate': row['incorrect_rate'],
                        'total_models': total_models,
                        'valid_answers': valid_answers,
                        'nan_count': nan_count,
                        'empty_count': empty_count,
                        'extracted_wrong_count': len(wrong_answers)  # ì‹¤ì œ ì¶”ì¶œëœ ì˜¤ë‹µ ìˆ˜
                    })
        
        # ğŸš¨ ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨ í†µê³„ í‘œì‹œ
        if extraction_failures:
            with st.expander(f"âš ï¸ ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨ í†µê³„ ({len(extraction_failures)}ê°œ ë¬¸ì œ)"):
                failure_df = pd.DataFrame(extraction_failures)
                failure_df = failure_df.sort_values('nan_count', ascending=False)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    total_nan = failure_df['nan_count'].sum()
                    st.metric("ì´ nan ê°œìˆ˜", f"{total_nan}ê°œ")
                with col2:
                    total_empty = failure_df['empty_count'].sum()
                    st.metric("ì´ ë¹ˆ ë‹µì•ˆ ê°œìˆ˜", f"{total_empty}ê°œ")
                with col3:
                    avg_extraction = failure_df['extraction_rate'].mean()
                    st.metric("í‰ê·  ì¶”ì¶œ ì„±ê³µë¥ ", f"{avg_extraction:.1f}%")
                
                st.warning("""
                ğŸ’¡ **ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨ ì›ì¸**:
                - **nan**: ëª¨ë¸ì´ ë‹µì„ ì¶”ì¶œí•˜ì§€ ëª»í•¨ (íŒŒì‹± ì˜¤ë¥˜, í˜•ì‹ ë¶ˆì¼ì¹˜)
                - **ë¹ˆ ë‹µì•ˆ**: ëª¨ë¸ì´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜
                
                **ì˜í–¥**:
                - ì˜¤ë‹µë¥  ê³„ì‚° ì‹œ ë¶„ëª¨ê°€ ê°ì†Œ
                - ì¼ê´€ì„± ê³„ì‚°ì—ì„œ ì œì™¸ë¨
                
                **ì¡°ì¹˜**:
                - ë²¤ì¹˜ë§ˆí¬ ë¡œê·¸ í™•ì¸
                - ë‹µì•ˆ ì¶”ì¶œ ë¡œì§ ê°œì„ 
                - í•´ë‹¹ ë¬¸ì œ ì¬ì‹¤í–‰ ê³ ë ¤
                """)
                
                st.dataframe(
                    failure_df.head(20).style.format({
                        'extraction_rate': '{:.1f}%'
                    }).background_gradient(
                        subset=['nan_count'],
                        cmap='Reds'
                    ),
                    width='stretch'
                )
        
        if consistent_wrong_patterns:
            consistent_df = pd.DataFrame(consistent_wrong_patterns)
            consistent_df = consistent_df.sort_values('consistency_ratio', ascending=False)
            
            # 100% ì¼ê´€ì„±ê³¼ 50-99% ì¼ê´€ì„± êµ¬ë¶„
            perfect_consistency = consistent_df[consistent_df['consistency_ratio'] == 1.0]
            high_but_not_perfect = consistent_df[consistent_df['consistency_ratio'] < 1.0]
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì¼ê´€ëœ ì˜¤ë‹µ íŒ¨í„´", f"{len(consistent_df)}ê°œ",
                         help="ì˜¤ë‹µë¥  50% ì´ìƒ & ì¼ê´€ì„± 50% ì´ìƒ ë¬¸ì œ")
            with col2:
                st.metric("100% ì¼ê´€ì„±", f"{len(perfect_consistency)}ê°œ", 
                         delta=f"{len(perfect_consistency)/len(consistent_df)*100:.1f}%",
                         help="ëª¨ë“  ì˜¤ë‹µ ëª¨ë¸ì´ ê°™ì€ ë‹µì„ ì„ íƒ")
            with col3:
                st.metric("50-99% ì¼ê´€ì„±", f"{len(high_but_not_perfect)}ê°œ",
                         delta=f"{len(high_but_not_perfect)/len(consistent_df)*100:.1f}%",
                         help="ëŒ€ë¶€ë¶„ ì˜¤ë‹µ ëª¨ë¸ì´ ê°™ì€ ë‹µì„ ì„ íƒ")
            
            st.success(f"""
            âœ… **{len(consistent_df)}ê°œì˜ ì¼ê´€ëœ ì˜¤ë‹µ íŒ¨í„´ ë°œê²¬!** (ì˜¤ë‹µë¥  â‰¥50% ë¬¸ì œ ì¤‘)
            
            ì´ëŠ” íŠ¹ì • ëª¨ë¸ì˜ ë¬¸ì œê°€ ì•„ë‹Œ, **ì—¬ëŸ¬ LLMì— ê³µí†µì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê³µí†µ ì˜¤ë‹µ**ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
            """)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.histogram(
                    consistent_df,
                    x='consistency_ratio',
                    nbins=20,
                    title='ì¼ê´€ëœ ì˜¤ë‹µ ì„ íƒ ë¹„ìœ¨ ë¶„í¬' if lang == 'ko' else 'Consistency Ratio Distribution',
                    labels={'consistency_ratio': 'ì¼ê´€ì„± ë¹„ìœ¨' if lang == 'ko' else 'Consistency Ratio'}
                )
                fig.update_traces(marker_line_color='black', marker_line_width=1.5)
                fig.update_layout(
                    xaxis_title='ì¼ê´€ì„± ë¹„ìœ¨ (1.0 = 100% ì¼ì¹˜)',
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    height=400
                )
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                if 'Subject' in consistent_df.columns:
                    subject_pattern_count = consistent_df['Subject'].value_counts().reset_index()
                    subject_pattern_count.columns = ['Subject', 'Count']
                    
                    fig = px.bar(
                        subject_pattern_count.head(10),
                        x='Subject',
                        y='Count',
                        title='ê³¼ëª©ë³„ ì¼ê´€ëœ ì˜¤ë‹µ íŒ¨í„´' if lang == 'ko' else 'Consistent Wrong Patterns by Subject',
                        color='Count',
                        color_continuous_scale='Reds'
                    )
                    fig.update_traces(marker_line_color='black', marker_line_width=1.5)
                    fig.update_layout(height=400)
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, width='stretch')
            
            # íƒ­ìœ¼ë¡œ 100% / 50-99% êµ¬ë¶„
            tab1, tab2 = st.tabs([
                f"ğŸ”´ 100% ì¼ê´€ì„± ({len(perfect_consistency)}ê°œ)",
                f"ğŸŸ  50-99% ì¼ê´€ì„± ({len(high_but_not_perfect)}ê°œ)"
            ])
            
            with tab1:
                st.markdown("#### " + ("ëª¨ë“  ì˜¤ë‹µ ëª¨ë¸ì´ ê°™ì€ ë‹µì„ ì„ íƒí•œ ë¬¸ì œ" if lang == 'ko' else "All Wrong Models Selected Same Answer"))
                st.caption("âš ï¸ ì˜¤ë‹µë¥  50% ì´ìƒ ë¬¸ì œë§Œ í¬í•¨")
                
                if len(perfect_consistency) > 0:
                    display_perfect = perfect_consistency.copy()
                    display_perfect['ì¼ê´€ì„±_pct'] = 100.0
                    display_perfect['ì˜¤ë‹µë¥ _pct'] = (display_perfect['incorrect_rate'] * 100).round(1)
                    display_perfect['ì˜¤ë‹µ_ì •ë³´'] = (display_perfect['common_wrong_answer'].astype(str) + 
                                                   ' (' + display_perfect['wrong_answer_count'].astype(str) + 
                                                   '/' + display_perfect['total_wrong'].astype(str) + ')')
                    
                    # ğŸ”¥ ì˜¤ë‹µë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    display_perfect = display_perfect.sort_values('incorrect_rate', ascending=False)
                    
                    # ğŸ” ê²€ì¦: ì˜¤ë‹µë¥  ê³„ì‚° í™•ì¸
                    display_perfect['ê²€ì¦_ì˜¤ë‹µë¥ '] = (display_perfect['total_wrong'] / display_perfect['total_models'] * 100).round(1)
                    display_perfect['ê²€ì¦_ì¼ì¹˜'] = (display_perfect['ì˜¤ë‹µë¥ _pct'] - display_perfect['ê²€ì¦_ì˜¤ë‹µë¥ ']).abs() < 1.0
                    
                    display_df = pd.DataFrame({
                        'ë¬¸ì œ ë²ˆí˜¸' if lang == 'ko' else 'Problem ID': display_perfect['problem_id'],
                        'ê³¼ëª©' if lang == 'ko' else 'Subject': display_perfect['Subject'],
                        'ì˜¤ë‹µë¥  (%)': display_perfect['ì˜¤ë‹µë¥ _pct'],
                        'ì •ë‹µ' if lang == 'ko' else 'Correct': display_perfect['correct_answer'],
                        'ê³µí†µ ì˜¤ë‹µ (íšŸìˆ˜/ì „ì²´)': display_perfect['ì˜¤ë‹µ_ì •ë³´'],
                        'ì¼ê´€ì„± (%)': display_perfect['ì¼ê´€ì„±_pct'],
                        'í‰ê°€ ëª¨ë¸ìˆ˜': display_perfect['total_models'],
                        'í•´ë‹¹ ì˜¤ë‹µ ì„ íƒ ëª¨ë¸': display_perfect['models_with_this_wrong']
                    })
                    
                    # ğŸš¨ ê²€ì¦ ì‹¤íŒ¨ ê²½ê³ 
                    if not display_perfect['ê²€ì¦_ì¼ì¹˜'].all():
                        st.warning(f"""
                        âš ï¸ **ë°ì´í„° ë¶ˆì¼ì¹˜ ê²½ê³ **: ì¼ë¶€ ë¬¸ì œì—ì„œ ì˜¤ë‹µë¥ ê³¼ ì‹¤ì œ ì˜¤ë‹µ ìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.
                        
                        ê°€ëŠ¥í•œ ì›ì¸:
                        - ì¼ë¶€ ëª¨ë¸ì˜ ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨ (nan ê°’)
                        - íŠ¹ì • ëª¨ë¸ì´ í•´ë‹¹ ë¬¸ì œë¥¼ í‰ê°€í•˜ì§€ ì•ŠìŒ
                        - ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ì¸í•œ ëª¨ë¸ ìˆ˜ ë³€í™”
                        
                        ğŸ’¡ "í‰ê°€ ëª¨ë¸ìˆ˜" ì»¬ëŸ¼ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ í‰ê°€ëœ ëª¨ë¸ ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.
                        """)
                    
                    st.dataframe(
                        display_df.style.background_gradient(
                            subset=['ì˜¤ë‹µë¥  (%)'],
                            cmap='Reds',
                            vmin=50,
                            vmax=100
                        ).format({
                            'ì¼ê´€ì„± (%)': '{:.1f}%',
                            'ì˜¤ë‹µë¥  (%)': '{:.1f}%',
                            'í‰ê°€ ëª¨ë¸ìˆ˜': '{:.0f}'
                        }),
                        width='stretch',
                        height=500
                    )
                    
                    if st.checkbox('ğŸ“‹ ' + ('100% ì¼ê´€ì„± ë¬¸ì œ ìƒì„¸ ë³´ê¸°' if lang == 'ko' else 'Show Details'), key='perfect_details'):
                        st.info(f"ğŸ’¡ ì´ {len(display_perfect)}ê°œ ë¬¸ì œì˜ ìƒì„¸ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤. (ì˜¤ë‹µë¥  ë†’ì€ ìˆœ)")
                        for idx, row in display_perfect.iterrows():  # ğŸ”¥ head(20) ì œê±° - ì „ì²´ í‘œì‹œ
                            with st.expander(f"ğŸ” {row['problem_id']} - ì¼ê´€ì„± 100% (ì˜¤ë‹µë¥  {row['incorrect_rate']*100:.1f}%)"):
                                q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                                
                                st.markdown(f"**{'ë¬¸ì œ' if lang == 'ko' else 'Question'}:** {q_detail['Question']}")
                                st.markdown(f"**{'ê³¼ëª©' if lang == 'ko' else 'Subject'}:** {row['Subject']}")
                                st.markdown(f"**ì˜¤ë‹µë¥ :** {row['incorrect_rate']*100:.1f}%")
                                
                                if all([f'Option {i}' in q_detail for i in range(1, 5)]):
                                    st.markdown("**ì„ íƒì§€:**")
                                    for i in range(1, 5):
                                        option = q_detail[f'Option {i}']
                                        if pd.notna(option):
                                            if str(i) == str(row['correct_answer']):
                                                st.markdown(f"âœ… **{i}. {option}** (ì •ë‹µ)")
                                            elif str(i) == str(row['common_wrong_answer']):
                                                st.markdown(f"âŒ **{i}. {option}** (ëª¨ë“  ì˜¤ë‹µ ëª¨ë¸ì´ ì„ íƒ)")
                                            else:
                                                st.markdown(f"  {i}. {option}")
                                
                                st.markdown(f"**ğŸ¯ ì •ë‹µ:** {row['correct_answer']}")
                                st.markdown(f"**âŒ ê³µí†µ ì˜¤ë‹µ:** {row['common_wrong_answer']} (ëª¨ë“  {row['total_wrong']}ê°œ ì˜¤ë‹µ ëª¨ë¸)")
                                st.markdown(f"**ğŸ¤– í•´ë‹¹ ì˜¤ë‹µ ì„ íƒ ëª¨ë¸:** {row['models_with_this_wrong']}")
                else:
                    st.info("100% ì¼ê´€ì„± íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            with tab2:
                st.markdown("#### " + ("50-99%ì˜ ì˜¤ë‹µ ëª¨ë¸ì´ ê°™ì€ ë‹µì„ ì„ íƒí•œ ë¬¸ì œ" if lang == 'ko' else "50-99% Wrong Models Selected Same Answer"))
                st.caption("âš ï¸ ì˜¤ë‹µë¥  50% ì´ìƒ ë¬¸ì œë§Œ í¬í•¨")
                
                if len(high_but_not_perfect) > 0:
                    display_high = high_but_not_perfect.copy()
                    display_high['ì¼ê´€ì„±_pct'] = (display_high['consistency_ratio'] * 100).round(1)
                    display_high['ì˜¤ë‹µë¥ _pct'] = (display_high['incorrect_rate'] * 100).round(1)
                    display_high['ì˜¤ë‹µ_ì •ë³´'] = (display_high['common_wrong_answer'].astype(str) + 
                                               ' (' + display_high['wrong_answer_count'].astype(str) + 
                                               '/' + display_high['total_wrong'].astype(str) + ')')
                    
                    # ğŸ”¥ ì˜¤ë‹µë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    display_high = display_high.sort_values('incorrect_rate', ascending=False)
                    
                    display_df = pd.DataFrame({
                        'ë¬¸ì œ ë²ˆí˜¸' if lang == 'ko' else 'Problem ID': display_high['problem_id'],
                        'ê³¼ëª©' if lang == 'ko' else 'Subject': display_high['Subject'],
                        'ì˜¤ë‹µë¥  (%)': display_high['ì˜¤ë‹µë¥ _pct'],
                        'ì •ë‹µ' if lang == 'ko' else 'Correct': display_high['correct_answer'],
                        'ê³µí†µ ì˜¤ë‹µ (íšŸìˆ˜/ì „ì²´)': display_high['ì˜¤ë‹µ_ì •ë³´'],
                        'ì¼ê´€ì„± (%)': display_high['ì¼ê´€ì„±_pct'],
                        'í‰ê°€ ëª¨ë¸ìˆ˜': display_high['total_models'],
                        'í•´ë‹¹ ì˜¤ë‹µ ì„ íƒ ëª¨ë¸': display_high['models_with_this_wrong']
                    })
                    
                    st.dataframe(
                        display_df.style.background_gradient(
                            subset=['ì¼ê´€ì„± (%)'],
                            cmap='Oranges',
                            vmin=50,
                            vmax=100
                        ).format({
                            'ì¼ê´€ì„± (%)': '{:.1f}%',
                            'ì˜¤ë‹µë¥  (%)': '{:.1f}%',
                            'í‰ê°€ ëª¨ë¸ìˆ˜': '{:.0f}'
                        }),
                        width='stretch',
                        height=500
                    )
                    
                    if st.checkbox('ğŸ“‹ ' + ('50-99% ì¼ê´€ì„± ë¬¸ì œ ìƒì„¸ ë³´ê¸°' if lang == 'ko' else 'Show Details'), key='high_details'):
                        st.info(f"ğŸ’¡ ì´ {len(display_high)}ê°œ ë¬¸ì œì˜ ìƒì„¸ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤. (ì˜¤ë‹µë¥  ë†’ì€ ìˆœ)")
                        for idx, row in display_high.iterrows():  # ğŸ”¥ head(30) ì œê±° - ì „ì²´ í‘œì‹œ
                            with st.expander(f"ğŸ” {row['problem_id']} - ì¼ê´€ì„± {row['consistency_ratio']*100:.1f}% (ì˜¤ë‹µë¥  {row['incorrect_rate']*100:.1f}%)"):
                                q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                                
                                st.markdown(f"**{'ë¬¸ì œ' if lang == 'ko' else 'Question'}:** {q_detail['Question']}")
                                st.markdown(f"**{'ê³¼ëª©' if lang == 'ko' else 'Subject'}:** {row['Subject']}")
                                st.markdown(f"**ì˜¤ë‹µë¥ :** {row['incorrect_rate']*100:.1f}%")
                                
                                if all([f'Option {i}' in q_detail for i in range(1, 5)]):
                                    st.markdown("**ì„ íƒì§€:**")
                                    for i in range(1, 5):
                                        option = q_detail[f'Option {i}']
                                        if pd.notna(option):
                                            if str(i) == str(row['correct_answer']):
                                                st.markdown(f"âœ… **{i}. {option}** (ì •ë‹µ)")
                                            elif str(i) == str(row['common_wrong_answer']):
                                                st.markdown(f"âŒ **{i}. {option}** (ì¼ê´€ëœ ì˜¤ë‹µ - {row['wrong_answer_count']}ê°œ ëª¨ë¸)")
                                            else:
                                                st.markdown(f"  {i}. {option}")
                                
                                st.markdown(f"**ğŸ¯ ì •ë‹µ:** {row['correct_answer']}")
                                st.markdown(f"**âŒ ê³µí†µ ì˜¤ë‹µ:** {row['common_wrong_answer']} ({row['wrong_answer_count']}/{row['total_wrong']} = {row['consistency_ratio']*100:.1f}% ì¼ê´€ì„±)")
                                st.markdown(f"**ğŸ¤– í•´ë‹¹ ì˜¤ë‹µ ì„ íƒ ëª¨ë¸:** {row['models_with_this_wrong']}")
                else:
                    st.info("50-99% ì¼ê´€ì„± íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì¼ê´€ëœ ì˜¤ë‹µ ì„ íƒ íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì˜¤ë‹µë¥  50% ì´ìƒ & ì¼ê´€ì„± 50% ì´ìƒ ë¬¸ì œ ì—†ìŒ)")
        
        # ì„¹ì…˜ 3: í”„ë¡¬í”„íŒ… ë°©ì‹ë³„ ê³µí†µ ì˜¤ë‹µ ë¹„êµ
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ“‹ " + ("í”„ë¡¬í”„íŒ… ë°©ì‹ë³„ ê³µí†µ ì˜¤ë‹µ ë¶„ì„" if lang == 'ko' else "Common Wrong Answer by Prompting"))
        
        if 'í”„ë¡¬í”„íŒ…' in filtered_df.columns and filtered_df['í”„ë¡¬í”„íŒ…'].nunique() > 1:
            st.info("""
            ğŸ’¡ **ë…¼ë¬¸ ë°©ë²•ë¡ **: íŠ¹ì • í”„ë¡¬í”„íŒ… ë°©ì‹ì—ì„œ ëª¨ë¸ë“¤ì´ ì¼ê´€ë˜ê²Œ í‹€ë¦¬ëŠ” ë¬¸ì œë¥¼ ì‹ë³„
            """)
            
            prompting_analysis = []
            
            for prompting in filtered_df['í”„ë¡¬í”„íŒ…'].unique():
                prompt_df = filtered_df[filtered_df['í”„ë¡¬í”„íŒ…'] == prompting]
                prompt_problems = prompt_df.groupby('Question').agg({'ì •ë‹µì—¬ë¶€': ['sum', 'count']}).reset_index()
                prompt_problems.columns = ['Question', 'correct_count', 'total_count']
                all_wrong_in_prompt = len(prompt_problems[prompt_problems['correct_count'] == 0])
                avg_acc = prompt_df['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                prompting_analysis.append({
                    'í”„ë¡¬í”„íŒ…': prompting,
                    'ì „ì²´_ë¬¸ì œìˆ˜': prompt_df['Question'].nunique(),
                    'ì™„ì „_ì§€ì‹ê²©ì°¨': all_wrong_in_prompt,
                    'í‰ê· _ì •í™•ë„': avg_acc,
                    'ì§€ì‹ê²©ì°¨_ë¹„ìœ¨': (all_wrong_in_prompt / prompt_df['Question'].nunique() * 100) if prompt_df['Question'].nunique() > 0 else 0
                })
            
            prompt_comp_df = pd.DataFrame(prompting_analysis).sort_values('ì™„ì „_ì§€ì‹ê²©ì°¨', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.bar(
                    prompt_comp_df,
                    x='í”„ë¡¬í”„íŒ…',
                    y='ì™„ì „_ì§€ì‹ê²©ì°¨',
                    title='í”„ë¡¬í”„íŒ… ë°©ì‹ë³„ ì™„ì „ ê³µí†µ ì˜¤ë‹µ',
                    text='ì™„ì „_ì§€ì‹ê²©ì°¨',
                    color='ì™„ì „_ì§€ì‹ê²©ì°¨',
                    color_continuous_scale='Reds'
                )
                fig.update_traces(textposition='outside', marker_line_color='black', marker_line_width=1.5)
                fig.update_layout(height=400)
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                fig = px.scatter(
                    prompt_comp_df,
                    x='í‰ê· _ì •í™•ë„',
                    y='ì§€ì‹ê²©ì°¨_ë¹„ìœ¨',
                    size='ì „ì²´_ë¬¸ì œìˆ˜',
                    text='í”„ë¡¬í”„íŒ…',
                    title='ì •í™•ë„ vs ì§€ì‹ê²©ì°¨ ë¹„ìœ¨',
                    labels={'í‰ê· _ì •í™•ë„': 'í‰ê·  ì •í™•ë„ (%)', 'ì§€ì‹ê²©ì°¨_ë¹„ìœ¨': 'ì§€ì‹ê²©ì°¨ ë¹„ìœ¨ (%)'}
                )
                fig.update_traces(textposition='top center', marker=dict(line=dict(width=2, color='black')))
                fig.update_layout(height=400)
                st.plotly_chart(fig, width='stretch')
            
            st.dataframe(
                prompt_comp_df.style.format({
                    'í‰ê· _ì •í™•ë„': '{:.2f}%',
                    'ì§€ì‹ê²©ì°¨_ë¹„ìœ¨': '{:.2f}%'
                }).background_gradient(subset=['ì™„ì „_ì§€ì‹ê²©ì°¨'], cmap='Reds'),
                width='stretch'
            )
        else:
            st.info("í”„ë¡¬í”„íŒ… ë°©ì‹ì´ 1ê°œë§Œ ì„ íƒë˜ì–´ ë¹„êµ ë¶ˆê°€")
        
        # ========================================
        # ì„¹ì…˜ 4: ëª¨ë¸ ê°„ ì˜¤ë‹µ ì¼ì¹˜ë„ ë§¤íŠ¸ë¦­ìŠ¤
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ”— " + ("ëª¨ë¸ ê°„ ì˜¤ë‹µ ì¼ì¹˜ë„ ë§¤íŠ¸ë¦­ìŠ¤" if lang == 'ko' else "Inter-Model Error Agreement"))
        
        st.info("""
        ğŸ’¡ **ë¶„ì„ ëª©ì **: ì–´ë–¤ ëª¨ë¸ë“¤ì´ ìœ ì‚¬í•œ ì‹¤ìˆ˜ë¥¼ í•˜ëŠ”ì§€ íŒŒì•…
        ë†’ì€ ì¼ì¹˜ë„ = ìœ ì‚¬í•œ ê³µí†µ ì˜¤ë‹µ ê³µìœ 
        """)
        
        models = filtered_df['ëª¨ë¸'].unique().tolist()
        
        if len(models) >= 2:
            problem_model_matrix = filtered_df.pivot_table(
                index='Question',
                columns='ëª¨ë¸',
                values='ì •ë‹µì—¬ë¶€',
                aggfunc='first'
            ).fillna(0)
            
            agreement_matrix = pd.DataFrame(index=models, columns=models, dtype=float)
            
            for model1 in models:
                for model2 in models:
                    if model1 in problem_model_matrix.columns and model2 in problem_model_matrix.columns:
                        both_wrong = ((problem_model_matrix[model1] == 0) & (problem_model_matrix[model2] == 0)).sum()
                        either_wrong = ((problem_model_matrix[model1] == 0) | (problem_model_matrix[model2] == 0)).sum()
                        agreement = both_wrong / either_wrong if either_wrong > 0 else 0
                        agreement_matrix.loc[model1, model2] = agreement * 100
                    else:
                        agreement_matrix.loc[model1, model2] = 0
            
            agreement_matrix = agreement_matrix.astype(float)
            
            fig = go.Figure(data=go.Heatmap(
                z=agreement_matrix.values,
                x=agreement_matrix.columns,
                y=agreement_matrix.index,
                colorscale='RdYlBu_r',
                text=np.round(agreement_matrix.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": int(12 * chart_text_size)},
                colorbar=dict(title="ì¼ì¹˜ë„ (%)"),
                xgap=2,
                ygap=2
            ))
            
            fig.update_layout(
                title='ëª¨ë¸ ê°„ ì˜¤ë‹µ ì¼ì¹˜ë„ (%)',
                height=max(400, len(models) * 40)
            )
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, width='stretch')
            
            agreement_pairs = []
            for i, model1 in enumerate(models):
                for j, model2 in enumerate(models):
                    if i < j:
                        agreement_pairs.append({
                            'Model 1': model1,
                            'Model 2': model2,
                            'Agreement': agreement_matrix.loc[model1, model2]
                        })
            
            if agreement_pairs:
                pairs_df = pd.DataFrame(agreement_pairs).sort_values('Agreement', ascending=False)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ê°€ì¥ ìœ ì‚¬í•œ ì˜¤ë‹µ íŒ¨í„´ (Top 5)**")
                    for idx, row in pairs_df.head(5).iterrows():
                        st.write(f"ğŸ”— **{row['Model 1']}** â†” **{row['Model 2']}**: {row['Agreement']:.1f}%")
                
                with col2:
                    st.markdown("**ê°€ì¥ ë‹¤ë¥¸ ì˜¤ë‹µ íŒ¨í„´ (Bottom 5)**")
                    for idx, row in pairs_df.tail(5).iterrows():
                        st.write(f"â†”ï¸ **{row['Model 1']}** â†” **{row['Model 2']}**: {row['Agreement']:.1f}%")
        else:
            st.warning("2ê°œ ì´ìƒì˜ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ========================================
        # ì„¹ì…˜ 5: ê³µí†µ ì˜¤ë‹µ ì˜ì—­ ë§¤í•‘
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ—ºï¸ " + ("ê³µí†µ ì˜¤ë‹µ ì˜ì—­ ë§¤í•‘" if lang == 'ko' else "Common Wrong Answer Domain Mapping"))
        
        all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
        
        if len(all_wrong) > 0:
            col1, col2 = st.columns(2)
            
            with col1:
                if 'Subject' in all_wrong.columns:
                    subject_gaps = all_wrong['Subject'].value_counts().reset_index()
                    subject_gaps.columns = ['Subject', 'Count']
                    total_by_subject = problem_analysis['Subject'].value_counts()
                    subject_gaps['Total'] = subject_gaps['Subject'].map(total_by_subject)
                    subject_gaps['Gap_Ratio'] = (subject_gaps['Count'] / subject_gaps['Total'] * 100).round(1)
                    
                    fig = px.bar(
                        subject_gaps.sort_values('Gap_Ratio', ascending=False),
                        x='Subject',
                        y='Gap_Ratio',
                        title='ê³¼ëª©ë³„ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨',
                        text='Gap_Ratio',
                        color='Gap_Ratio',
                        color_continuous_scale='Reds',
                        hover_data=['Count', 'Total']
                    )
                    fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside', marker_line_color='black', marker_line_width=1.5)
                    fig.update_layout(height=400, yaxis_title='ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨ (%)')
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, width='stretch')
            
            with col2:
                if 'Year' in all_wrong.columns:
                    year_gaps = all_wrong['Year'].value_counts().reset_index()
                    year_gaps.columns = ['Year', 'Count']
                    year_gaps = year_gaps[year_gaps['Year'] != 'Unknown']
                    
                    if len(year_gaps) > 0:
                        year_gaps['Year_Int'] = year_gaps['Year'].apply(safe_convert_to_int)
                        year_gaps = year_gaps[year_gaps['Year_Int'].notna()].sort_values('Year_Int')
                        
                        fig = px.line(
                            year_gaps,
                            x='Year_Int',
                            y='Count',
                            title='ì—°ë„ë³„ ê³µí†µ ì˜¤ë‹µ ë¬¸ì œ ìˆ˜',
                            markers=True,
                            text='Count'
                        )
                        fig.update_traces(textposition='top center', marker_size=10, marker_line_color='black', marker_line_width=2, line_width=3)
                        fig.update_layout(height=400, xaxis_title='ì—°ë„', yaxis_title='ë¬¸ì œ ìˆ˜')
                        st.plotly_chart(fig, width='stretch')
        
        # ========================================
        # ì„¹ì…˜ 6: Top 20 ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ“Š " + ("ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 20" if lang == 'ko' else "Top 20 Problems by Incorrect Rate"))
        
        top_20 = problem_analysis.head(20)
        
        display_top_20 = pd.DataFrame({
            ('ë¬¸ì œ ë²ˆí˜¸' if lang == 'ko' else 'Problem ID'): top_20['problem_id'],
            ('ê³¼ëª©' if lang == 'ko' else 'Subject'): top_20['Subject'],
            ('ì˜¤ë‹µ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Incorrect Count'): top_20['incorrect_count'].astype(int),
            ('ì •ë‹µ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Correct Count'): top_20['correct_count'].astype(int),
            ('ì´ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Total Models'): top_20['total_count'].astype(int),
            ('ì˜¤ë‹µë¥ ' if lang == 'ko' else 'Wrong Rate'): (top_20['incorrect_rate'] * 100).round(2),
            'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': top_20['correct_models'],
            'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': top_20['incorrect_models']
        })
        
        st.dataframe(
            display_top_20.style.background_gradient(
                subset=['ì˜¤ë‹µë¥ ' if lang == 'ko' else 'Wrong Rate'],
                cmap='Reds',
                vmin=0,
                vmax=100
            ),
            width='stretch',
            height=600
        )
        
        # ========================================
        # ì„¹ì…˜ 7: ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (ì™„ì „ ê³µí†µ ì˜¤ë‹µ)
        # ========================================
        st.markdown("---")
        st.subheader("ğŸš¨ " + ("ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (ì™„ì „ ê³µí†µ ì˜¤ë‹µ)" if lang == 'ko' else "All Models Incorrect (Complete Common Wrong Answer)"))
        
        all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
        
        if len(all_wrong) > 0:
            st.error(f"""
            âš ï¸ **ì‹¬ê°í•œ ê³µí†µ ì˜¤ë‹µ ë°œê²¬: {len(all_wrong)}ê°œ ë¬¸ì œ**
            
            ì´ ë¬¸ì œë“¤ì€ **ëª¨ë“  í‰ê°€ ëª¨ë¸ì´ í‹€ë ¸ìŠµë‹ˆë‹¤**. í˜„ì¬ LLMë“¤ì´ ê³µí†µì ìœ¼ë¡œ 
            í•´ë‹¹ ì§€ì‹ ì˜ì—­ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            """)
            
            display_all_wrong = pd.DataFrame({
                ('ë¬¸ì œ ë²ˆí˜¸' if lang == 'ko' else 'Problem ID'): all_wrong['problem_id'],
                ('ê³¼ëª©' if lang == 'ko' else 'Subject'): all_wrong['Subject'],
                ('ì—°ë„' if lang == 'ko' else 'Year'): all_wrong['Year'],
                ('ì˜¤ë‹µ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Incorrect Count'): all_wrong['incorrect_count'].astype(int),
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': all_wrong['incorrect_models']
            })
            
            st.dataframe(display_all_wrong, width='stretch', height=400)
            
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸° (ì™„ì „ ê³µí†µ ì˜¤ë‹µ)' if lang == 'ko' else 'Show Details (Complete Gap)', key='all_wrong_details'):
                st.info(f"ì´ {len(all_wrong)}ê°œ ë¬¸ì œì˜ ìƒì„¸ ë‚´ìš©")
                for idx, row in all_wrong.head(20).iterrows():
                    with st.expander(f"ğŸš¨ {row['problem_id']}"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**{'ë¬¸ì œ' if lang == 'ko' else 'Question'}:** {q_detail['Question']}")
                        
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©:** {q_detail['Subject']}")
                        
                        if all([f'Option {i}' in q_detail for i in range(1, 5)]):
                            st.write("**ì„ íƒì§€:**")
                            for i in range(1, 5):
                                option = q_detail[f'Option {i}']
                                if pd.notna(option):
                                    if str(i) == str(row['CorrectAnswer']):
                                        st.write(f"  âœ… {i}. {option} **(ì •ë‹µ)**")
                                    else:
                                        st.write(f"  {i}. {option}")
                        
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ:** {q_detail['Answer']}")
                        
                        st.write("**ê° ëª¨ë¸ì´ ì„ íƒí•œ ë‹µ:**")
                        for model, answer in row['selected_answers'].items():
                            st.write(f"  â€¢ {model}: {answer}")
        else:
            st.success("âœ… ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ========================================
        # ì„¹ì…˜ 8: ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (â‰¥50%) â­ ê¸°ì¡´ ê¸°ëŠ¥
        # ========================================
        st.markdown("---")
        st.subheader("âš ï¸ " + ("ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (â‰¥50%)" if lang == 'ko' else "Most Models Incorrect (â‰¥50%)"))
        
        most_wrong = problem_analysis[problem_analysis['incorrect_rate'] >= 0.5]
        
        if len(most_wrong) > 0:
            st.warning(f"""
            âš ï¸ **ì£¼ìš” ê³µí†µ ì˜¤ë‹µ: {len(most_wrong)}ê°œ ë¬¸ì œ**
            
            ì´ ë¬¸ì œë“¤ì€ **50% ì´ìƒì˜ ëª¨ë¸ì´ í‹€ë ¸ìŠµë‹ˆë‹¤**. í•´ë‹¹ ì§€ì‹ ì˜ì—­ì´ 
            ë§ì€ LLMì—ê²Œ ì–´ë ¤ìš´ ì˜ì—­ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            """)
            
            display_most_wrong = pd.DataFrame({
                ('ë¬¸ì œ ë²ˆí˜¸' if lang == 'ko' else 'Problem ID'): most_wrong['problem_id'],
                ('ê³¼ëª©' if lang == 'ko' else 'Subject'): most_wrong['Subject'],
                ('ì˜¤ë‹µ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Incorrect Count'): most_wrong['incorrect_count'].astype(int),
                ('ì •ë‹µ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Correct Count'): most_wrong['correct_count'].astype(int),
                ('ì´ ëª¨ë¸ìˆ˜' if lang == 'ko' else 'Total Models'): most_wrong['total_count'].astype(int),
                ('ì˜¤ë‹µë¥ ' if lang == 'ko' else 'Wrong Rate'): (most_wrong['incorrect_rate'] * 100).round(2),
                'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': most_wrong['correct_models'],
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': most_wrong['incorrect_models']
            })
            
            st.dataframe(
                display_most_wrong.style.background_gradient(
                    subset=['ì˜¤ë‹µë¥ ' if lang == 'ko' else 'Wrong Rate'],
                    cmap='Reds',
                    vmin=0,
                    vmax=100
                ),
                width='stretch',
                height=400
            )
            
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸° (ëŒ€ë¶€ë¶„ í‹€ë¦° ë¬¸ì œ)' if lang == 'ko' else 'Show Details (Most Incorrect)', key='most_wrong_details'):
                st.info(f"ì´ {len(most_wrong)}ê°œ ë¬¸ì œ ì¤‘ ìƒìœ„ 20ê°œ")
                for idx, row in most_wrong.head(20).iterrows():
                    with st.expander(f"âš ï¸ {row['problem_id']} - ì˜¤ë‹µë¥  {row['incorrect_rate']*100:.1f}%"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**ë¬¸ì œ:** {q_detail['Question']}")
                        
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©:** {q_detail['Subject']}")
                        
                        if all([f'Option {i}' in q_detail for i in range(1, 5)]):
                            st.write("**ì„ íƒì§€:**")
                            for i in range(1, 5):
                                option = q_detail[f'Option {i}']
                                if pd.notna(option):
                                    if str(i) == str(row['CorrectAnswer']):
                                        st.write(f"  âœ… {i}. {option} **(ì •ë‹µ)**")
                                    else:
                                        st.write(f"  {i}. {option}")
                        
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ:** {q_detail['Answer']}")
                        
                        st.markdown("---")
                        
                        # ì •ë‹µ ëª¨ë¸ê³¼ ì˜¤ë‹µ ëª¨ë¸ ë¶„ë¦¬ í‘œì‹œ
                        if row['correct_count'] > 0:
                            st.write("**âœ“ ì •ë‹µ ëª¨ë¸:**")
                            for model, answer in row['selected_answers'].items():
                                if str(answer) == str(row['CorrectAnswer']):
                                    st.write(f"  â€¢ **{model}**: ì„ íƒ {answer} âœ…")
                        
                        st.write("**âœ— ì˜¤ë‹µ ëª¨ë¸:**")
                        for model, answer in row['selected_answers'].items():
                            if str(answer) != str(row['CorrectAnswer']):
                                st.write(f"  â€¢ **{model}**: ì„ íƒ {answer} (ì •ë‹µ: {row['CorrectAnswer']})")
        else:
            st.info("50% ì´ìƒì˜ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ========================================
        # ì„¹ì…˜ 9: ë²•ë ¹/ë¹„ë²•ë ¹ ì˜¤ë‹µ ë¶„ì„ â­ ì‹ ê·œ ê¸°ëŠ¥
        # ========================================
        if 'law' in filtered_df.columns:
            st.markdown("---")
            st.subheader("âš–ï¸ " + ("ë²•ë ¹/ë¹„ë²•ë ¹ ì˜¤ë‹µ ë¶„ì„" if lang == 'ko' else "Law/Non-Law Incorrect Analysis"))
            
            st.info("""
            ğŸ’¡ **ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ ë¶„ì„**: ë²•ë ¹ ë¬¸ì œì™€ ë¹„ë²•ë ¹ ë¬¸ì œì—ì„œ ëª¨ë¸ë“¤ì˜ ì˜¤ë‹µ íŒ¨í„´ì´ 
            ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¶„ì„í•˜ì—¬ ë²•ë¥  ì§€ì‹ì˜ ê²©ì°¨ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
            """)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ë²•ë ¹/ë¹„ë²•ë ¹ë³„ ì™„ì „ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨
                law_all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
                law_gap_by_type = law_all_wrong['law_status'].value_counts()
                
                law_gap_data = pd.DataFrame({
                    'êµ¬ë¶„': ['ë²•ë ¹' if x == 'O' else 'ë¹„ë²•ë ¹' for x in law_gap_by_type.index],
                    'ì™„ì „_ì§€ì‹ê²©ì°¨': law_gap_by_type.values
                })
                
                total_law = len(problem_analysis[problem_analysis['law_status'] == 'O'])
                total_non_law = len(problem_analysis[problem_analysis['law_status'] != 'O'])
                
                law_gap_data['ì „ì²´ë¬¸ì œ'] = law_gap_data['êµ¬ë¶„'].apply(
                    lambda x: total_law if x == 'ë²•ë ¹' else total_non_law
                )
                law_gap_data['ë¹„ìœ¨'] = (law_gap_data['ì™„ì „_ì§€ì‹ê²©ì°¨'] / law_gap_data['ì „ì²´ë¬¸ì œ'] * 100).round(1)
                
                fig = px.bar(
                    law_gap_data,
                    x='êµ¬ë¶„',
                    y='ë¹„ìœ¨',
                    title='ë²•ë ¹/ë¹„ë²•ë ¹ ì™„ì „ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨',
                    text='ë¹„ìœ¨',
                    color='ë¹„ìœ¨',
                    color_continuous_scale='Reds',
                    hover_data=['ì™„ì „_ì§€ì‹ê²©ì°¨', 'ì „ì²´ë¬¸ì œ']
                )
                fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside', marker_line_color='black', marker_line_width=1.5)
                fig.update_layout(height=400, yaxis_title='ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨ (%)')
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                # ë²•ë ¹/ë¹„ë²•ë ¹ë³„ í‰ê·  ì˜¤ë‹µë¥ 
                law_incorrect_rate = problem_analysis.groupby('law_status')['incorrect_rate'].mean().reset_index()
                law_incorrect_rate['êµ¬ë¶„'] = law_incorrect_rate['law_status'].apply(lambda x: 'ë²•ë ¹' if x == 'O' else 'ë¹„ë²•ë ¹')
                law_incorrect_rate['í‰ê· _ì˜¤ë‹µë¥ '] = law_incorrect_rate['incorrect_rate'] * 100
                
                fig = px.bar(
                    law_incorrect_rate,
                    x='êµ¬ë¶„',
                    y='í‰ê· _ì˜¤ë‹µë¥ ',
                    title='ë²•ë ¹/ë¹„ë²•ë ¹ í‰ê·  ì˜¤ë‹µë¥ ',
                    text='í‰ê· _ì˜¤ë‹µë¥ ',
                    color='í‰ê· _ì˜¤ë‹µë¥ ',
                    color_continuous_scale='Reds'
                )
                fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside', marker_line_color='black', marker_line_width=1.5)
                fig.update_layout(height=400, yaxis_title='í‰ê·  ì˜¤ë‹µë¥  (%)', yaxis=dict(range=[0, 100]))
                st.plotly_chart(fig, width='stretch')
            
            # ë²•ë ¹ ë¬¸ì œ ì¤‘ ì™„ì „ ê³µí†µ ì˜¤ë‹µ
            st.markdown("#### ğŸ“œ " + ("ë²•ë ¹ ë¬¸ì œ ì¤‘ ì™„ì „ ê³µí†µ ì˜¤ë‹µ" if lang == 'ko' else "Law Problems - Complete Gap"))
            
            law_complete_gap = law_all_wrong[law_all_wrong['law_status'] == 'O']
            
            if len(law_complete_gap) > 0:
                st.error(f"""
                âš ï¸ **ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ: {len(law_complete_gap)}ê°œ ë¬¸ì œ**
                
                ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë²•ë ¹ ë¬¸ì œì…ë‹ˆë‹¤. ë²•ë¥  ìš©ì–´, ê·œì • í•´ì„, ë²•ì  íŒë‹¨ì— ëŒ€í•œ 
                ê·¼ë³¸ì ì¸ ì§€ì‹ ë¶€ì¡±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
                """)
                
                display_law_gap = pd.DataFrame({
                    'ë¬¸ì œ ë²ˆí˜¸': law_complete_gap['problem_id'],
                    'ê³¼ëª©': law_complete_gap['Subject'],
                    'ì—°ë„': law_complete_gap['Year'],
                    'ì˜¤ë‹µ ëª¨ë¸ìˆ˜': law_complete_gap['incorrect_count'].astype(int)
                })
                
                st.dataframe(display_law_gap, width='stretch')
                
                if st.checkbox('ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ ë¬¸ì œ ìƒì„¸ ë³´ê¸°', key='law_gap_details'):
                    for idx, row in law_complete_gap.head(10).iterrows():
                        with st.expander(f"ğŸ“œ {row['problem_id']}"):
                            q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                            st.write(f"**ë¬¸ì œ:** {q_detail['Question']}")
                            
                            if 'ë²•ë ¹ ì´ë¦„' in q_detail and pd.notna(q_detail['ë²•ë ¹ ì´ë¦„']):
                                st.write(f"**ğŸ“š ë²•ë ¹:** {q_detail['ë²•ë ¹ ì´ë¦„']}")
                            
                            if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                                st.write(f"**ê³¼ëª©:** {q_detail['Subject']}")
                            
                            if all([f'Option {i}' in q_detail for i in range(1, 5)]):
                                st.write("**ì„ íƒì§€:**")
                                for i in range(1, 5):
                                    option = q_detail[f'Option {i}']
                                    if pd.notna(option):
                                        if str(i) == str(row['CorrectAnswer']):
                                            st.write(f"  âœ… {i}. {option} **(ì •ë‹µ)**")
                                        else:
                                            st.write(f"  {i}. {option}")
                            
                            st.write("**ê° ëª¨ë¸ì´ ì„ íƒí•œ ë‹µ:**")
                            for model, answer in row['selected_answers'].items():
                                st.write(f"  â€¢ {model}: {answer}")
            else:
                st.success("âœ… ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë²•ë ¹ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤!")
            
            # ë¹„ë²•ë ¹ ë¬¸ì œ ì¤‘ ì™„ì „ ê³µí†µ ì˜¤ë‹µ
            st.markdown("#### ğŸ“˜ " + ("ë¹„ë²•ë ¹ ë¬¸ì œ ì¤‘ ì™„ì „ ê³µí†µ ì˜¤ë‹µ" if lang == 'ko' else "Non-Law Problems - Complete Gap"))
            
            non_law_complete_gap = law_all_wrong[law_all_wrong['law_status'] != 'O']
            
            if len(non_law_complete_gap) > 0:
                st.warning(f"""
                â„¹ï¸ **ë¹„ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ: {len(non_law_complete_gap)}ê°œ ë¬¸ì œ**
                
                ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¹„ë²•ë ¹ ë¬¸ì œì…ë‹ˆë‹¤. ê¸°ìˆ ì  ì§€ì‹, ì‹¤ë¬´ ê²½í—˜, 
                ì „ë¬¸ ìš©ì–´ ì´í•´ ë“±ì— ëŒ€í•œ ê²©ì°¨ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
                """)
                
                display_non_law_gap = pd.DataFrame({
                    'ë¬¸ì œ ë²ˆí˜¸': non_law_complete_gap['problem_id'],
                    'ê³¼ëª©': non_law_complete_gap['Subject'],
                    'ì—°ë„': non_law_complete_gap['Year'],
                    'ì˜¤ë‹µ ëª¨ë¸ìˆ˜': non_law_complete_gap['incorrect_count'].astype(int)
                })
                
                st.dataframe(display_non_law_gap, width='stretch')
            else:
                st.success("âœ… ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¹„ë²•ë ¹ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤!")
            
            # ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ ì¡°ì¹˜
            st.markdown("---")
            st.markdown("#### ğŸ’¡ " + ("ë²•ë ¹/ë¹„ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ ì¸ì‚¬ì´íŠ¸" if lang == 'ko' else "Law/Non-Law Gap Insights"))
            
            law_gap_count = len(law_complete_gap)
            non_law_gap_count = len(non_law_complete_gap)
            law_gap_ratio = (law_gap_count / total_law * 100) if total_law > 0 else 0
            non_law_gap_ratio = (non_law_gap_count / total_non_law * 100) if total_non_law > 0 else 0
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨", f"{law_gap_ratio:.1f}%", f"{law_gap_count}/{total_law}")
            
            with col2:
                st.metric("ë¹„ë²•ë ¹ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨", f"{non_law_gap_ratio:.1f}%", f"{non_law_gap_count}/{total_non_law}")
            
            with col3:
                if law_gap_ratio > non_law_gap_ratio:
                    st.metric("ë” ì·¨ì•½í•œ ì˜ì—­", "ë²•ë ¹", f"+{law_gap_ratio - non_law_gap_ratio:.1f}%p")
                else:
                    st.metric("ë” ì·¨ì•½í•œ ì˜ì—­", "ë¹„ë²•ë ¹", f"+{non_law_gap_ratio - law_gap_ratio:.1f}%p")
            
            if law_gap_ratio > non_law_gap_ratio * 1.5:
                st.error(f"""
                ğŸš¨ **ë²•ë ¹ ì§€ì‹ì´ íŠ¹íˆ ì·¨ì•½í•©ë‹ˆë‹¤!**
                
                ë²•ë ¹ ë¬¸ì œì˜ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨({law_gap_ratio:.1f}%)ì´ ë¹„ë²•ë ¹({non_law_gap_ratio:.1f}%)ë³´ë‹¤ 
                {law_gap_ratio / non_law_gap_ratio:.1f}ë°° ë†’ìŠµë‹ˆë‹¤.
                
                **ê¶Œì¥ ì¡°ì¹˜**:
                - ë²•ë¥  ìš©ì–´ ë° ê·œì •ì— ëŒ€í•œ í•™ìŠµ ë°ì´í„° ë³´ê°•
                - ë²•ë ¹ í•´ì„ ì˜ˆì‹œ ì¶”ê°€
                - ë²•ë¥  ì „ë¬¸ê°€ ê²€í†  ë° í”¼ë“œë°± ë°˜ì˜
                """)
            elif non_law_gap_ratio > law_gap_ratio * 1.5:
                st.warning(f"""
                âš ï¸ **ê¸°ìˆ /ì‹¤ë¬´ ì§€ì‹ì´ ìƒëŒ€ì ìœ¼ë¡œ ì·¨ì•½í•©ë‹ˆë‹¤!**
                
                ë¹„ë²•ë ¹ ë¬¸ì œì˜ ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨({non_law_gap_ratio:.1f}%)ì´ ë²•ë ¹({law_gap_ratio:.1f}%)ë³´ë‹¤ 
                {non_law_gap_ratio / law_gap_ratio:.1f}ë°° ë†’ìŠµë‹ˆë‹¤.
                
                **ê¶Œì¥ ì¡°ì¹˜**:
                - ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ í•™ìŠµ ë°ì´í„° ë³´ê°•
                - ì‹¤ë¬´ ì‚¬ë¡€ ë° ì ìš© ì˜ˆì‹œ ì¶”ê°€
                - ì „ë¬¸ ìš©ì–´ ì •ì˜ ëª…í™•í™”
                """)
            else:
                st.success(f"""
                âœ… **ë²•ë ¹ê³¼ ë¹„ë²•ë ¹ ê³µí†µ ì˜¤ë‹µê°€ ê· í˜•ì ì…ë‹ˆë‹¤.**
                
                ë²•ë ¹({law_gap_ratio:.1f}%)ê³¼ ë¹„ë²•ë ¹({non_law_gap_ratio:.1f}%) ë¬¸ì œì˜ 
                ê³µí†µ ì˜¤ë‹µ ë¹„ìœ¨ì´ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.
                """)
        
        # ========================================
        # ì„¹ì…˜ 10: ì˜¤ë‹µë¥  Top 10 ì°¨íŠ¸
        # ========================================
        st.markdown("---")
        top_10_chart = top_20.head(10)
        
        fig = px.bar(
            top_10_chart,
            x='problem_id',
            y='incorrect_rate',
            title='ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 10' if lang == 'ko' else 'Top 10 Problems by Incorrect Rate',
            text=[f"{x:.0%}" for x in top_10_chart['incorrect_rate']],
            color='incorrect_rate',
            color_continuous_scale='Reds',
            range_color=[0, 1]
        )
        fig.update_traces(textposition='outside', marker_line_color='black', marker_line_width=1.5)
        fig.update_layout(
            height=500,
            showlegend=False,
            yaxis_title='ì˜¤ë‹µë¥ ',
            xaxis_title='ë¬¸ì œ ë²ˆí˜¸',
            yaxis=dict(range=[0, 1])
        )
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, width='stretch')
        
        # ========================================
        # ì„¹ì…˜ 11: ê³ ì˜¤ë‹µë¥  & ê³ ì¼ê´€ì„± ë¬¸ì œ ë¶„ì„ (NEW!)
        # ========================================
        st.markdown("---")
        st.subheader("ğŸ¯ " + ("ê³ ì˜¤ë‹µë¥  & ê³ ì¼ê´€ì„± ë¬¸ì œ ë¶„ì„" if lang == 'ko' else "High Incorrect Rate & High Consistency Analysis"))
        
        st.markdown("""
        > ğŸ’¡ **ë¶„ì„ ëª©ì **: ì˜¤ë‹µë¥  50% ì´ìƒì´ë©´ì„œ ì¼ê´€ì„±(ê°™ì€ ì˜¤ë‹µ ì„ íƒë¥ ) 50% ì´ìƒì¸ ë¬¸ì œëŠ” 
        > ëª¨ë¸ë“¤ì´ **ì²´ê³„ì ìœ¼ë¡œ í‹€ë¦¬ëŠ”** ë¬¸ì œì…ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ì´ë‚˜ ì§€ì‹ ê²©ì°¨ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """ if lang == 'ko' else """
        > ğŸ’¡ **Purpose**: Problems with incorrect rate â‰¥50% AND consistency â‰¥50% indicate 
        > **systematic errors** where models consistently choose the same wrong answer.
        """)
        
        # ë¬¸ì œë³„ ì˜¤ë‹µë¥ ê³¼ ì¼ê´€ì„± ê³„ì‚°
        problem_stats = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean'],
            'ì˜ˆì¸¡ë‹µ': lambda x: x.value_counts().iloc[0] / len(x) if len(x) > 0 else 0  # ê°€ì¥ ë§ì´ ì„ íƒëœ ë‹µì˜ ë¹„ìœ¨
        }).reset_index()
        problem_stats.columns = ['Question', 'correct_count', 'total_count', 'accuracy', 'top_answer_ratio']
        problem_stats['incorrect_rate'] = 1 - problem_stats['accuracy']
        
        # ì˜¤ë‹µì¸ ê²½ìš°ì˜ ì¼ê´€ì„± (ê°™ì€ ì˜¤ë‹µì„ ì„ íƒí•œ ë¹„ìœ¨)
        consistency_list = []
        for q in problem_stats['Question']:
            q_df = filtered_df[filtered_df['Question'] == q]
            wrong_df = q_df[q_df['ì •ë‹µì—¬ë¶€'] == False]
            if len(wrong_df) > 0:
                # ê°€ì¥ ë§ì´ ì„ íƒëœ ì˜¤ë‹µì˜ ë¹„ìœ¨
                wrong_answer_counts = wrong_df['ì˜ˆì¸¡ë‹µ'].value_counts()
                if len(wrong_answer_counts) > 0:
                    consistency = wrong_answer_counts.iloc[0] / len(wrong_df)
                else:
                    consistency = 0
            else:
                consistency = 0
            consistency_list.append(consistency)
        
        problem_stats['wrong_consistency'] = consistency_list
        
        # í…ŒìŠ¤íŠ¸ëª… ë§¤í•‘
        test_mapping = filtered_df.groupby('Question')['í…ŒìŠ¤íŠ¸ëª…'].first().to_dict()
        problem_stats['í…ŒìŠ¤íŠ¸ëª…'] = problem_stats['Question'].map(test_mapping)
        
        # ê³ ì˜¤ë‹µë¥  & ê³ ì¼ê´€ì„± ë¬¸ì œ í•„í„°ë§ (ì˜¤ë‹µë¥  50%+, ì¼ê´€ì„± 50%+)
        high_risk = problem_stats[
            (problem_stats['incorrect_rate'] >= 0.5) & 
            (problem_stats['wrong_consistency'] >= 0.5)
        ].copy()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "ì „ì²´ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Total Problems",
                f"{len(problem_stats):,}"
            )
        with col2:
            st.metric(
                "ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "High-Risk Problems",
                f"{len(high_risk):,}",
                f"{len(high_risk)/len(problem_stats)*100:.1f}%" if len(problem_stats) > 0 else "0%"
            )
        with col3:
            avg_consistency = high_risk['wrong_consistency'].mean() * 100 if len(high_risk) > 0 else 0
            st.metric(
                "í‰ê·  ì˜¤ë‹µ ì¼ê´€ì„±" if lang == 'ko' else "Avg Wrong Consistency",
                f"{avg_consistency:.1f}%"
            )
        
        # ----- 1. ì˜¤ë‹µë¥ -ì¼ê´€ì„± êµ¬ê°„ë³„ ë¬¸ì œ ê°œìˆ˜ íˆíŠ¸ë§µ -----
        st.markdown("#### " + ("ğŸ“Š ì˜¤ë‹µë¥ -ì¼ê´€ì„± êµ¬ê°„ë³„ ë¬¸ì œ ë¶„í¬ íˆíŠ¸ë§µ" if lang == 'ko' else "ğŸ“Š Problem Distribution Heatmap by Incorrect Rate & Consistency"))
        
        # 5% êµ¬ê°„ìœ¼ë¡œ binning (50~100%)
        bins = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.01]
        bin_labels = ['50-55%', '55-60%', '60-65%', '65-70%', '70-75%', '75-80%', '80-85%', '85-90%', '90-95%', '95-100%']
        
        # ì˜¤ë‹µë¥  50% ì´ìƒ, ì¼ê´€ì„± 50% ì´ìƒë§Œ í•„í„°ë§
        heatmap_data = problem_stats[
            (problem_stats['incorrect_rate'] >= 0.5) & 
            (problem_stats['wrong_consistency'] >= 0.5)
        ].copy()
        
        if len(heatmap_data) > 0:
            heatmap_data['incorrect_bin'] = pd.cut(
                heatmap_data['incorrect_rate'], 
                bins=bins, 
                labels=bin_labels,
                include_lowest=True
            )
            heatmap_data['consistency_bin'] = pd.cut(
                heatmap_data['wrong_consistency'], 
                bins=bins, 
                labels=bin_labels,
                include_lowest=True
            )
            
            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            heatmap_pivot = pd.crosstab(
                heatmap_data['consistency_bin'], 
                heatmap_data['incorrect_bin'],
                dropna=False
            )
            
            # ëª¨ë“  êµ¬ê°„ì´ ìˆë„ë¡ reindex
            heatmap_pivot = heatmap_pivot.reindex(index=bin_labels, columns=bin_labels, fill_value=0)
            
            # íˆíŠ¸ë§µ ìƒì„±
            fig_heatmap = go.Figure(data=go.Heatmap(
                z=heatmap_pivot.values,
                x=heatmap_pivot.columns.tolist(),
                y=heatmap_pivot.index.tolist(),
                colorscale='YlOrRd',
                text=heatmap_pivot.values,
                texttemplate='%{text}',
                textfont={"size": annotation_size},
                colorbar=dict(title="ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Count"),
                hoverongaps=False
            ))
            
            fig_heatmap.update_layout(
                title='ì˜¤ë‹µë¥  vs ì¼ê´€ì„± êµ¬ê°„ë³„ ë¬¸ì œ ë¶„í¬' if lang == 'ko' else 'Problem Distribution: Incorrect Rate vs Consistency',
                xaxis_title='ì˜¤ë‹µë¥  êµ¬ê°„' if lang == 'ko' else 'Incorrect Rate Range',
                yaxis_title='ì˜¤ë‹µ ì¼ê´€ì„± êµ¬ê°„' if lang == 'ko' else 'Wrong Consistency Range',
                height=500,
                xaxis=dict(side='bottom'),
                yaxis=dict(autorange='reversed')  # ìœ„ì—ì„œ ì•„ë˜ë¡œ
            )
            fig_heatmap.update_xaxes(tickfont=dict(size=annotation_size))
            fig_heatmap.update_yaxes(tickfont=dict(size=annotation_size))
            
            st.plotly_chart(fig_heatmap, width='stretch')
            
            # ì¸ì‚¬ì´íŠ¸
            max_cell = heatmap_pivot.max().max()
            max_pos = heatmap_pivot.stack().idxmax()
            if max_cell > 0:
                st.info(f"ğŸ’¡ " + (f"ê°€ì¥ ë§ì€ ë¬¸ì œê°€ ì§‘ì¤‘ëœ êµ¬ê°„: ì˜¤ë‹µë¥  **{max_pos[1]}**, ì¼ê´€ì„± **{max_pos[0]}** ({int(max_cell)}ê°œ)" 
                        if lang == 'ko' else f"Most concentrated: Incorrect Rate **{max_pos[1]}**, Consistency **{max_pos[0]}** ({int(max_cell)} problems)"))
        else:
            st.info("ì˜¤ë‹µë¥  50% ì´ìƒì´ë©´ì„œ ì¼ê´€ì„± 50% ì´ìƒì¸ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No problems with both incorrect rate â‰¥50% and consistency â‰¥50%.")
        
        st.markdown("---")
        
        # ----- 2. í…ŒìŠ¤íŠ¸ì…‹ë³„ ê³ ìœ„í—˜ ë¬¸ì œ ê°œìˆ˜ ì°¨íŠ¸ -----
        st.markdown("#### " + ("ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ë³„ ê³ ìœ„í—˜ ë¬¸ì œ ë¶„í¬" if lang == 'ko' else "ğŸ“Š High-Risk Problems by Test Set"))
        
        if len(high_risk) > 0 and 'í…ŒìŠ¤íŠ¸ëª…' in high_risk.columns:
            # í…ŒìŠ¤íŠ¸ì…‹ë³„ ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜
            testset_risk = high_risk.groupby('í…ŒìŠ¤íŠ¸ëª…').agg({
                'Question': 'count',
                'incorrect_rate': 'mean',
                'wrong_consistency': 'mean'
            }).reset_index()
            testset_risk.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜', 'í‰ê·  ì˜¤ë‹µë¥ ', 'í‰ê·  ì¼ê´€ì„±']
            
            # ì „ì²´ ë¬¸ì œ ìˆ˜ ëŒ€ë¹„ ë¹„ìœ¨ ê³„ì‚°
            total_by_test = problem_stats.groupby('í…ŒìŠ¤íŠ¸ëª…')['Question'].count().to_dict()
            testset_risk['ì „ì²´ ë¬¸ì œ ìˆ˜'] = testset_risk['í…ŒìŠ¤íŠ¸ëª…'].map(total_by_test)
            testset_risk['ê³ ìœ„í—˜ ë¹„ìœ¨'] = testset_risk['ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜'] / testset_risk['ì „ì²´ ë¬¸ì œ ìˆ˜'] * 100
            
            testset_risk = testset_risk.sort_values('ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜', ascending=False)
            
            # ë§‰ëŒ€ ì°¨íŠ¸
            fig_testset = go.Figure()
            
            # ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜ ë§‰ëŒ€
            fig_testset.add_trace(go.Bar(
                name='ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'High-Risk Problems',
                x=testset_risk['í…ŒìŠ¤íŠ¸ëª…'],
                y=testset_risk['ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜'],
                text=testset_risk['ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜'],
                textposition='outside',
                textfont=dict(size=annotation_size),
                marker_color='#e74c3c',
                marker_line_color='black',
                marker_line_width=1.5
            ))
            
            fig_testset.update_layout(
                title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜ (ì˜¤ë‹µë¥ â‰¥50% & ì¼ê´€ì„±â‰¥50%)' if lang == 'ko' else 'High-Risk Problems by Test Set (Incorrectâ‰¥50% & Consistencyâ‰¥50%)',
                xaxis_title='í…ŒìŠ¤íŠ¸ì…‹' if lang == 'ko' else 'Test Set',
                yaxis_title='ë¬¸ì œ ìˆ˜' if lang == 'ko' else 'Problem Count',
                height=450,
                showlegend=False
            )
            fig_testset.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig_testset.update_yaxes(tickfont=dict(size=annotation_size))
            
            st.plotly_chart(fig_testset, width='stretch')
            
            # ë¹„ìœ¨ ì°¨íŠ¸
            st.markdown("##### " + ("ê³ ìœ„í—˜ ë¬¸ì œ ë¹„ìœ¨ (í…ŒìŠ¤íŠ¸ì…‹ ë‚´)" if lang == 'ko' else "High-Risk Problem Ratio (within Test Set)"))
            
            fig_ratio = px.bar(
                testset_risk.sort_values('ê³ ìœ„í—˜ ë¹„ìœ¨', ascending=False),
                x='í…ŒìŠ¤íŠ¸ëª…',
                y='ê³ ìœ„í—˜ ë¹„ìœ¨',
                text=[f"{x:.1f}%" for x in testset_risk.sort_values('ê³ ìœ„í—˜ ë¹„ìœ¨', ascending=False)['ê³ ìœ„í—˜ ë¹„ìœ¨']],
                color='ê³ ìœ„í—˜ ë¹„ìœ¨',
                color_continuous_scale='Reds',
                title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ê³ ìœ„í—˜ ë¬¸ì œ ë¹„ìœ¨' if lang == 'ko' else 'High-Risk Problem Ratio by Test Set'
            )
            fig_ratio.update_traces(textposition='outside', textfont=dict(size=annotation_size), marker_line_color='black', marker_line_width=1)
            fig_ratio.update_layout(
                height=400,
                showlegend=False,
                yaxis_title='ë¹„ìœ¨ (%)' if lang == 'ko' else 'Ratio (%)',
                xaxis_title='í…ŒìŠ¤íŠ¸ì…‹' if lang == 'ko' else 'Test Set',
                coloraxis_showscale=False
            )
            fig_ratio.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig_ratio.update_yaxes(tickfont=dict(size=annotation_size))
            
            st.plotly_chart(fig_ratio, width='stretch')
            
            # ìƒì„¸ í…Œì´ë¸”
            with st.expander("ğŸ“‹ " + ("ìƒì„¸ ë°ì´í„° ë³´ê¸°" if lang == 'ko' else "View Detailed Data")):
                display_testset = testset_risk.copy()
                display_testset['í‰ê·  ì˜¤ë‹µë¥ '] = display_testset['í‰ê·  ì˜¤ë‹µë¥ '].apply(lambda x: f"{x*100:.1f}%")
                display_testset['í‰ê·  ì¼ê´€ì„±'] = display_testset['í‰ê·  ì¼ê´€ì„±'].apply(lambda x: f"{x*100:.1f}%")
                display_testset['ê³ ìœ„í—˜ ë¹„ìœ¨'] = display_testset['ê³ ìœ„í—˜ ë¹„ìœ¨'].apply(lambda x: f"{x:.1f}%")
                
                st.dataframe(
                    display_testset,
                    width='stretch',
                    hide_index=True
                )
            
            # ê°€ì¥ ìœ„í—˜í•œ í…ŒìŠ¤íŠ¸ì…‹ ê°•ì¡°
            most_risky = testset_risk.iloc[0]
            st.warning(f"""
            âš ï¸ **ê°€ì¥ ì£¼ì˜ê°€ í•„ìš”í•œ í…ŒìŠ¤íŠ¸ì…‹: {most_risky['í…ŒìŠ¤íŠ¸ëª…']}**
            - ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜: {int(most_risky['ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜'])}ê°œ
            - í‰ê·  ì˜¤ë‹µë¥ : {most_risky['í‰ê·  ì˜¤ë‹µë¥ ']*100:.1f}%
            - í‰ê·  ì˜¤ë‹µ ì¼ê´€ì„±: {most_risky['í‰ê·  ì¼ê´€ì„±']*100:.1f}%
            """ if lang == 'ko' else f"""
            âš ï¸ **Test set requiring most attention: {most_risky['í…ŒìŠ¤íŠ¸ëª…']}**
            - High-risk problems: {int(most_risky['ê³ ìœ„í—˜ ë¬¸ì œ ìˆ˜'])}
            - Avg incorrect rate: {most_risky['í‰ê·  ì˜¤ë‹µë¥ ']*100:.1f}%
            - Avg wrong consistency: {most_risky['í‰ê·  ì¼ê´€ì„±']*100:.1f}%
            """)
        else:
            st.info("ê³ ìœ„í—˜ ë¬¸ì œê°€ ì—†ê±°ë‚˜ í…ŒìŠ¤íŠ¸ì…‹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No high-risk problems or test set info unavailable.")
    
    # íƒ­ 8: ë‚œì´ë„ ë¶„ì„
    with tabs[7]:
        st.header(f"ğŸ“ˆ {t['difficulty_analysis']}")
        
        # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚° (ì •ë‹µë¥  ê¸°ë°˜)
        difficulty = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        difficulty.columns = ['Question', 'correct_count', 'total_count', 'difficulty_score']
        difficulty['difficulty_score'] = difficulty['difficulty_score'] * 100
        
        # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
        def classify_difficulty(score, lang='ko'):
            if lang == 'ko':
                if score < 20:
                    return 'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)'
                elif score < 40:
                    return 'ì–´ë ¤ì›€ (20-40%)'
                elif score < 60:
                    return 'ë³´í†µ (40-60%)'
                elif score < 80:
                    return 'ì‰¬ì›€ (60-80%)'
                else:
                    return 'ë§¤ìš° ì‰¬ì›€ (80-100%)'
            else:  # English
                if score < 20:
                    return 'Very Hard (0-20%)'
                elif score < 40:
                    return 'Hard (20-40%)'
                elif score < 60:
                    return 'Medium (40-60%)'
                elif score < 80:
                    return 'Easy (60-80%)'
                else:
                    return 'Very Easy (80-100%)'
        
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = difficulty['difficulty_score'].apply(lambda x: classify_difficulty(x, lang))
        
        # ë‚œì´ë„ êµ¬ê°„ ìˆœì„œ ì •ì˜ (ì–´ë ¤ìš´ ê²ƒë¶€í„° ì‰¬ìš´ ê²ƒ ìˆœ)
        if lang == 'ko':
            difficulty_order = [
                'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)',
                'ì–´ë ¤ì›€ (20-40%)',
                'ë³´í†µ (40-60%)',
                'ì‰¬ì›€ (60-80%)',
                'ë§¤ìš° ì‰¬ì›€ (80-100%)'
            ]
        else:
            difficulty_order = [
                'Very Hard (0-20%)',
                'Hard (20-40%)',
                'Medium (40-60%)',
                'Easy (60-80%)',
                'Very Easy (80-100%)'
            ]
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(difficulty['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # ì›ë³¸ ë°ì´í„°ì— ë‚œì´ë„ ì •ë³´ ë³‘í•©
        analysis_df = filtered_df.merge(difficulty[['Question', 'difficulty_score', 'ë‚œì´ë„_êµ¬ê°„']], on='Question')
        
        # analysis_dfì—ë„ ë™ì¼í•œ ìˆœì„œ ì ìš©
        analysis_df['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(analysis_df['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # 1. ë‚œì´ë„ ë¶„í¬
        st.subheader("ğŸ“ˆ " + (t['problem_distribution'] if 'problem_distribution' in t else ('ë¬¸ì œ ë‚œì´ë„ ë¶„í¬' if lang == 'ko' else 'Problem Difficulty Distribution')))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ë‚œì´ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            fig = px.histogram(
                difficulty,
                x='difficulty_score',
                nbins=20,
                title=t['difficulty_score'] + ' Distribution',
                labels={'difficulty_score': t['difficulty_score'], 'count': t['problem_count']}
            )
            fig.update_traces(
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, width='stretch')
        
        with col2:
            # ë‚œì´ë„ êµ¬ê°„ë³„ ë¬¸ì œ ìˆ˜
            difficulty_dist = difficulty['ë‚œì´ë„_êµ¬ê°„'].value_counts()
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
            difficulty_dist = difficulty_dist.reindex(difficulty_order, fill_value=0)
            
            fig = px.bar(
                x=difficulty_dist.index,
                y=difficulty_dist.values,
                title=t['problem_count'] + (' by ' + t['difficulty_range'] if lang == 'en' else ' (' + t['difficulty_range'] + 'ë³„)'),
                labels={'x': t['difficulty_range'], 'y': t['problem_count']},
                text=difficulty_dist.values,
                color=difficulty_dist.values,
                color_continuous_scale='RdYlGn_r'
            )
            fig.update_traces(
                texttemplate='%{text}',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
        
        # í†µê³„ ìš”ì•½
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric(
                t['correct_rate'] if lang == 'ko' else 'Average Correct Rate',
                f"{difficulty['difficulty_score'].mean():.1f}%"
            )
        with col2:
            st.metric(
                'ì¤‘ì•™ê°’' if lang == 'ko' else 'Median',
                f"{difficulty['difficulty_score'].median():.1f}%"
            )
        with col3:
            very_hard_label = difficulty_order[0]
            very_hard = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == very_hard_label])
            st.metric(
                t['very_hard'] + (' ë¬¸ì œ' if lang == 'ko' else ' Problems'),
                f"{very_hard}" + (t['problems'] if lang == 'ko' else '')
            )
        with col4:
            very_easy_label = difficulty_order[-1]
            very_easy = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == very_easy_label])
            st.metric(
                t['very_easy'] + (' ë¬¸ì œ' if lang == 'ko' else ' Problems'),
                f"{very_easy}" + (t['problems'] if lang == 'ko' else '')
            )
        
        st.markdown("---")
        
        # 2. ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥
        st.subheader("ğŸ¯ " + ('ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥' if lang == 'ko' else 'Model Performance by Difficulty Level'))
        
        # ëª¨ë¸ë³„ ë‚œì´ë„ êµ¬ê°„ë³„ ì •ë‹µë¥ 
        model_difficulty = analysis_df.groupby(['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„']).agg({
            'ì •ë‹µì—¬ë¶€': ['mean', 'count']
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
        if lang == 'ko':
            model_difficulty.columns = ['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„', 'ì •ë‹µë¥ ', 'ë¬¸ì œìˆ˜']
        else:
            model_difficulty.columns = ['Model', 'Difficulty', 'Correct Rate', 'Problem Count']
        
        # ì •ë‹µë¥  ì»¬ëŸ¼ëª… (ì–¸ì–´ë³„)
        acc_col = 'ì •ë‹µë¥ ' if lang == 'ko' else 'Correct Rate'
        model_col = 'ëª¨ë¸' if lang == 'ko' else 'Model'
        diff_col = 'ë‚œì´ë„_êµ¬ê°„' if lang == 'ko' else 'Difficulty'
        
        model_difficulty[acc_col] = model_difficulty[acc_col] * 100
        
        # ë¼ì¸ ì°¨íŠ¸
        fig = px.line(
            model_difficulty,
            x=diff_col,
            y=acc_col,
            color=model_col,
            markers=True,
            title='ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ' if lang == 'ko' else 'Model Performance by Difficulty Level',
            labels={
                acc_col: t['accuracy'] + ' (%)',
                diff_col: t['difficulty_range'],
                model_col: t['model']
            },
            category_orders={diff_col: difficulty_order}
        )
        fig.update_traces(
            marker_size=10,
            marker_line_color='black',
            marker_line_width=2,
            line_width=3
        )
        fig.update_layout(height=500)
        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
        st.plotly_chart(fig, width='stretch')
        
        # íˆíŠ¸ë§µ
        pivot_difficulty = model_difficulty.pivot(
            index=model_col,
            columns=diff_col,
            values=acc_col
        )
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        pivot_difficulty = pivot_difficulty.reindex(columns=difficulty_order)
        
        fig = go.Figure(data=go.Heatmap(
            z=pivot_difficulty.values,
            x=pivot_difficulty.columns,
            y=pivot_difficulty.index,
            colorscale='RdYlGn',
            text=np.round(pivot_difficulty.values, 1),
            texttemplate='%{text:.1f}',
            textfont={"size": annotation_size},
            colorbar=dict(title=t['accuracy'] + " (%)"),
            xgap=2,  # ì…€ ê²½ê³„ì„ 
            ygap=2
        ))
        fig.update_layout(
            height=400,
            title='ëª¨ë¸ Ã— ë‚œì´ë„ íˆíŠ¸ë§µ' if lang == 'ko' else 'Model Ã— Difficulty Heatmap',
            xaxis_title=t['difficulty_range'],
            yaxis_title=t['model']
        )
        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
        fig.update_yaxes(tickfont=dict(size=annotation_size))
        st.plotly_chart(fig, width='stretch')
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        # ëª¨ë¸ë³„ ë‚œì´ë„ ì ì‘ë ¥ ë¶„ì„
        difficulty_adaptability = {}
        for model in pivot_difficulty.index:
            # ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ ì •í™•ë„
            very_hard_acc = pivot_difficulty.loc[model, difficulty_order[0]] if difficulty_order[0] in pivot_difficulty.columns else 0
            # ë§¤ìš° ì‰¬ìš´ ë¬¸ì œ ì •í™•ë„
            very_easy_acc = pivot_difficulty.loc[model, difficulty_order[-1]] if difficulty_order[-1] in pivot_difficulty.columns else 0
            # ê²©ì°¨ (ì‘ì„ìˆ˜ë¡ ì¼ê´€ì )
            gap = very_easy_acc - very_hard_acc
            difficulty_adaptability[model] = {'hard': very_hard_acc, 'easy': very_easy_acc, 'gap': gap}
        
        best_hard_model = max(difficulty_adaptability.items(), key=lambda x: x[1]['hard'])[0]
        most_consistent_model = min(difficulty_adaptability.items(), key=lambda x: x[1]['gap'])[0]
        
        st.success(f"""
        ğŸ’¡ **{"ë‚œì´ë„ë³„ ëª¨ë¸ ì ì‘ë ¥ ë¶„ì„" if lang == 'ko' else "Model Adaptability by Difficulty"}**:
        
        ğŸ† **{"ì–´ë ¤ìš´ ë¬¸ì œ ëŒ€ì‘ë ¥" if lang == 'ko' else "Hard Problem Performance"}**:
        - **{"ìµœê³ " if lang == 'ko' else "Best"}**: {best_hard_model} ({difficulty_adaptability[best_hard_model]['hard']:.1f}% {"ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œì—ì„œ" if lang == 'ko' else "on very hard"})
        - **{"íŠ¹ì§•" if lang == 'ko' else "Note"}**: {"ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš° í™œìš©" if lang == 'ko' else "Use for complex reasoning tasks"}
        
        âš–ï¸ **{"ì¼ê´€ì„±" if lang == 'ko' else "Consistency"}**:
        - **{"ê°€ì¥ ì¼ê´€ì " if lang == 'ko' else "Most Consistent"}**: {most_consistent_model} ({"ë‚œì´ë„ ê²©ì°¨" if lang == 'ko' else "difficulty gap"}: {difficulty_adaptability[most_consistent_model]['gap']:.1f}%p)
        - **{"ì˜ë¯¸" if lang == 'ko' else "Meaning"}**: {"ëª¨ë“  ë‚œì´ë„ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥" if lang == 'ko' else "Stable across all difficulties"}
        
        ğŸ“Š **{"í™œìš© ì „ëµ" if lang == 'ko' else "Usage Strategy"}**:
        - **{"ê³ ë‚œë„ ì‹œí—˜" if lang == 'ko' else "High-difficulty exams"}**: {best_hard_model} {"ê¶Œì¥" if lang == 'ko' else "recommended"}
        - **{"ë²”ìš© í•™ìŠµ" if lang == 'ko' else "General learning"}**: {most_consistent_model} {"ê¶Œì¥" if lang == 'ko' else "recommended"}
        - **{"ë¼ì¸ ì°¨íŠ¸" if lang == 'ko' else "Line chart"}**: {"ë‚œì´ë„ê°€ ì˜¬ë¼ê°ˆìˆ˜ë¡ ì„±ëŠ¥ í•˜ë½í­ í™•ì¸" if lang == 'ko' else "Check performance drop as difficulty increases"}
        """)
        
        st.markdown("---")
        
        # 3. ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„
        if 'Subject' in analysis_df.columns:
            st.subheader("ğŸ“š " + ('ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„' if lang == 'ko' else 'Difficulty Analysis by Subject'))
            
            subject_difficulty = analysis_df.groupby('Subject').agg({
                'difficulty_score': 'mean',
                'Question': 'count'
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
            if lang == 'ko':
                subject_difficulty.columns = ['ê³¼ëª©', 'í‰ê· _ë‚œì´ë„', 'ë¬¸ì œìˆ˜']
                subj_col = 'ê³¼ëª©'
                avg_diff_col = 'í‰ê· _ë‚œì´ë„'
            else:
                subject_difficulty.columns = ['Subject', 'Avg Difficulty', 'Problem Count']
                subj_col = 'Subject'
                avg_diff_col = 'Avg Difficulty'
            
            subject_difficulty = subject_difficulty.sort_values(avg_diff_col)
            
            fig = px.bar(
                subject_difficulty,
                x=subj_col,
                y=avg_diff_col,
                title='ê³¼ëª©ë³„ í‰ê·  ë‚œì´ë„ (ì •ë‹µë¥ )' if lang == 'ko' else 'Average Difficulty by Subject (Correct Rate)',
                text=avg_diff_col,
                color=avg_diff_col,
                color_continuous_scale='RdYlGn',
                labels={subj_col: t['by_subject'].replace('ë³„', ''), avg_diff_col: t['avg_difficulty']}
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_layout(
                height=500,
                showlegend=False
            )
            st.plotly_chart(fig, width='stretch')
            
            # ê³¼ëª© Ã— ë‚œì´ë„ êµ¬ê°„ íˆíŠ¸ë§µ
            subject_diff_dist = analysis_df.groupby(['Subject', 'ë‚œì´ë„_êµ¬ê°„']).size().reset_index(name='ë¬¸ì œìˆ˜')
            pivot_subject_diff = subject_diff_dist.pivot(
                index='Subject',
                columns='ë‚œì´ë„_êµ¬ê°„',
                values='ë¬¸ì œìˆ˜'
            ).fillna(0)
            
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
            pivot_subject_diff = pivot_subject_diff.reindex(columns=difficulty_order, fill_value=0)
            
            fig = go.Figure(data=go.Heatmap(
                z=pivot_subject_diff.values,
                x=pivot_subject_diff.columns,
                y=pivot_subject_diff.index,
                colorscale='Blues',
                text=pivot_subject_diff.values.astype(int),
                texttemplate='%{text}',
                textfont={"size": annotation_size},
                colorbar=dict(title=t['problem_count']),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            fig.update_layout(
                height=500,
                title='ê³¼ëª© Ã— ë‚œì´ë„ ë¶„í¬' if lang == 'ko' else 'Subject Ã— Difficulty Distribution',
                xaxis_title=t['difficulty_range'],
                yaxis_title=t['by_subject'].replace('ë³„', '')  # 'ê³¼ëª©' or 'Subject'
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
        
        st.markdown("---")
        
        # 4. ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ìƒì„¸ ë¶„ì„
        st.subheader("ğŸ” " + (
            "ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ë¹„êµ" if lang == 'ko' else "Hard vs Easy Problems Comparison"
        ))
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### " + (
                "ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ (ì •ë‹µë¥  < 20%)" if lang == 'ko' else "Very Hard Problems (Correct Rate < 20%)"
            ))
            very_hard_problems = difficulty[difficulty['difficulty_score'] < 20].sort_values('difficulty_score')
            
            if len(very_hard_problems) > 0:
                st.metric(
                    t['problem_count'],
                    f"{len(very_hard_problems)}" + (t['problems'] if lang == 'ko' else '')
                )
                st.metric(
                    'í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Average Correct Rate',
                    f"{very_hard_problems['difficulty_score'].mean():.1f}%"
                )
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_hard_questions = very_hard_problems['Question'].tolist()
                very_hard_model_perf = filtered_df[filtered_df['Question'].isin(very_hard_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**" + (
                    "ëª¨ë¸ë³„ ì„±ëŠ¥" if lang == 'ko' else "Performance by Model"
                ) + "**")
                for model, acc in very_hard_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info(
                    "ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No very hard problems found."
                )
        
        with col2:
            st.markdown("#### " + (
                "ë§¤ìš° ì‰¬ìš´ ë¬¸ì œ (ì •ë‹µë¥  > 80%)" if lang == 'ko' else "Very Easy Problems (Correct Rate > 80%)"
            ))
            very_easy_problems = difficulty[difficulty['difficulty_score'] > 80].sort_values('difficulty_score', ascending=False)
            
            if len(very_easy_problems) > 0:
                st.metric(
                    t['problem_count'],
                    f"{len(very_easy_problems)}" + (t['problems'] if lang == 'ko' else '')
                )
                st.metric(
                    'í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Average Correct Rate',
                    f"{very_easy_problems['difficulty_score'].mean():.1f}%"
                )
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_easy_questions = very_easy_problems['Question'].tolist()
                very_easy_model_perf = filtered_df[filtered_df['Question'].isin(very_easy_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**" + (
                    "ëª¨ë¸ë³„ ì„±ëŠ¥" if lang == 'ko' else "Performance by Model"
                ) + "**")
                for model, acc in very_easy_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info(
                    "ë§¤ìš° ì‰¬ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No very easy problems found."
                )
        
        st.markdown("---")
        
        # 5. ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í…Œì´ë¸”
        st.subheader("ğŸ“‹ " + t['difficulty_stats_by_range'])
        
        detailed_difficulty = model_difficulty.pivot_table(
            index=model_col,
            columns=diff_col,
            values=acc_col,
            aggfunc='mean'
        ).round(2)
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        detailed_difficulty = detailed_difficulty.reindex(columns=difficulty_order)
        
        st.dataframe(
            detailed_difficulty.style.background_gradient(cmap='RdYlGn', axis=None),
            width='stretch'
        )
        
        # ë‚œì´ë„ ë¶„ì„ ì¢…í•© ì¸ì‚¬ì´íŠ¸
        # ì „ì²´ ë¬¸ì œ ë‚œì´ë„ ë¶„í¬ ë¶„ì„
        total_problems = len(difficulty)
        very_hard_pct = (len(difficulty[difficulty['difficulty_score'] < 20]) / total_problems * 100) if total_problems > 0 else 0
        hard_pct = (len(difficulty[(difficulty['difficulty_score'] >= 20) & (difficulty['difficulty_score'] < 40)]) / total_problems * 100) if total_problems > 0 else 0
        medium_pct = (len(difficulty[(difficulty['difficulty_score'] >= 40) & (difficulty['difficulty_score'] < 60)]) / total_problems * 100) if total_problems > 0 else 0
        easy_pct = (len(difficulty[(difficulty['difficulty_score'] >= 60) & (difficulty['difficulty_score'] < 80)]) / total_problems * 100) if total_problems > 0 else 0
        very_easy_pct = (len(difficulty[difficulty['difficulty_score'] >= 80]) / total_problems * 100) if total_problems > 0 else 0
        
        st.info(f"""
        ğŸ’¡ **{"ë‚œì´ë„ ë¶„í¬ ì¢…í•© ë¶„ì„" if lang == 'ko' else "Overall Difficulty Distribution"}**:
        
        ğŸ“Š **{"ë¬¸ì œ ë‚œì´ë„ êµ¬ì„±" if lang == 'ko' else "Problem Composition"}**:
        - **{"ë§¤ìš° ì–´ë ¤ì›€" if lang == 'ko' else "Very Hard"}**: {very_hard_pct:.1f}% ({len(difficulty[difficulty['difficulty_score'] < 20])}{"ê°œ" if lang == 'ko' else ""})
        - **{"ì–´ë ¤ì›€" if lang == 'ko' else "Hard"}**: {hard_pct:.1f}% ({len(difficulty[(difficulty['difficulty_score'] >= 20) & (difficulty['difficulty_score'] < 40)])}{"ê°œ" if lang == 'ko' else ""})
        - **{"ë³´í†µ" if lang == 'ko' else "Medium"}**: {medium_pct:.1f}% ({len(difficulty[(difficulty['difficulty_score'] >= 40) & (difficulty['difficulty_score'] < 60)])}{"ê°œ" if lang == 'ko' else ""})
        - **{"ì‰¬ì›€" if lang == 'ko' else "Easy"}**: {easy_pct:.1f}% ({len(difficulty[(difficulty['difficulty_score'] >= 60) & (difficulty['difficulty_score'] < 80)])}{"ê°œ" if lang == 'ko' else ""})
        - **{"ë§¤ìš° ì‰¬ì›€" if lang == 'ko' else "Very Easy"}**: {very_easy_pct:.1f}% ({len(difficulty[difficulty['difficulty_score'] >= 80])}{"ê°œ" if lang == 'ko' else ""})
        
        ğŸ¯ **{"ë³€ë³„ë ¥ í‰ê°€" if lang == 'ko' else "Discriminatory Power"}**:
        - {"ì¤‘ê°„ ë‚œì´ë„(40-60%)" if lang == 'ko' else "Medium difficulty (40-60%)"}: {medium_pct:.1f}% - {"ì´ìƒì  ë³€ë³„ë ¥ êµ¬ê°„" if lang == 'ko' else "Ideal discriminatory range"}
        - {"ë³€ë³„ë ¥" if lang == 'ko' else "Overall discriminatory power"}: {"ìš°ìˆ˜" if medium_pct > 30 else "ë³´í†µ" if medium_pct > 20 else "ê°œì„  í•„ìš”" if lang == 'ko' else "Good" if medium_pct > 30 else "Fair" if medium_pct > 20 else "Needs improvement"}
        
        ğŸ“ **{"í•™ìŠµ ì „ëµ" if lang == 'ko' else "Study Strategy"}**:
        - **{"ê¸°ì´ˆ ë‹¤ì§€ê¸°" if lang == 'ko' else "Foundation"}**: {"ì‰¬ìš´ ë¬¸ì œë¡œ ê°œë… í™•ë¦½" if lang == 'ko' else "Master basics with easy problems"}
        - **{"ì‹¤ë ¥ í–¥ìƒ" if lang == 'ko' else "Improvement"}**: {"ì¤‘ê°„ ë‚œì´ë„ë¡œ ì‹¤ì „ ëŒ€ë¹„" if lang == 'ko' else "Practice with medium problems"}
        - **{"ì‹¬í™” í•™ìŠµ" if lang == 'ko' else "Advanced"}**: {"ì–´ë ¤ìš´ ë¬¸ì œë¡œ ê³ ë“ì  ë…¸ë¦¬ê¸°" if lang == 'ko' else "Challenge with hard problems"}
        """)
    
    # íƒ­ 9: í† í° ë° ë¹„ìš© ë¶„ì„
    with tabs[8]:
        st.header(f"ğŸ’° {t['token_cost_analysis']}")
        
        # í† í° ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        token_columns = {
            'input': ['ì…ë ¥í† í°', 'input_tokens', 'Input Tokens'],
            'output': ['ì¶œë ¥í† í°', 'output_tokens', 'Output Tokens'],
            'total': ['ì´í† í°', 'total_tokens', 'Total Tokens'],
            'cost': ['ë¹„ìš©ìˆ˜ì¤€', 'cost_level', 'Cost Level']
        }
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì°¾ê¸°
        available_cols = {}
        for key, possible_names in token_columns.items():
            for col_name in possible_names:
                if col_name in filtered_df.columns:
                    available_cols[key] = col_name
                    break
        
        if not available_cols:
            st.info("Token usage data not available in the dataset." if lang == 'en' else "í† í° ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ë°ì´í„°ì…‹ì— ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë°ì´í„° ì¤€ë¹„ - NaN í•„í„°ë§ì„ í•œ ë²ˆì— ì²˜ë¦¬ (copy ì œê±°)
            valid_mask = pd.Series(True, index=filtered_df.index)
            for key, col in available_cols.items():
                if col in filtered_df.columns:
                    valid_mask &= filtered_df[col].notna()
            
            token_df = filtered_df[valid_mask]
            
            if len(token_df) == 0:
                st.info("No valid token data available after filtering." if lang == 'en' else "í•„í„°ë§ í›„ ìœ íš¨í•œ í† í° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # 1. í† í° í†µê³„ ìš”ì•½
                st.subheader(f"ğŸ“Š {t['token_stats']}")
                
                # ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
                agg_dict = {}
                if 'input' in available_cols:
                    agg_dict[available_cols['input']] = ['sum', 'mean']
                if 'output' in available_cols:
                    agg_dict[available_cols['output']] = ['sum', 'mean']
                if 'total' in available_cols:
                    agg_dict[available_cols['total']] = ['sum', 'mean']
                
                model_token_stats = token_df.groupby('ëª¨ë¸').agg(agg_dict).reset_index()
                
                # ì»¬ëŸ¼ëª… ì •ë¦¬
                new_cols = ['ëª¨ë¸']
                for col in model_token_stats.columns[1:]:
                    if col[0] == available_cols.get('input', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_ì…ë ¥í† í°')
                        else:
                            new_cols.append('í‰ê· _ì…ë ¥í† í°')
                    elif col[0] == available_cols.get('output', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_ì¶œë ¥í† í°')
                        else:
                            new_cols.append('í‰ê· _ì¶œë ¥í† í°')
                    elif col[0] == available_cols.get('total', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_í† í°')
                        else:
                            new_cols.append('í‰ê· _í† í°')
                
                model_token_stats.columns = new_cols
                
                # ì •í™•ë„ ì¶”ê°€
                model_acc = token_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                model_acc.columns = ['ëª¨ë¸', 'ì •í™•ë„']
                model_acc['ì •í™•ë„'] = model_acc['ì •í™•ë„'] * 100
                
                model_token_stats = model_token_stats.merge(model_acc, on='ëª¨ë¸')
                
                # ë¬¸ì œ ìˆ˜ ì¶”ê°€
                model_problem_count = token_df.groupby('ëª¨ë¸')['Question'].count().reset_index()
                model_problem_count.columns = ['ëª¨ë¸', 'ë¬¸ì œìˆ˜']
                model_token_stats = model_token_stats.merge(model_problem_count, on='ëª¨ë¸')
                
                # ë¹„ìš© ìˆ˜ì¤€ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if 'cost' in available_cols:
                    cost_col = available_cols['cost']
                    # ê°€ì¥ ë¹ˆë²ˆí•œ ë¹„ìš© ìˆ˜ì¤€ ì°¾ê¸°
                    model_cost = token_df.groupby('ëª¨ë¸')[cost_col].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown').reset_index()
                    model_cost.columns = ['ëª¨ë¸', 'ë¹„ìš©ìˆ˜ì¤€']
                    model_token_stats = model_token_stats.merge(model_cost, on='ëª¨ë¸')
                
                # í† í° íš¨ìœ¨ì„± ê³„ì‚° (ì •ë‹µë‹¹ í† í°)
                if 'ì´_í† í°' in model_token_stats.columns:
                    model_token_stats['ì •ë‹µë‹¹_í† í°'] = model_token_stats.apply(
                        lambda row: row['ì´_í† í°'] / (row['ë¬¸ì œìˆ˜'] * row['ì •í™•ë„'] / 100) if row['ì •í™•ë„'] > 0 else 0,
                        axis=1
                    )
                
                # ì£¼ìš” ë©”íŠ¸ë¦­ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    if 'ì´_í† í°' in model_token_stats.columns:
                        total_tokens = model_token_stats['ì´_í† í°'].sum()
                        st.metric(
                            t['total_tokens'],
                            f"{total_tokens:,.0f}"
                        )
                
                with col2:
                    if 'í‰ê· _í† í°' in model_token_stats.columns:
                        avg_tokens = model_token_stats['í‰ê· _í† í°'].mean()
                        st.metric(
                            t['avg_tokens_per_problem'],
                            f"{avg_tokens:,.0f}"
                        )
                
                with col3:
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        total_input = model_token_stats['ì´_ì…ë ¥í† í°'].sum()
                        total_output = model_token_stats['ì´_ì¶œë ¥í† í°'].sum()
                        io_ratio = total_input / total_output if total_output > 0 else 0
                        st.metric(
                            t['io_ratio'],
                            f"{io_ratio:.2f}:1"
                        )
                
                with col4:
                    if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns and len(model_token_stats[model_token_stats['ì •ë‹µë‹¹_í† í°'] > 0]) > 0:
                        # ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸ (ì •ë‹µë‹¹ í† í°ì´ ì ì€ ëª¨ë¸)
                        valid_stats = model_token_stats[model_token_stats['ì •ë‹µë‹¹_í† í°'] > 0]
                        most_efficient = valid_stats.loc[valid_stats['ì •ë‹µë‹¹_í† í°'].idxmin()]
                        st.metric(
                            t['most_efficient'],
                            most_efficient['ëª¨ë¸'],
                            f"{most_efficient['ì •ë‹µë‹¹_í† í°']:,.0f} " + t['tokens']
                        )
                
                # ìƒì„¸ í…Œì´ë¸”
                st.markdown("---")
                st.subheader("ğŸ“‹ " + ("ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸" if lang == 'ko' else "Detailed Token Usage by Model"))
                
                # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                display_cols = ['ëª¨ë¸']
                if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_ì…ë ¥í† í°')
                if 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_ì¶œë ¥í† í°')
                if 'ì´_í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_í† í°')
                if 'í‰ê· _í† í°' in model_token_stats.columns:
                    display_cols.append('í‰ê· _í† í°')
                display_cols.extend(['ì •í™•ë„', 'ë¬¸ì œìˆ˜'])
                if 'ë¹„ìš©ìˆ˜ì¤€' in model_token_stats.columns:
                    display_cols.append('ë¹„ìš©ìˆ˜ì¤€')
                if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns:
                    display_cols.append('ì •ë‹µë‹¹_í† í°')
                
                display_df = model_token_stats[display_cols].sort_values('ì´_í† í°' if 'ì´_í† í°' in display_cols else 'ëª¨ë¸', ascending=False)
                
                # í¬ë§·íŒ…
                format_dict = {
                    'ì´_ì…ë ¥í† í°': '{:,.0f}',
                    'ì´_ì¶œë ¥í† í°': '{:,.0f}',
                    'ì´_í† í°': '{:,.0f}',
                    'í‰ê· _í† í°': '{:,.0f}',
                    'ì •í™•ë„': '{:.2f}%',
                    'ì •ë‹µë‹¹_í† í°': '{:,.0f}'
                }
                
                st.dataframe(
                    display_df.style.format(format_dict).background_gradient(
                        subset=['ì •ë‹µë‹¹_í† í°'] if 'ì •ë‹µë‹¹_í† í°' in display_cols else [],
                        cmap='RdYlGn_r'
                    ),
                    width='stretch'
                )
                
                st.markdown("---")
                
                # 2. ì‹œê°í™”
                st.subheader("ğŸ“Š " + ("í† í° ì‚¬ìš©ëŸ‰ ì‹œê°í™”" if lang == 'ko' else "Token Usage Visualization"))
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # ëª¨ë¸ë³„ ì´ í† í° ì‚¬ìš©ëŸ‰
                    if 'ì´_í† í°' in model_token_stats.columns:
                        fig = px.bar(
                            display_df,
                            x='ëª¨ë¸',
                            y='ì´_í† í°',
                            title=t['total_tokens'] + ' (' + ('ëª¨ë¸ë³„' if lang == 'ko' else 'by Model') + ')',
                            text='ì´_í† í°',
                            color='ì´_í† í°',
                            color_continuous_scale='Blues'
                        )
                        fig.update_traces(
                            texttemplate='%{text:,.0f}',
                            textposition='outside',                textfont=dict(size=annotation_size),
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['total_tokens'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig, width='stretch')
                
                with col2:
                    # ì…ì¶œë ¥ í† í° ë¹„êµ
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        fig = go.Figure()
                        fig.add_trace(go.Bar(
                            name=t['input_tokens'],
                            x=display_df['ëª¨ë¸'],
                            y=display_df['ì´_ì…ë ¥í† í°'],
                            marker_color='lightblue',
                            marker_line_color='black',
                            marker_line_width=1.5
                        ))
                        fig.add_trace(go.Bar(
                            name=t['output_tokens'],
                            x=display_df['ëª¨ë¸'],
                            y=display_df['ì´_ì¶œë ¥í† í°'],
                            marker_color='lightcoral',
                            marker_line_color='black',
                            marker_line_width=1.5
                        ))
                        
                        fig.update_layout(
                            barmode='stack',
                            title=f"{t['input_tokens']} vs {t['output_tokens']}",
                            height=400,
                            yaxis_title=t['tokens'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig, width='stretch')
                
                st.markdown("---")
                
                # 3. í† í° íš¨ìœ¨ì„± ë¶„ì„
                if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns:
                    st.subheader("ğŸ¯ " + (t['token_efficiency']))
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ì •ë‹µë‹¹ í† í° ì‚¬ìš©ëŸ‰
                        fig = px.bar(
                            display_df.sort_values('ì •ë‹µë‹¹_í† í°'),
                            x='ëª¨ë¸',
                            y='ì •ë‹µë‹¹_í† í°',
                            title=t['token_per_correct'],
                            text='ì •ë‹µë‹¹_í† í°',
                            color='ì •ë‹µë‹¹_í† í°',
                            color_continuous_scale='RdYlGn_r'
                        )
                        fig.update_traces(
                            texttemplate='%{text:,.0f}',
                            textposition='outside',                textfont=dict(size=annotation_size),
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['tokens'] + ' / ' + t['correct'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig, width='stretch')
                    
                    with col2:
                        # í† í° vs ì •í™•ë„ ì‚°ì ë„
                        if 'í‰ê· _í† í°' in model_token_stats.columns:
                            fig = px.scatter(
                                display_df,
                                x='í‰ê· _í† í°',
                                y='ì •í™•ë„',
                                size='ë¬¸ì œìˆ˜',
                                text='ëª¨ë¸',
                                title=t['token_efficiency'] + ' vs ' + t['accuracy'],
                                labels={
                                    'í‰ê· _í† í°': t['avg_tokens_per_problem'],
                                    'ì •í™•ë„': t['accuracy'] + ' (%)'
                                }
                            )
                            fig.update_traces(
                                textposition='top center',
                                marker=dict(
                                    line=dict(width=2, color='black'),
                                    opacity=0.7
                                )
                            )
                            fig.update_layout(height=400)
                            st.plotly_chart(fig, width='stretch')
                
                st.markdown("---")
                
                # 4. ë¹„ìš© ë¶„ì„ (ë¹„ìš© ìˆ˜ì¤€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
                if 'cost' in available_cols:
                    st.subheader("ğŸ’µ " + t['cost_analysis'])
                    
                    cost_col = available_cols['cost']
                    
                    # ğŸ” ë””ë²„ê¹… ì •ë³´ (í¼ì¹˜ê¸°/ì ‘ê¸°)
                    with st.expander("ğŸ” " + ("ë¹„ìš© ë°ì´í„° í™•ì¸" if lang == 'ko' else "Check Cost Data")):
                        st.write("**" + ("ì›ë³¸ ë¹„ìš© ìˆ˜ì¤€ ê°’" if lang == 'ko' else "Original Cost Level Values") + ":**")
                        original_values = filtered_df[cost_col].unique().tolist()
                        st.write(f"ê³ ìœ  ê°’: {original_values}")
                        st.write(f"ê°œìˆ˜: {len(original_values)}")
                    
                    # ë¹„ìš© ìˆ˜ì¤€ì„ ì •ê·œí™” ë° ìˆœì„œ ì •ì˜
                    def normalize_cost_level(level):
                        if pd.isna(level):
                            return 'unknown'
                        level_str = str(level).lower().strip()
                        # ë¬´ë£Œ/ë¡œì»¬ ëª¨ë¸
                        if level_str in ['ë¬´ë£Œ', 'free', 'f', '0', 'local', 'localhost', 'ë¡œì»¬']:
                            return t['free']
                        # ë§¤ìš° ë‚®ìŒ
                        elif level_str in ['ë§¤ìš°ë‚®ìŒ', 'very low', 'very_low', 'vl', 'verylow']:
                            return t['very_low']
                        # ë‚®ìŒ
                        elif level_str in ['ë‚®ìŒ', 'low', 'l']:
                            return t['low']
                        # ì¤‘ê°„
                        elif level_str in ['ì¤‘ê°„', 'medium', 'mid', 'm']:
                            return t['medium_cost']
                        # ë†’ìŒ
                        elif level_str in ['ë†’ìŒ', 'high', 'h']:
                            return t['high']
                        return level
                    
                    # ë¹„ìš© ìˆœì„œ ì •ì˜ (ë¬´ë£Œ â†’ ë§¤ìš°ë‚®ìŒ â†’ ë‚®ìŒ â†’ ì¤‘ê°„ â†’ ë†’ìŒ)
                    cost_order = [t['free'], t['very_low'], t['low'], t['medium_cost'], t['high']]
                    
                    token_df['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'] = token_df[cost_col].apply(normalize_cost_level)
                    model_token_stats['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'] = model_token_stats['ë¹„ìš©ìˆ˜ì¤€'].apply(normalize_cost_level) if 'ë¹„ìš©ìˆ˜ì¤€' in model_token_stats.columns else t['medium_cost']
                    
                    # ğŸ” ì •ê·œí™” í›„ ê°’ í™•ì¸
                    with st.expander("ğŸ” " + ("ì •ê·œí™” í›„ ë¹„ìš© ìˆ˜ì¤€ ê°’" if lang == 'ko' else "Normalized Cost Level Values")):
                        normalized_values = token_df['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'].unique().tolist()
                        st.write(f"**ì •ê·œí™”ëœ ê³ ìœ  ê°’**: {normalized_values}")
                        st.write(f"**ì •ì˜ëœ ìˆœì„œ (cost_order)**: {cost_order}")
                        
                        # ìˆœì„œì— ì—†ëŠ” ê°’ í™•ì¸
                        unexpected = [v for v in normalized_values if v not in cost_order]
                        if unexpected:
                            st.warning(f"âš ï¸ ì •ì˜ëœ ìˆœì„œì— ì—†ëŠ” ê°’: {unexpected}")
                        else:
                            st.success("âœ… ëª¨ë“  ê°’ì´ ì •ì˜ëœ ìˆœì„œì— í¬í•¨ë¨")
                    
                    # ğŸ†• ì‹¤ì œ ë¹„ìš© ê³„ì‚° ê¸°ëŠ¥ ì¶”ê°€
                    st.markdown("---")
                    st.subheader("ğŸ’° " + t['actual_cost'] + " " + ('ê³„ì‚°ê¸°' if lang == 'ko' else 'Calculator'))
                    
                    # ëª¨ë¸ë³„ API ê°€ê²© ì •ì˜ (2024-2025 ê¸°ì¤€, USD per 1M tokens)
                    MODEL_PRICING = {
                        # OpenAI (2025ë…„ 11ì›” ê¸°ì¤€, per 1M tokens)
                        'GPT-4o': {'input': 5.00, 'output': 15.00},  # 2025ë…„ ì—…ë°ì´íŠ¸
                        'GPT-4o-Mini': {'input': 0.150, 'output': 0.600},
                        'GPT-4-Turbo': {'input': 10.00, 'output': 30.00},
                        'GPT-3.5-Turbo': {'input': 0.50, 'output': 1.50},
                        # Anthropic (2025ë…„ 11ì›” ê¸°ì¤€, per 1M tokens)
                        'Claude-Opus-4.5': {'input': 5.00, 'output': 25.00},  # 2025ë…„ 11ì›” ì¶œì‹œ
                        'Claude-Sonnet-4.5': {'input': 3.00, 'output': 15.00},  # 2025ë…„ 10ì›” ì¶œì‹œ
                        'Claude-Sonnet-4': {'input': 3.00, 'output': 15.00},
                        'Claude-Haiku-4.5': {'input': 1.00, 'output': 5.00},  # 2025ë…„ 11ì›” ì¶œì‹œ (ì—…ë°ì´íŠ¸ë¨!)
                        'Claude-3.5-Sonnet': {'input': 3.00, 'output': 15.00},
                        'Claude-3.5-Haiku': {'input': 0.80, 'output': 4.00},
                        'Claude-3-Opus': {'input': 15.00, 'output': 75.00},
                        'Claude-3-Sonnet': {'input': 3.00, 'output': 15.00},
                        'Claude-3-Haiku': {'input': 0.25, 'output': 1.25},
                        # Google (2025ë…„ ê¸°ì¤€, per 1M tokens)
                        'Gemini-1.5-Pro': {'input': 1.25, 'output': 5.00},
                        'Gemini-1.5-Flash': {'input': 0.075, 'output': 0.30},
                        # Alibaba (ì˜¤í”ˆì†ŒìŠ¤)
                        'Qwen-2.5': {'input': 0.00, 'output': 0.00},  # ì˜¤í”ˆì†ŒìŠ¤/ë¡œì»¬
                        'Qwen2.5': {'input': 0.00, 'output': 0.00},  # ì˜¤í”ˆì†ŒìŠ¤/ë¡œì»¬
                        # LG AI Research
                        'EXAONE-3.5': {'input': 0.00, 'output': 0.00},  # ë¡œì»¬/ë¬´ë£Œ
                        # Meta
                        'Llama-3.3': {'input': 0.00, 'output': 0.00},  # ì˜¤í”ˆì†ŒìŠ¤/ë¡œì»¬
                        'Llama-3': {'input': 0.00, 'output': 0.00},  # ì˜¤í”ˆì†ŒìŠ¤/ë¡œì»¬
                    }
                    
                    # ê°€ê²© ì •ë³´ í‘œì‹œ
                    with st.expander("ğŸ“‹ " + ("ëª¨ë¸ë³„ API ê°€ê²© ì •ë³´ (2025ë…„ 11ì›” ê¸°ì¤€)" if lang == 'ko' else "API Pricing by Model (November 2025)")):
                        pricing_data = []
                        for model, prices in MODEL_PRICING.items():
                            pricing_data.append({
                                'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                                'ì…ë ¥ ($/1M)' if lang == 'ko' else 'Input ($/1M)': f"${prices['input']:.3f}",
                                'ì¶œë ¥ ($/1M)' if lang == 'ko' else 'Output ($/1M)': f"${prices['output']:.3f}"
                            })
                        st.dataframe(pd.DataFrame(pricing_data), width='stretch')
                        st.caption("ğŸ’¡ " + ("ê°€ê²©ì€ ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ê°€ê²©ì€ ê° ì œê³µì—…ì²´ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”." if lang == 'ko' else "Prices may vary. Check provider websites for latest pricing."))
                        st.caption("ğŸ“… " + ("ì—…ë°ì´íŠ¸: 2025ë…„ 11ì›” (Claude Opus 4.5, Sonnet 4.5, Haiku 4.5 í¬í•¨)" if lang == 'ko' else "Updated: November 2025 (includes Claude Opus 4.5, Sonnet 4.5, Haiku 4.5)"))
                    
                    # ì‹¤ì œ ë¹„ìš© ê³„ì‚°
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        st.markdown("---")
                        
                        cost_calculations = []
                        for _, row in model_token_stats.iterrows():
                            model = row['ëª¨ë¸']
                            input_tokens = row['ì´_ì…ë ¥í† í°']
                            output_tokens = row['ì´_ì¶œë ¥í† í°']
                            
                            # ëª¨ë¸ëª… ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­)
                            matched_pricing = None
                            for price_model, pricing in MODEL_PRICING.items():
                                if price_model.replace('-', '').replace('.', '').lower() in model.replace('-', '').replace('.', '').lower():
                                    matched_pricing = pricing
                                    break
                            
                            if matched_pricing:
                                # ë¹„ìš© ê³„ì‚° (USD)
                                input_cost = (input_tokens / 1_000_000) * matched_pricing['input']
                                output_cost = (output_tokens / 1_000_000) * matched_pricing['output']
                                total_cost = input_cost + output_cost
                                
                                # ë¬¸ì œë‹¹ ë¹„ìš©
                                cost_per_problem = total_cost / row['ë¬¸ì œìˆ˜'] if row['ë¬¸ì œìˆ˜'] > 0 else 0
                                
                                # ì •ë‹µë‹¹ ë¹„ìš© (íš¨ìœ¨ì„± ì§€í‘œ)
                                correct_answers = row['ë¬¸ì œìˆ˜'] * row['ì •í™•ë„'] / 100
                                cost_per_correct = total_cost / correct_answers if correct_answers > 0 else 0
                                
                                cost_calculations.append({
                                    'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                                    'ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)': total_cost,
                                    'ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)': cost_per_problem,
                                    'ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)': cost_per_correct,
                                    'ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)': row['ì •í™•ë„'],
                                    'ì…ë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Input Cost ($)': input_cost,
                                    'ì¶œë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Output Cost ($)': output_cost
                                })
                        
                        if cost_calculations:
                            cost_df = pd.DataFrame(cost_calculations)
                            
                            # ë¹„ìš© íš¨ìœ¨ì„±ìœ¼ë¡œ ì •ë ¬ (ì •ë‹µë‹¹ ë¹„ìš© ê¸°ì¤€)
                            cost_df = cost_df.sort_values('ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)')
                            
                            st.subheader("ğŸ’µ " + t['actual_cost'] + " " + ('ë¶„ì„' if lang == 'ko' else 'Analysis'))
                            
                            # ì£¼ìš” ë©”íŠ¸ë¦­
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                total_cost_all = cost_df['ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)'].sum()
                                st.metric(
                                    t['total_estimated_cost'],
                                    f"${total_cost_all:.4f}"
                                )
                            
                            with col2:
                                avg_cost_per_problem = cost_df['ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)'].mean()
                                st.metric(
                                    t['cost_per_problem'],
                                    f"${avg_cost_per_problem:.6f}"
                                )
                            
                            with col3:
                                # ê°€ì¥ ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸
                                most_efficient = cost_df.iloc[0]
                                st.metric(
                                    'ìµœê³  íš¨ìœ¨' if lang == 'ko' else 'Most Efficient',
                                    most_efficient['ëª¨ë¸' if lang == 'ko' else 'Model'],
                                    f"${most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.6f}"
                                )
                            
                            with col4:
                                # ê°€ì¥ ë¹„ìš© ë¹„íš¨ìœ¨ì ì¸ ëª¨ë¸
                                least_efficient = cost_df.iloc[-1]
                                st.metric(
                                    'ìµœì € íš¨ìœ¨' if lang == 'ko' else 'Least Efficient',
                                    least_efficient['ëª¨ë¸' if lang == 'ko' else 'Model'],
                                    f"${least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.6f}"
                                )
                            
                            # ìƒì„¸ í…Œì´ë¸”
                            st.markdown("---")
                            st.dataframe(
                                cost_df.style.format({
                                    'ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)': '${:.6f}',
                                    'ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)': '${:.8f}',
                                    'ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)': '${:.8f}',
                                    'ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)': '{:.2f}%',
                                    'ì…ë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Input Cost ($)': '${:.6f}',
                                    'ì¶œë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Output Cost ($)': '${:.6f}'
                                }).background_gradient(
                                    subset=['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'],
                                    cmap='RdYlGn_r'
                                ),
                                width='stretch'
                            )
                            
                            st.markdown("---")
                            
                            # ë¹„ìš© ì‹œê°í™”
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                # ì´ ë¹„ìš© ë¹„êµ
                                fig = px.bar(
                                    cost_df,
                                    x='ëª¨ë¸' if lang == 'ko' else 'Model',
                                    y='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    title=t['total_estimated_cost'],
                                    text='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    color='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    color_continuous_scale='Reds'
                                )
                                fig.update_traces(
                                    texttemplate='$%{text:.6f}',
                                    textposition='outside',                textfont=dict(size=annotation_size),
                                    marker_line_color='black',
                                    marker_line_width=1.5
                                )
                                fig.update_layout(
                                    height=400,
                                    showlegend=False,
                                    yaxis_title=t['cost'] + ' (USD)',
                                    xaxis_title=t['model']
                                )
                                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                                st.plotly_chart(fig, width='stretch')
                            
                            with col2:
                                # ì •ë‹µë‹¹ ë¹„ìš© (íš¨ìœ¨ì„±)
                                fig = px.bar(
                                    cost_df.sort_values('ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'),
                                    x='ëª¨ë¸' if lang == 'ko' else 'Model',
                                    y='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    title=t['cost_efficiency'] + ' (' + ('ì •ë‹µë‹¹ ë¹„ìš©' if lang == 'ko' else 'Cost per Correct') + ')',
                                    text='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    color='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    color_continuous_scale='RdYlGn_r'
                                )
                                fig.update_traces(
                                    texttemplate='$%{text:.8f}',
                                    textposition='outside',                textfont=dict(size=annotation_size),
                                    marker_line_color='black',
                                    marker_line_width=1.5
                                )
                                fig.update_layout(
                                    height=400,
                                    showlegend=False,
                                    yaxis_title=t['cost'] + ' (USD)',
                                    xaxis_title=t['model']
                                )
                                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                                st.plotly_chart(fig, width='stretch')
                            
                            st.markdown("---")
                            
                            # ë¹„ìš© vs ì •í™•ë„ ì‚°ì ë„
                            fig = px.scatter(
                                cost_df,
                                x='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                y='ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)',
                                text='ëª¨ë¸' if lang == 'ko' else 'Model',
                                title=t['cost'] + ' vs ' + t['accuracy'],
                                color='ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)',
                                color_continuous_scale='RdYlGn',
                                size='ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)'
                            )
                            fig.update_traces(
                                textposition='top center',
                                marker=dict(
                                    line=dict(width=2, color='black'),
                                    opacity=0.7
                                )
                            )
                            fig.update_layout(
                                height=500,
                                yaxis=dict(range=[0, 100])
                            )
                            st.plotly_chart(fig, width='stretch')
                            
                            # ì¸ì‚¬ì´íŠ¸
                            st.success(f"""
                            ğŸ’¡ **{t['cost_efficiency']} {'ì¸ì‚¬ì´íŠ¸' if lang == 'ko' else 'Insights'}**:
                            - **{'ìµœê³  íš¨ìœ¨' if lang == 'ko' else 'Most Efficient'}**: {most_efficient['ëª¨ë¸' if lang == 'ko' else 'Model']} (${most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.8f} / {'ì •ë‹µ' if lang == 'ko' else 'correct'})
                            - **{'ìµœì € íš¨ìœ¨' if lang == 'ko' else 'Least Efficient'}**: {least_efficient['ëª¨ë¸' if lang == 'ko' else 'Model']} (${least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.8f} / {'ì •ë‹µ' if lang == 'ko' else 'correct'})
                            - **{'íš¨ìœ¨ ì°¨ì´' if lang == 'ko' else 'Efficiency Gap'}**: {(least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'] / most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']):.1f}x
                            """)
                        else:
                            st.info("ğŸ’¡ " + ("í˜„ì¬ ë°ì´í„°ì˜ ëª¨ë¸ë“¤ì— ëŒ€í•œ ê°€ê²© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ëª…ì„ í™•ì¸í•˜ê±°ë‚˜ ê°€ê²© ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”." if lang == 'ko' else "No pricing information available for current models. Please check model names or add pricing info."))
                    
                    st.markdown("---")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ë¹„ìš© ìˆ˜ì¤€ë³„ ëª¨ë¸ ë¶„í¬
                        cost_dist = token_df.groupby('ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”')['ëª¨ë¸'].nunique().reset_index()
                        cost_dist.columns = ['ë¹„ìš©ìˆ˜ì¤€', 'ëª¨ë¸ìˆ˜']
                        
                        # cost_orderì— ìˆëŠ” ê°’ë§Œ í•„í„°ë§
                        cost_dist = cost_dist[cost_dist['ë¹„ìš©ìˆ˜ì¤€'].isin(cost_order)]
                        
                        # ë¬¸ìì—´ë¡œ ëª…ì‹œì  ë³€í™˜
                        cost_dist['ë¹„ìš©ìˆ˜ì¤€'] = cost_dist['ë¹„ìš©ìˆ˜ì¤€'].astype(str)
                        
                        fig = px.pie(
                            cost_dist,
                            values='ëª¨ë¸ìˆ˜',
                            names='ë¹„ìš©ìˆ˜ì¤€',
                            title=t['cost_level'] + ' ' + ('ë¶„í¬' if lang == 'ko' else 'Distribution'),
                            hole=0.3
                        )
                        fig.update_traces(
                            textposition='inside',
                            textinfo='percent+label',                textfont=dict(size=annotation_size),
                            marker=dict(line=dict(color='black', width=2))
                        )
                        fig.update_layout(height=400)
                        st.plotly_chart(fig, width='stretch')
                    
                    with col2:
                        # ë¹„ìš© ìˆ˜ì¤€ë³„ í‰ê·  ì •í™•ë„
                        cost_acc = token_df.groupby('ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                        cost_acc.columns = ['ë¹„ìš©ìˆ˜ì¤€', 'ì •í™•ë„']
                        cost_acc['ì •í™•ë„'] = cost_acc['ì •í™•ë„'] * 100
                        
                        # cost_orderì— ìˆëŠ” ê°’ë§Œ í•„í„°ë§
                        cost_acc = cost_acc[cost_acc['ë¹„ìš©ìˆ˜ì¤€'].isin(cost_order)]
                        
                        # ë¬¸ìì—´ë¡œ ëª…ì‹œì  ë³€í™˜
                        cost_acc['ë¹„ìš©ìˆ˜ì¤€'] = cost_acc['ë¹„ìš©ìˆ˜ì¤€'].astype(str)
                        
                        fig = px.bar(
                            cost_acc,
                            x='ë¹„ìš©ìˆ˜ì¤€',
                            y='ì •í™•ë„',
                            title=t['cost_level'] + ' vs ' + t['accuracy'],
                            text='ì •í™•ë„',
                            color='ì •í™•ë„',
                            color_continuous_scale='RdYlGn'
                        )
                        fig.update_traces(
                            texttemplate='%{text:.1f}%',
                            textposition='outside',                textfont=dict(size=annotation_size),
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['accuracy'] + ' (%)',
                            yaxis=dict(range=[0, 100]),
                            xaxis=dict(
                                categoryorder='array',
                                categoryarray=cost_order
                            )
                        )
                        st.plotly_chart(fig, width='stretch')
                    
                    st.markdown("---")
                    
                    # ë¹„ìš© íš¨ìœ¨ì„± ë§¤íŠ¸ë¦­ìŠ¤
                    st.subheader("ğŸ“Š " + t['cost_efficiency'] + (' ë§¤íŠ¸ë¦­ìŠ¤' if lang == 'ko' else ' Matrix'))
                    
                    # ë¹„ìš© ìˆ˜ì¤€ê³¼ ì •í™•ë„ë¡œ ëª¨ë¸ ë¶„ë¥˜
                    if 'ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”' in model_token_stats.columns:
                        # ë°ì´í„° ì¤€ë¹„ ë° í•„í„°ë§
                        plot_data = model_token_stats.copy()
                        
                        # cost_orderì— ìˆëŠ” ê°’ë§Œ í•„í„°ë§
                        plot_data = plot_data[plot_data['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'].isin(cost_order)]
                        
                        # ë¹„ìš©ìˆ˜ì¤€ì„ ë¬¸ìì—´ë¡œ ëª…ì‹œì  ë³€í™˜
                        plot_data['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'] = plot_data['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'].astype(str)
                        
                        fig = px.scatter(
                            plot_data,
                            x='ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”',
                            y='ì •í™•ë„',
                            size='ì´_í† í°' if 'ì´_í† í°' in plot_data.columns else 'ë¬¸ì œìˆ˜',
                            text='ëª¨ë¸',
                            title=t['cost_level'] + ' vs ' + t['accuracy'],
                            color='ì •í™•ë„',
                            color_continuous_scale='RdYlGn'
                        )
                        fig.update_traces(
                            textposition='top center',
                            marker=dict(
                                line=dict(width=2, color='black'),
                                opacity=0.7
                            )
                        )
                        fig.update_layout(
                            height=500,
                            yaxis=dict(range=[0, 100]),
                            xaxis=dict(
                                title=t['cost_level'],
                                categoryorder='array',
                                categoryarray=cost_order
                            ),
                            yaxis_title=t['accuracy'] + ' (%)'
                        )
                        st.plotly_chart(fig, width='stretch')
                        
                        # ì¸ì‚¬ì´íŠ¸
                        st.info(f"""
                        ğŸ’¡ **{t['cost_efficiency']} {'ì¸ì‚¬ì´íŠ¸' if lang == 'ko' else 'Insights'}**:
                        - **{'ê³ íš¨ìœ¨ ì˜ì—­' if lang == 'ko' else 'High Efficiency Zone'}** ({'ë‚®ì€ ë¹„ìš© + ë†’ì€ ì •í™•ë„' if lang == 'ko' else 'Low cost + High accuracy'}): {'ì¢Œì¸¡ ìƒë‹¨' if lang == 'ko' else 'Top left'}
                        - **{'ê³ ë¹„ìš© ì˜ì—­' if lang == 'ko' else 'High Cost Zone'}** ({'ë†’ì€ ë¹„ìš©' if lang == 'ko' else 'High cost'}): {'ìš°ì¸¡' if lang == 'ko' else 'Right side'}
                        - {'ëª¨ë¸ ì„ íƒ ì‹œ ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì„ ê³ ë ¤í•˜ì„¸ìš”' if lang == 'ko' else 'Consider cost-performance ratio when selecting models'}
                        """)
                
                st.markdown("---")
                
                # 5. í…ŒìŠ¤íŠ¸ë³„ í† í° ë¶„ì„ (í…ŒìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
                if 'í…ŒìŠ¤íŠ¸ëª…' in token_df.columns and token_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
                    st.subheader("ğŸ“š " + ("í…ŒìŠ¤íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰" if lang == 'ko' else "Token Usage by Test"))
                    
                    token_col = available_cols.get('total', available_cols.get('input', list(available_cols.values())[0]))
                    test_token = token_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])[token_col].sum().reset_index()
                    test_token.columns = ['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…', 'ì´í† í°']
                    
                    fig = px.bar(
                        test_token,
                        x='í…ŒìŠ¤íŠ¸ëª…',
                        y='ì´í† í°',
                        color='ëª¨ë¸',
                        barmode='group',
                        title='í…ŒìŠ¤íŠ¸ë³„ ëª¨ë¸ í† í° ì‚¬ìš©ëŸ‰' if lang == 'ko' else 'Token Usage by Test and Model',
                        labels={'ì´í† í°': t['total_tokens']}
                    )
                    fig.update_layout(
                        height=400,
                        xaxis_title=t['testname'],
                        yaxis_title=t['total_tokens']
                    )
                    fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                    st.plotly_chart(fig, width='stretch')
                
                st.markdown("---")
                
                # 6. ë¬¸ì œ ìœ í˜•ë³„ í† í° ë¶„ì„ (ì´ë¯¸ì§€ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°)
                if 'image' in token_df.columns:
                    st.subheader("ğŸ–¼ï¸ " + ("ë¬¸ì œ ìœ í˜•ë³„ í† í° ì‚¬ìš©ëŸ‰" if lang == 'ko' else "Token Usage by Problem Type"))
                    
                    # ì´ë¯¸ì§€ ë¬¸ì œ ì—¬ë¶€ êµ¬ë¶„
                    token_df['ë¬¸ì œìœ í˜•'] = token_df['image'].apply(
                        lambda x: t['text_only'] if str(x).lower() == 'text_only' or str(x) == 'X' else t['image_problem']
                    )
                    
                    token_col = available_cols.get('total', available_cols.get('input', list(available_cols.values())[0]))
                    problem_type_token = token_df.groupby(['ëª¨ë¸', 'ë¬¸ì œìœ í˜•']).agg({
                        token_col: 'mean',
                        'ì •ë‹µì—¬ë¶€': 'mean'
                    }).reset_index()
                    problem_type_token.columns = ['ëª¨ë¸', 'ë¬¸ì œìœ í˜•', 'í‰ê· í† í°', 'ì •í™•ë„']
                    problem_type_token['ì •í™•ë„'] = problem_type_token['ì •í™•ë„'] * 100
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ë¬¸ì œ ìœ í˜•ë³„ í‰ê·  í† í°
                        fig = px.bar(
                            problem_type_token,
                            x='ëª¨ë¸',
                            y='í‰ê· í† í°',
                            color='ë¬¸ì œìœ í˜•',
                            barmode='group',
                            title=t['avg_tokens_per_problem'] + ' (' + t['problem_type'] + 'ë³„)',
                            labels={'í‰ê· í† í°': t['avg_tokens_per_problem']}
                        )
                        fig.update_layout(
                            height=400,
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig, width='stretch')
                    
                    with col2:
                        # ë¬¸ì œ ìœ í˜•ë³„ ì •í™•ë„ ë¹„êµ
                        fig = px.bar(
                            problem_type_token,
                            x='ëª¨ë¸',
                            y='ì •í™•ë„',
                            color='ë¬¸ì œìœ í˜•',
                            barmode='group',
                            title=t['accuracy'] + ' (' + t['problem_type'] + 'ë³„)',
                            labels={'ì •í™•ë„': t['accuracy'] + ' (%)'}
                        )
                        fig.update_layout(
                            height=400,
                            xaxis_title=t['model'],
                            yaxis=dict(range=[0, 100])
                        )
                        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig, width='stretch')
    
    # íƒ­ 10: í…ŒìŠ¤íŠ¸ì…‹ í†µê³„
    with tabs[9]:
        st.header(f"ğŸ“‹ {t['testset_stats']}")
        
        # ìƒë‹¨ì— ì „ì²´ í†µê³„ ìš”ì•½ ì¶”ê°€
        st.subheader("ğŸ“Š " + ("ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í†µê³„" if lang == 'ko' else "Overall Test Set Statistics"))
        
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì „ì²´ í†µê³„
        total_all_problems = 0
        total_law_problems = 0
        total_non_law_problems = 0
        
        if selected_tests:
            for test_name in selected_tests:
                if test_name in testsets:
                    test_df = testsets[test_name]
                    total_all_problems += len(test_df)
                    if 'law' in test_df.columns:
                        total_law_problems += len(test_df[test_df['law'] == 'O'])
                        total_non_law_problems += len(test_df[test_df['law'] != 'O'])
                    else:
                        total_non_law_problems += len(test_df)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "ì´ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Total Problems",
                f"{total_all_problems:,}",
                help="ì„ íƒëœ ëª¨ë“  í…ŒìŠ¤íŠ¸ì˜ ì „ì²´ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)"
            )
        with col2:
            st.metric(
                "ë²•ë ¹ ë¬¸ì œ" if lang == 'ko' else "Law Problems",
                f"{total_law_problems:,}",
                help="ë²•ë ¹ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)"
            )
        with col3:
            st.metric(
                "ë¹„ë²•ë ¹ ë¬¸ì œ" if lang == 'ko' else "Non-Law Problems",
                f"{total_non_law_problems:,}",
                help="ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)"
            )
        
        st.info("ğŸ’¡ " + (
            "ì´ í†µê³„ëŠ” í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ ê¸°ì¤€ì…ë‹ˆë‹¤. ì „ì²´ ìš”ì•½ íƒ­ì˜ ìˆ˜ì¹˜ì™€ ë™ì¼í•©ë‹ˆë‹¤." 
            if lang == 'ko' 
            else "These statistics are based on test set files. They match the Overview tab."
        ))
        
        st.markdown("---")
        
        if selected_tests:
            # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ê°œë³„ í†µê³„ í‘œì‹œ
            for test_name in selected_tests:
                stats = get_testset_statistics(testsets, test_name, lang)
                if stats:
                    st.subheader(f"ğŸ“– {test_name}")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(t['total_problems'], stats['total_problems'])
                    
                    with col2:
                        if 'law_problems' in stats:
                            st.metric(t['law_problems'], stats['law_problems'])
                    
                    with col3:
                        if 'non_law_problems' in stats:
                            st.metric(t['non_law_problems'], stats['non_law_problems'])
                    
                    # ê³¼ëª©ë³„, ì—°ë„ë³„, ì„¸ì…˜ë³„ í†µê³„
                    if 'by_subject' in stats or 'by_year' in stats or 'by_session' in stats:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if 'by_subject' in stats:
                                st.markdown(f"**{t['by_subject']}**")
                                subject_df = pd.DataFrame(list(stats['by_subject'].items()), 
                                                         columns=['Subject', 'Count'])
                                fig = px.bar(subject_df, x='Subject', y='Count', 
                                           title=t['subject_distribution'])
                                fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
                                st.plotly_chart(fig, width='stretch')
                        
                        with col2:
                            if 'by_year' in stats:
                                st.markdown(f"**{t['by_year']}**")
                                year_df = pd.DataFrame(list(stats['by_year'].items()), 
                                                      columns=['Year', 'Count'])
                                fig = px.bar(year_df, x='Year', y='Count', 
                                           title=t['year_distribution'])
                                st.plotly_chart(fig, width='stretch')
                        
                        with col3:
                            if 'by_session' in stats:
                                st.markdown(f"**{t['by_session']}**")
                                session_df = pd.DataFrame(list(stats['by_session'].items()), 
                                                         columns=['Session', 'Count'])
                                fig = px.bar(session_df, x='Session', y='Count', 
                                           title=t['session_distribution'])
                                st.plotly_chart(fig, width='stretch')
                    
                    st.markdown("---")
        else:
            st.info("í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # ì‚¬ì´ë“œë°” í•˜ë‹¨ ì •ë³´
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"### ğŸ“Œ {t['help']}")
    st.sidebar.markdown(f"""
    **{t['new_features']}:**
    - âœ¨ **{t['token_cost_analysis']}**: í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„
    - âœ¨ **{t['session']} {t['filters']}**: {t['session_filter']}
    - âœ¨ **{t['incorrect_analysis']}**: {t['incorrect_pattern']}
    - âœ¨ **{t['difficulty_analysis']}**: {t['difficulty_comparison']}
    - âœ¨ **{t['problem_type']} {t['filters']}**: {t['problem_type_filter']}
    
    **{t['existing_features']}:**
    - {t['basic_filters']}
    - {t['law_analysis_desc']}
    - {t['detail_analysis']}
    """)
    
    # íƒ­ 11: ì¶”ê°€ ë¶„ì„
    with tabs[10]:
        st.header("ğŸ“‘ " + ("ì¶”ê°€ ë¶„ì„ í‘œ ë° ì‹œê°í™”" if lang == 'ko' else "Additional Analysis Tables and Visualizations"))
        
        # ========== ì¶”ê°€ ë¶„ì„ í‘œ ì„¹ì…˜ ==========
        
        st.markdown("### ğŸ“Š " + ("ì¶”ê°€ ë¶„ì„ í‘œ" if lang == 'ko' else "Additional Analysis Tables"))
        st.markdown("---")
        
        # í‘œ 1: í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •ë‹µë¥  (NEW!)
        st.subheader("ğŸ“‹ " + ("í‘œ 1: í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •ë‹µë¥  ë° í†µê³„" if lang == 'ko' else "Table 1: Average Accuracy and Statistics by Test Set"))
        table1 = create_testset_accuracy_table(filtered_df, testsets, lang)
        if table1 is not None and len(table1) > 0:
            display_table_with_download(table1, "", "table1_testset_accuracy.xlsx", lang)
            
            # ê°„ë‹¨í•œ ì‹œê°í™” ì¶”ê°€
            st.markdown("#### " + ("í…ŒìŠ¤íŠ¸ì…‹ë³„ ì •í™•ë„ ë¹„êµ" if lang == 'ko' else "Accuracy Comparison by Test Set"))
            fig = px.bar(
                table1,
                x='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                y='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                text='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                color='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                color_continuous_scale='RdYlGn',
                title='í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Average Accuracy by Test Set'
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',
                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                xaxis_title='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                yaxis=dict(range=[0, 100])
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
        else:
            st.info("í…ŒìŠ¤íŠ¸ì…‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No test set data available.")
        
        st.markdown("---")
        
        # í‘œ 2: ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ (NEW!)
        st.subheader("âš–ï¸ " + ("í‘œ 2: í…ŒìŠ¤íŠ¸ì…‹ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ" if lang == 'ko' else "Table 2: Law vs Non-Law Accuracy Comparison by Test Set"))
        table2 = create_law_nonlaw_comparison_table(filtered_df, testsets, lang)
        if table2 is not None and len(table2) > 0:
            display_table_with_download(table2, "", "table2_law_nonlaw_comparison.xlsx", lang)
            
            # ê°„ë‹¨í•œ ì‹œê°í™” ì¶”ê°€ - ë²•ë ¹ vs ë¹„ë²•ë ¹ ì •ë‹µë¥ 
            st.markdown("#### " + ("ë²•ë ¹ vs ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ" if lang == 'ko' else "Law vs Non-Law Accuracy Comparison"))
            
            # ë°ì´í„° ì¤€ë¹„
            chart_data = []
            for _, row in table2.iterrows():
                test_name = row['í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name']
                chart_data.append({
                    'í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name': test_name,
                    'êµ¬ë¶„' if lang == 'ko' else 'Type': 'ë²•ë ¹' if lang == 'ko' else 'Law',
                    'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': row['ë²•ë ¹ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Law Accuracy (%)']
                })
                chart_data.append({
                    'í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name': test_name,
                    'êµ¬ë¶„' if lang == 'ko' else 'Type': 'ë¹„ë²•ë ¹' if lang == 'ko' else 'Non-Law',
                    'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': row['ë¹„ë²•ë ¹ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Non-Law Accuracy (%)']
                })
            
            chart_df = pd.DataFrame(chart_data)
            
            fig = px.bar(
                chart_df,
                x='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                y='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                color='êµ¬ë¶„' if lang == 'ko' else 'Type',
                barmode='group',
                title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ' if lang == 'ko' else 'Law vs Non-Law Accuracy by Test Set',
                color_discrete_map={
                    'ë²•ë ¹' if lang == 'ko' else 'Law': '#FF6B6B',
                    'ë¹„ë²•ë ¹' if lang == 'ko' else 'Non-Law': '#4ECDC4'
                }
            )
            fig.update_traces(
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                xaxis_title='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                yaxis=dict(range=[0, 100]),
                legend=dict(
                    title='êµ¬ë¶„' if lang == 'ko' else 'Type',
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1,
                    font=dict(size=annotation_size)
                )
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            # ì •ë‹µë¥  ì°¨ì´ ë§‰ëŒ€ ê·¸ë˜í”„
            st.markdown("#### " + ("ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹ - ë¹„ë²•ë ¹)" if lang == 'ko' else "Accuracy Difference (Law - Non-Law)"))
            fig2 = px.bar(
                table2,
                x='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                y='ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹-ë¹„ë²•ë ¹)' if lang == 'ko' else 'Accuracy Diff (Law-NonLaw)',
                text='ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹-ë¹„ë²•ë ¹)' if lang == 'ko' else 'Accuracy Diff (Law-NonLaw)',
                color='ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹-ë¹„ë²•ë ¹)' if lang == 'ko' else 'Accuracy Diff (Law-NonLaw)',
                color_continuous_scale='RdYlGn',
                title='ë²•ë ¹ ë¬¸ì œì˜ ìƒëŒ€ì  ë‚œì´ë„' if lang == 'ko' else 'Relative Difficulty of Law Problems'
            )
            fig2.update_traces(
                texttemplate='%{text:.1f}%p',
                textposition='outside',
                textfont=dict(size=annotation_size),
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig2.update_layout(
                height=400,
                showlegend=False,
                yaxis_title='ì •ë‹µë¥  ì°¨ì´ (%p)' if lang == 'ko' else 'Accuracy Difference (%p)',
                xaxis_title='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name'
            )
            fig2.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig2.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig2, width='stretch')
            
            # ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
            avg_diff = table2['ì •ë‹µë¥  ì°¨ì´ (ë²•ë ¹-ë¹„ë²•ë ¹)' if lang == 'ko' else 'Accuracy Diff (Law-NonLaw)'].mean()
            if avg_diff > 0:
                insight = f"ğŸ’¡ í‰ê· ì ìœ¼ë¡œ ë²•ë ¹ ë¬¸ì œê°€ ë¹„ë²•ë ¹ ë¬¸ì œë³´ë‹¤ {abs(avg_diff):.1f}%p ë” ì‰½ìŠµë‹ˆë‹¤." if lang == 'ko' else f"ğŸ’¡ On average, law problems are {abs(avg_diff):.1f}%p easier than non-law problems."
            elif avg_diff < 0:
                insight = f"ğŸ’¡ í‰ê· ì ìœ¼ë¡œ ë²•ë ¹ ë¬¸ì œê°€ ë¹„ë²•ë ¹ ë¬¸ì œë³´ë‹¤ {abs(avg_diff):.1f}%p ë” ì–´ë µìŠµë‹ˆë‹¤." if lang == 'ko' else f"ğŸ’¡ On average, law problems are {abs(avg_diff):.1f}%p harder than non-law problems."
            else:
                insight = "ğŸ’¡ ë²•ë ¹ ë¬¸ì œì™€ ë¹„ë²•ë ¹ ë¬¸ì œì˜ ë‚œì´ë„ê°€ ë¹„ìŠ·í•©ë‹ˆë‹¤." if lang == 'ko' else "ğŸ’¡ Law and non-law problems have similar difficulty."
            st.info(insight)
        else:
            st.info("ë²•ë ¹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No law classification data available.")
        
        st.markdown("---")
        
        # í‘œ 3: ëª¨ë¸ ì¶œì‹œ ì‹œê¸°ì™€ ì„±ëŠ¥
        st.subheader("ğŸ“… " + ("í‘œ 3: ëª¨ë¸ ì¶œì‹œ ì‹œê¸°ì™€ SafetyQ&A ì„±ëŠ¥" if lang == 'ko' else "Table 3: Model Release Date and Performance"))
        table3 = create_model_release_performance_table(filtered_df, lang)
        if table3 is not None and len(table3) > 0:
            # ë‚ ì§œë¥¼ ìˆ«ìë¡œ ë³€í™˜ (YYYY-MM -> YYYYMM)
            table3_copy = table3.copy()
            date_col = 'ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date'
            table3_copy['date_numeric'] = table3_copy[date_col].str.replace('-', '').astype(int)
            
            display_table_with_download(table3, "", "table3_model_release_performance.xlsx", lang)
        
        st.markdown("---")
        
        # í‘œ 4: ì‘ë‹µ ì‹œê°„ ë° íŒŒë¼ë¯¸í„°
        st.subheader("â±ï¸ " + ("í‘œ 4: ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„ ë° ì •ë‹µë¥ " if lang == 'ko' else "Table 4: Response Time and Accuracy by Model"))
        table4 = create_response_time_parameters_table(filtered_df, lang)
        if table4 is not None and len(table4) > 0:
            display_table_with_download(table4, "", "table4_response_time_parameters.xlsx", lang)
        else:
            st.info("ì‘ë‹µ ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No response time data available.")
        
        st.markdown("---")
        
        # í‘œ 5: ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥ ë¹„êµ (NEW!)
        st.subheader("âš–ï¸ " + ("í‘œ 5: ëª¨ë¸ë³„ ë²•ë ¹ ë¬¸í•­ vs ë¹„ë²•ë ¹ ë¬¸í•­ ì„±ëŠ¥ ë¹„êµ" if lang == 'ko' else "Table 5: Law vs Non-Law Performance by Model"))
        table5 = create_model_law_performance_table(filtered_df, lang)
        if table5 is not None and len(table5) > 0:
            display_table_with_download(table5, "", "table5_model_law_performance.xlsx", lang)
        else:
            st.info("ë²•ë ¹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No law classification data available.")
        
        st.markdown("---")
        
        # í‘œ 6: ì¶œì œ ì—°ë„ë³„ ìƒê´€ë¶„ì„
        st.subheader("ğŸ“… " + ("í‘œ 6: ì¶œì œ ì—°ë„ë³„ í‰ê·  ì •ë‹µë¥  ë° ìƒê´€ê´€ê³„" if lang == 'ko' else "Table 6: Accuracy by Year with Correlation"))
        table6 = create_year_correlation_table(filtered_df, lang)
        if table6 is not None and len(table6) > 0:
            display_table_with_download(table6, "", "table6_year_correlation.xlsx", lang)
        else:
            st.info("ì—°ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No year data available.")
        
        st.markdown("---")
        
        # í‘œ 7: ë‚œì´ë„ êµ¬ê°„ë³„ ë¶„í¬
        st.subheader("ğŸ“ˆ " + ("í‘œ 7: ë‚œì´ë„ êµ¬ê°„ë³„ ë¬¸í•­ ë¶„í¬" if lang == 'ko' else "Table 7: Problem Distribution by Difficulty"))
        table7 = create_difficulty_distribution_table(filtered_df, lang)
        if table7 is not None and len(table7) > 0:
            display_table_with_download(table7, "", "table7_difficulty_distribution.xlsx", lang)
        
        st.markdown("---")
        
        # í‘œ 8: ë‚œì´ë„ êµ¬ê°„ë³„ ëª¨ë¸ ì„±ëŠ¥ (NEW!)
        st.subheader("ğŸ¯ " + ("í‘œ 8: ì£¼ìš” ëª¨ë¸ì˜ ë‚œì´ë„ êµ¬ê°„ë³„ ì •ë‹µë¥ " if lang == 'ko' else "Table 8: Model Performance by Difficulty Level"))
        table8 = create_difficulty_model_performance_table(filtered_df, lang)
        if table8 is not None and len(table8) > 0:
            display_table_with_download(table8, "", "table8_difficulty_model_performance.xlsx", lang)
        else:
            st.info("ë‚œì´ë„ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No difficulty analysis data available.")
        
        st.markdown("---")
        
        # í‘œ 9: ë¹„ìš© íš¨ìœ¨ì„± ë¹„êµ (NEW!)
        st.subheader("ğŸ’° " + ("í‘œ 9: ì£¼ìš” ìƒì—…ìš© ëª¨ë¸ì˜ ë¹„ìš© íš¨ìœ¨ì„± ë¹„êµ" if lang == 'ko' else "Table 9: Cost Efficiency Comparison"))
        table9 = create_cost_efficiency_table(filtered_df, lang)
        if table9 is not None and len(table9) > 0:
            display_table_with_download(table9, "", "table9_cost_efficiency.xlsx", lang)
        else:
            st.info("í† í°/ë¹„ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No token/cost data available.")
        
        st.markdown("---")
        
        # í‘œ 10: ì˜¤ë‹µ íŒ¨í„´
        st.subheader("âŒ " + ("í‘œ 10: ì£¼ìš” ì˜¤ë‹µ íŒ¨í„´ ë° ë¹ˆë„" if lang == 'ko' else "Table 10: Major Error Patterns"))
        table10 = create_incorrect_pattern_table(filtered_df, lang)
        if table10 is not None and len(table10) > 0:
            display_table_with_download(table10, "", "table10_error_patterns.xlsx", lang)
        
        st.markdown("---")
        
        # í‘œ 11: ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (NEW!)
        st.subheader("ğŸ“Š " + ("í‘œ 11: SafetyQ&Aì™€ ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¹„êµ" if lang == 'ko' else "Table 11: SafetyQ&A vs General Benchmarks"))
        table11 = create_benchmark_comparison_table(filtered_df, lang)
        if table11 is not None and len(table11) > 0:
            display_table_with_download(table11, "", "table11_benchmark_comparison.xlsx", lang)
            st.caption("ğŸ’¡ " + ("ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ëŠ” SafetyQ&A ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤." if lang == 'ko' else "General benchmark scores are estimates based on SafetyQ&A performance."))
        else:
            st.info("ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "Cannot generate benchmark comparison data.")
        
        # ========== ì¶”ê°€ ì‹œê°í™” ì„¹ì…˜ ==========
        
        st.markdown("---")
        st.markdown("### ğŸ“ˆ " + ("ì¶”ê°€ ì‹œê°í™”" if lang == 'ko' else "Additional Visualizations"))
        st.markdown("---")
        
        # Figure 1: ëª¨ë¸ë³„ ì „ì²´ ì •ë‹µë¥  ë§‰ëŒ€ ê·¸ë˜í”„ (NEW!)
        st.subheader("ğŸ“Š " + ("Figure 1: ëª¨ë¸ë³„ ì „ì²´ ì •ë‹µë¥  ë§‰ëŒ€ ê·¸ë˜í”„" if lang == 'ko' else "Figure 1: Overall Accuracy by Model"))
        
        model_acc = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
        model_acc_df = model_acc.reset_index()
        model_acc_df.columns = ['ëª¨ë¸' if lang == 'ko' else 'Model', 'ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy']
        model_acc_df = model_acc_df.sort_values('ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy', ascending=False)
        
        # í‰ê· ì„  ê³„ì‚°
        avg_acc = model_acc_df['ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy'].mean()
        
        fig = px.bar(
            model_acc_df,
            x='ëª¨ë¸' if lang == 'ko' else 'Model',
            y='ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy',
            title='ëª¨ë¸ë³„ ì „ì²´ ì •ë‹µë¥ ' if lang == 'ko' else 'Overall Accuracy by Model',
            text='ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy',
            color='ì •ë‹µë¥ ' if lang == 'ko' else 'Accuracy',
            color_continuous_scale='RdYlGn'
        )
        
        # í‰ê· ì„  ì¶”ê°€
        fig.add_hline(
            y=avg_acc,
            line_dash="dash",
            line_color="red",
            annotation_text=f"í‰ê· : {avg_acc:.1f}%" if lang == 'ko' else f"Average: {avg_acc:.1f}%",
            annotation_position="right"
        )
        
        fig.update_traces(
            texttemplate='%{text:.1f}%',
            textposition='outside',
            textfont=dict(size=annotation_size),
            marker_line_color='black',
            marker_line_width=1.5
        )
        fig.update_layout(
            height=500,
            showlegend=False,
            yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
            xaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model',
            yaxis=dict(range=[0, 100])
        )
        fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
        fig.update_yaxes(tickfont=dict(size=annotation_size))
        st.plotly_chart(fig, width='stretch')
        
        st.markdown("---")
        
        # Figure 2: í…ŒìŠ¤íŠ¸ì…‹ë³„ ì •ë‹µë¥  ë°•ìŠ¤í”Œë¡¯ (NEW!)
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns and 'ì •ë‹µì—¬ë¶€' in filtered_df.columns:
            st.subheader("ğŸ“¦ " + ("Figure 2: í…ŒìŠ¤íŠ¸ì…‹ë³„ ì •ë‹µë¥  ë°•ìŠ¤í”Œë¡¯" if lang == 'ko' else "Figure 2: Accuracy Distribution by Test Set"))
            
            # ëª¨ë¸ë³„, í…ŒìŠ¤íŠ¸ë³„ ì •ë‹µë¥  ê³„ì‚°
            test_model_acc = filtered_df.groupby(['í…ŒìŠ¤íŠ¸ëª…', 'ëª¨ë¸'])['ì •ë‹µì—¬ë¶€'].mean().reset_index()
            test_model_acc['ì •ë‹µë¥ '] = test_model_acc['ì •ë‹µì—¬ë¶€'] * 100
            
            # ë””ë²„ê¹… ì •ë³´ (í¼ì¹˜ê¸°/ì ‘ê¸°)
            with st.expander("ğŸ” " + ("ë””ë²„ê¹… ì •ë³´" if lang == 'ko' else "Debug Info")):
                st.write("**ì •ë‹µë¥  ë²”ìœ„:**", f"{test_model_acc['ì •ë‹µë¥ '].min():.2f}% ~ {test_model_acc['ì •ë‹µë¥ '].max():.2f}%")
                st.write("**ë°ì´í„° ìƒ˜í”Œ:**")
                st.dataframe(test_model_acc.head(10))
            
            fig = px.box(
                test_model_acc,
                x='í…ŒìŠ¤íŠ¸ëª…',
                y='ì •ë‹µë¥ ',
                title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ì •ë‹µë¥  ë¶„í¬ (ëª¨ë¸ë³„)' if lang == 'ko' else 'Accuracy Distribution by Test Set (per Model)',
                labels={
                    'í…ŒìŠ¤íŠ¸ëª…': 'í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                    'ì •ë‹µë¥ ': 'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)'
                },
                color='í…ŒìŠ¤íŠ¸ëª…',
                points='all'  # ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
            )
            
            fig.update_layout(
                height=500,
                yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                xaxis_title='í…ŒìŠ¤íŠ¸ëª…' if lang == 'ko' else 'Test Name',
                yaxis=dict(range=[0, 100]),
                showlegend=False
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            st.markdown("---")
        
        # Figure 3: ê³¼ëª© ìœ í˜•ë³„ íˆíŠ¸ë§µ (NEW!)
        if 'Subject' in filtered_df.columns:
            st.subheader("ğŸ”¥ " + ("Figure 3: ê³¼ëª© ìœ í˜•ë³„ í‰ê·  ì •ë‹µë¥  íˆíŠ¸ë§µ" if lang == 'ko' else "Figure 3: Accuracy Heatmap by Subject Type"))
            
            # ëª¨ë¸ Ã— ê³¼ëª© íˆíŠ¸ë§µ
            subject_model = filtered_df.groupby(['ëª¨ë¸', 'Subject'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            subject_model_pivot = subject_model.unstack(fill_value=0)
            
            fig = go.Figure(data=go.Heatmap(
                z=subject_model_pivot.values,
                x=subject_model_pivot.columns,
                y=subject_model_pivot.index,
                colorscale='RdYlGn',
                text=np.round(subject_model_pivot.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": annotation_size},
                colorbar=dict(title="ì •ë‹µë¥  (%)" if lang == 'ko' else "Accuracy (%)")
            ))
            
            fig.update_layout(
                title='ëª¨ë¸ Ã— ê³¼ëª© ì •ë‹µë¥  íˆíŠ¸ë§µ' if lang == 'ko' else 'Model Ã— Subject Accuracy Heatmap',
                height=600,
                xaxis_title='ê³¼ëª©' if lang == 'ko' else 'Subject',
                yaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model'
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            st.markdown("---")
        
        # Figure 5: ì‘ë‹µ ì‹œê°„-ì •ë‹µë¥  ì‚°ì ë„ (NEW!)
        if table4 is not None and len(table4) > 0:
            st.subheader("âš¡ " + ("Figure 5: ì‘ë‹µ ì‹œê°„-ì •ë‹µë¥  ì‚°ì ë„" if lang == 'ko' else "Figure 5: Response Time vs Accuracy"))
            
            # ì‚°ì ë„ ìƒì„± (size ì œê±° - íŒŒë¼ë¯¸í„° ìˆ˜ë¡œ ìƒ‰ìƒ í‘œí˜„)
            fig = px.scatter(
                table4,
                x='í‰ê·  ì‘ë‹µì‹œê°„ (ì´ˆ)' if lang == 'ko' else 'Avg Response Time (s)',
                y='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                text='ëª¨ë¸ëª…' if lang == 'ko' else 'Model',
                color='íŒŒë¼ë¯¸í„° ìˆ˜ (B)' if lang == 'ko' else 'Parameters (B)',
                title='ì‘ë‹µ ì‹œê°„ vs ì •í™•ë„' if lang == 'ko' else 'Response Time vs Accuracy',
                color_continuous_scale='Viridis'
            )
            
            fig.update_traces(
                textposition='top center',
                textfont=dict(size=annotation_size),
                marker=dict(size=15, line=dict(width=1.5, color='black'))
            )
            fig.update_layout(
                height=500,
                yaxis=dict(range=[0, 100])
            )
            fig.update_xaxes(tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            st.info("ğŸ’¡ " + ("ì™¼ìª½ ìœ„(ë¹ ë¥¸ ì‹œê°„ + ë†’ì€ ì •í™•ë„)ê°€ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤." if lang == 'ko' else "Top left (fast time + high accuracy) is most efficient."))
            
            st.markdown("---")
        
        # Figure 6: ë²•ë ¹/ë¹„ë²•ë ¹ ê·¸ë£¹ ë§‰ëŒ€ ì°¨íŠ¸ (NEW!)
        if table5 is not None and len(table5) > 0:
            st.subheader("âš–ï¸ " + ("Figure 6: ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸í•­ ì •ë‹µë¥  ë¹„êµ" if lang == 'ko' else "Figure 6: Law vs Non-Law Accuracy Comparison"))
            
            # ë°ì´í„° ì¤€ë¹„
            chart_data = []
            for _, row in table5.iterrows():
                model = row['ëª¨ë¸ëª…' if lang == 'ko' else 'Model']
                chart_data.append({
                    'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                    'êµ¬ë¶„' if lang == 'ko' else 'Type': 'ë²•ë ¹' if lang == 'ko' else 'Law',
                    'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': row['ë²•ë ¹ ë¬¸í•­ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Law Accuracy (%)']
                })
                chart_data.append({
                    'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                    'êµ¬ë¶„' if lang == 'ko' else 'Type': 'ë¹„ë²•ë ¹' if lang == 'ko' else 'Non-Law',
                    'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': row['ë¹„ë²•ë ¹ ë¬¸í•­ ì •ë‹µë¥  (%)' if lang == 'ko' else 'Non-Law Accuracy (%)']
                })
            
            chart_df = pd.DataFrame(chart_data)
            
            fig = px.bar(
                chart_df,
                x='ëª¨ë¸' if lang == 'ko' else 'Model',
                y='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                color='êµ¬ë¶„' if lang == 'ko' else 'Type',
                barmode='group',
                title='ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ' if lang == 'ko' else 'Law vs Non-Law Accuracy by Model',
                color_discrete_map={
                    'ë²•ë ¹' if lang == 'ko' else 'Law': '#FF6B6B',
                    'ë¹„ë²•ë ¹' if lang == 'ko' else 'Non-Law': '#4ECDC4'
                }
            )
            
            fig.update_traces(marker_line_color='black', marker_line_width=1.5)
            fig.update_layout(
                height=500,
                yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                xaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model',
                yaxis=dict(range=[0, 100]),
                legend=dict(font=dict(size=annotation_size))
            )
            fig.update_xaxes(tickangle=45, tickfont=dict(size=annotation_size))
            fig.update_yaxes(tickfont=dict(size=annotation_size))
            st.plotly_chart(fig, width='stretch')
            
            st.markdown("---")
        
        # Figure 7: ì¶œì œ ì—°ë„ë³„ ì¶”ì´ ì„  ê·¸ë˜í”„ (NEW!)
        if table6 is not None and len(table6) > 0:
            st.subheader("ğŸ“ˆ " + ("Figure 7: ì¶œì œ ì—°ë„ë³„ ì •ë‹µë¥  ì¶”ì´" if lang == 'ko' else "Figure 7: Accuracy Trend by Year"))
            
            # ìƒê´€ê³„ìˆ˜, p-value í–‰ ì œê±°
            year_col = 'ì—°ë„' if lang == 'ko' else 'Year'
            # ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸ (í‘œ 6ì—ì„œëŠ” 'í‰ê·  ì •ë‹µë¥ 'ë¡œ ì €ì¥ë¨)
            acc_col = 'í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Avg Accuracy'
            
            plot_data = table6.copy()
            
            # ì—°ë„ì™€ ì •ë‹µë¥ ì´ ëª¨ë‘ ìˆ«ìì¸ í–‰ë§Œ ì„ íƒ
            if year_col in plot_data.columns and acc_col in plot_data.columns:
                # 1. íŠ¹ì • ë¬¸ìì—´ í–‰ ì œê±°
                exclude_keywords = ['ìƒê´€ê³„ìˆ˜', 'p-value', 'correlation', 'Correlation']
                for keyword in exclude_keywords:
                    plot_data = plot_data[~plot_data[year_col].astype(str).str.contains(keyword, na=False)]
                
                # 2. ì—°ë„ê°€ ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ í–‰ë§Œ ì„ íƒ
                def is_numeric_convertible(x):
                    try:
                        float(str(x))
                        return True
                    except (ValueError, TypeError):
                        return False
                
                plot_data = plot_data[plot_data[year_col].apply(is_numeric_convertible)]
                plot_data = plot_data[plot_data[acc_col].apply(is_numeric_convertible)]
                
                # 3. ë°ì´í„° íƒ€ì… ë³€í™˜
                plot_data[year_col] = plot_data[year_col].astype(int)
                plot_data[acc_col] = plot_data[acc_col].astype(float)
            
            if len(plot_data) > 0:
                try:
                    fig = px.line(
                        plot_data,
                        x=year_col,
                        y=acc_col,
                        title='ì—°ë„ë³„ í‰ê·  ì •ë‹µë¥  ì¶”ì´' if lang == 'ko' else 'Average Accuracy Trend by Year',
                        markers=True,
                        text=acc_col
                    )
                    
                    fig.update_traces(
                        texttemplate='%{text:.1f}%',
                        textposition='top center',
                        textfont=dict(size=annotation_size),
                        marker=dict(size=10, line=dict(width=2, color='black')),
                        line=dict(width=3)
                    )
                    fig.update_layout(
                        height=500,
                        yaxis_title='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                        xaxis_title='ì¶œì œ ì—°ë„' if lang == 'ko' else 'Year',
                        yaxis=dict(range=[0, 100])
                    )
                    fig.update_xaxes(tickfont=dict(size=annotation_size))
                    fig.update_yaxes(tickfont=dict(size=annotation_size))
                    st.plotly_chart(fig, width='stretch')
                except Exception as e:
                    st.error(f"{'ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜' if lang == 'ko' else 'Chart creation error'}: {str(e)}")
                    with st.expander("ğŸ” " + ("ë””ë²„ê¹… ì •ë³´" if lang == 'ko' else "Debug Info")):
                        st.write("**í•„í„°ë§ëœ ë°ì´í„°:**")
                        st.dataframe(plot_data)
            else:
                st.warning("ì—°ë„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤." if lang == 'ko' else "Insufficient year data.")
            
            st.markdown("---")
        
        # Figure 7-1: í…ŒìŠ¤íŠ¸ì…‹ë³„ ì—°ë„ë³„ ì •ë‹µë¥  ì¶”ì´ (NEW!)
        st.subheader("ğŸ“ˆ " + ("Figure 7-1: í…ŒìŠ¤íŠ¸ì…‹ë³„ ì—°ë„ë³„ ì •ë‹µë¥  ì¶”ì´" if lang == 'ko' else "Figure 7-1: Accuracy Trend by Year and Test Set"))
        
        if 'Year' in filtered_df.columns:
            # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            year_int_series = filtered_df['Year'].apply(safe_convert_to_int)
            valid_year_mask = year_int_series.notna()
            
            if valid_year_mask.any():
                # ë¶„ì„ìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
                year_test_df = pd.DataFrame({
                    'Year_Int': year_int_series[valid_year_mask],
                    'ì •ë‹µì—¬ë¶€': filtered_df.loc[valid_year_mask, 'ì •ë‹µì—¬ë¶€'],
                    'í…ŒìŠ¤íŠ¸ëª…': filtered_df.loc[valid_year_mask, 'í…ŒìŠ¤íŠ¸ëª…']
                })
                
                # 1. ì „ì²´ ì—°ë„ë³„ ì •ë‹µë¥ 
                overall_year_acc = year_test_df.groupby('Year_Int')['ì •ë‹µì—¬ë¶€'].mean() * 100
                overall_year_acc = overall_year_acc.reset_index()
                overall_year_acc.columns = ['ì—°ë„', 'ì •ë‹µë¥ ']
                overall_year_acc['í…ŒìŠ¤íŠ¸ëª…'] = 'ì „ì²´ (Overall)' if lang == 'ko' else 'Overall'
                
                # 2. í…ŒìŠ¤íŠ¸ì…‹ë³„ ì—°ë„ë³„ ì •ë‹µë¥ 
                testset_year_acc = year_test_df.groupby(['í…ŒìŠ¤íŠ¸ëª…', 'Year_Int'])['ì •ë‹µì—¬ë¶€'].mean() * 100
                testset_year_acc = testset_year_acc.reset_index()
                testset_year_acc.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ì—°ë„', 'ì •ë‹µë¥ ']
                
                # ì „ì²´ì™€ í…ŒìŠ¤íŠ¸ì…‹ë³„ ë°ì´í„° ê²°í•©
                combined_data = pd.concat([overall_year_acc, testset_year_acc], ignore_index=True)
                combined_data['ì—°ë„'] = combined_data['ì—°ë„'].astype(int)
                combined_data = combined_data.sort_values(['í…ŒìŠ¤íŠ¸ëª…', 'ì—°ë„'])
                
                # í…ŒìŠ¤íŠ¸ì…‹ ëª©ë¡ (ì „ì²´ë¥¼ ë§¨ ì•ìœ¼ë¡œ)
                test_names = combined_data['í…ŒìŠ¤íŠ¸ëª…'].unique().tolist()
                overall_name = 'ì „ì²´ (Overall)' if lang == 'ko' else 'Overall'
                if overall_name in test_names:
                    test_names.remove(overall_name)
                    test_names = [overall_name] + sorted(test_names)
                
                # ê·¸ë˜í”„ ìœ í˜• ì„ íƒ
                graph_type = st.radio(
                    "ê·¸ë˜í”„ í‘œì‹œ ë°©ì‹" if lang == 'ko' else "Graph Display Type",
                    ["í†µí•© ê·¸ë˜í”„" if lang == 'ko' else "Combined Chart", 
                     "ê°œë³„ ê·¸ë˜í”„" if lang == 'ko' else "Individual Charts"],
                    horizontal=True,
                    key="year_testset_graph_type"
                )
                
                if graph_type == ("í†µí•© ê·¸ë˜í”„" if lang == 'ko' else "Combined Chart"):
                    # í†µí•© êº¾ì€ì„  ê·¸ë˜í”„
                    fig = px.line(
                        combined_data,
                        x='ì—°ë„',
                        y='ì •ë‹µë¥ ',
                        color='í…ŒìŠ¤íŠ¸ëª…',
                        title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ì—°ë„ë³„ ì •ë‹µë¥  ì¶”ì´' if lang == 'ko' else 'Accuracy Trend by Year and Test Set',
                        markers=True,
                        category_orders={'í…ŒìŠ¤íŠ¸ëª…': test_names}
                    )
                    
                    # ì „ì²´ ë¼ì¸ì„ êµµê²Œ í‘œì‹œ
                    for trace in fig.data:
                        if trace.name == overall_name:
                            trace.line.width = 4
                            trace.line.dash = 'solid'
                            trace.marker.size = 12
                        else:
                            trace.line.width = 2
                            trace.marker.size = 8
                    
                    fig.update_traces(
                        marker=dict(line=dict(width=1, color='black'))
                    )
                    fig.update_layout(
                        height=550,
                        yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                        xaxis_title='ì¶œì œ ì—°ë„' if lang == 'ko' else 'Year',
                        yaxis=dict(range=[0, 100]),
                        legend_title='í…ŒìŠ¤íŠ¸ì…‹' if lang == 'ko' else 'Test Set',
                        legend=dict(
                            orientation="h",
                            yanchor="bottom",
                            y=-0.3,
                            xanchor="center",
                            x=0.5,
                            font=dict(size=annotation_size)
                        )
                    )
                    fig.update_xaxes(tickfont=dict(size=annotation_size))
                    fig.update_yaxes(tickfont=dict(size=annotation_size))
                    st.plotly_chart(fig, width='stretch')
                    
                else:
                    # ê°œë³„ ê·¸ë˜í”„ - ì „ì²´ + ê° í…ŒìŠ¤íŠ¸ì…‹ë³„
                    # ë¨¼ì € ì „ì²´ ê·¸ë˜í”„
                    st.markdown("#### " + ("ğŸ“Š ì „ì²´ ì—°ë„ë³„ ì •ë‹µë¥ " if lang == 'ko' else "ğŸ“Š Overall Accuracy by Year"))
                    overall_data = combined_data[combined_data['í…ŒìŠ¤íŠ¸ëª…'] == overall_name]
                    
                    if len(overall_data) > 0:
                        fig_overall = px.line(
                            overall_data,
                            x='ì—°ë„',
                            y='ì •ë‹µë¥ ',
                            title='ì „ì²´ ì—°ë„ë³„ ì •ë‹µë¥  ì¶”ì´' if lang == 'ko' else 'Overall Accuracy Trend by Year',
                            markers=True,
                            text='ì •ë‹µë¥ '
                        )
                        fig_overall.update_traces(
                            texttemplate='%{text:.1f}%',
                            textposition='top center',
                            textfont=dict(size=annotation_size),
                            marker=dict(size=12, line=dict(width=2, color='black')),
                            line=dict(width=4, color='#1f77b4')
                        )
                        fig_overall.update_layout(
                            height=400,
                            yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                            xaxis_title='ì¶œì œ ì—°ë„' if lang == 'ko' else 'Year',
                            yaxis=dict(range=[0, 100]),
                            showlegend=False
                        )
                        fig_overall.update_xaxes(tickfont=dict(size=annotation_size))
                        fig_overall.update_yaxes(tickfont=dict(size=annotation_size))
                        st.plotly_chart(fig_overall, width='stretch')
                    
                    st.markdown("---")
                    
                    # í…ŒìŠ¤íŠ¸ì…‹ë³„ ê°œë³„ ê·¸ë˜í”„
                    st.markdown("#### " + ("ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ë³„ ì—°ë„ë³„ ì •ë‹µë¥ " if lang == 'ko' else "ğŸ“Š Accuracy by Year per Test Set"))
                    
                    testset_only = [t for t in test_names if t != overall_name]
                    
                    # 2ì—´ ë ˆì´ì•„ì›ƒìœ¼ë¡œ í‘œì‹œ
                    cols = st.columns(2)
                    for idx, test_name in enumerate(testset_only):
                        test_data = combined_data[combined_data['í…ŒìŠ¤íŠ¸ëª…'] == test_name]
                        
                        if len(test_data) > 0:
                            with cols[idx % 2]:
                                fig_test = px.line(
                                    test_data,
                                    x='ì—°ë„',
                                    y='ì •ë‹µë¥ ',
                                    title=f'{test_name}',
                                    markers=True,
                                    text='ì •ë‹µë¥ '
                                )
                                fig_test.update_traces(
                                    texttemplate='%{text:.1f}%',
                                    textposition='top center',
                                    textfont=dict(size=annotation_size),
                                    marker=dict(size=10, line=dict(width=1.5, color='black')),
                                    line=dict(width=3)
                                )
                                fig_test.update_layout(
                                    height=350,
                                    yaxis_title='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                                    xaxis_title='ì¶œì œ ì—°ë„' if lang == 'ko' else 'Year',
                                    yaxis=dict(range=[0, 100]),
                                    showlegend=False,
                                    margin=dict(t=50, b=50)
                                )
                                fig_test.update_xaxes(tickfont=dict(size=annotation_size))
                                fig_test.update_yaxes(tickfont=dict(size=annotation_size))
                                st.plotly_chart(fig_test, width='stretch')
                
                # ë°ì´í„° í…Œì´ë¸” (ì ‘ì´ì‹)
                with st.expander("ğŸ“‹ " + ("ìƒì„¸ ë°ì´í„° ë³´ê¸°" if lang == 'ko' else "View Detailed Data")):
                    # í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜
                    pivot_data = combined_data.pivot(index='í…ŒìŠ¤íŠ¸ëª…', columns='ì—°ë„', values='ì •ë‹µë¥ ')
                    pivot_data = pivot_data.reindex(test_names)
                    
                    # í‰ê·  ì»¬ëŸ¼ ì¶”ê°€
                    pivot_data['í‰ê· ' if lang == 'ko' else 'Avg'] = pivot_data.mean(axis=1)
                    
                    # ì†Œìˆ˜ì  1ìë¦¬ë¡œ í¬ë§·íŒ…
                    st.dataframe(
                        pivot_data.style.format("{:.1f}%")
                        .background_gradient(cmap='RdYlGn', axis=None, vmin=0, vmax=100),
                        width='stretch'
                    )
                    
                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                    csv_data = pivot_data.reset_index().to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSV " + ("ë‹¤ìš´ë¡œë“œ" if lang == 'ko' else "Download"),
                        data=csv_data,
                        file_name="year_testset_accuracy.csv",
                        mime="text/csv"
                    )
            else:
                st.info("ìœ íš¨í•œ ì—°ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No valid year data available.")
        else:
            st.info("ì—°ë„(Year) ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "Year column not found in data.")
        
        st.markdown("---")
        
        # Figure 4: ì¶œì‹œ ì‹œê¸°-ì„±ëŠ¥ ì‚°ì ë„
        if table3 is not None and len(table3) > 0:
            st.subheader("ğŸ“… " + ("Figure 4: ì¶œì‹œ ì‹œê¸°-ì„±ëŠ¥ ì¶”ì´" if lang == 'ko' else "Figure 4: Release Date vs Performance"))
            
            # ë‚ ì§œë¥¼ ìˆ«ìë¡œ ë³€í™˜ (YYYY-MM -> YYYYMM)
            table3_plot = table3.copy()
            date_col = 'ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date'
            
            try:
                table3_plot['date_numeric'] = table3_plot[date_col].str.replace('-', '').astype(int)
            except:
                st.warning("ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì˜¤ë¥˜" if lang == 'ko' else "Date format conversion error")
                st.markdown("---")
                # Figure 8ë¡œ ê±´ë„ˆë›°ê¸°
            else:
                # ì¶”ì„¸ì„  ê·¸ë¦¬ê¸° ì‹œë„ (statsmodels í•„ìš”)
                try:
                    fig = px.scatter(
                        table3_plot,
                        x='date_numeric',
                        y='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                        text='ëª¨ë¸ëª…' if lang == 'ko' else 'Model',
                        title='ëª¨ë¸ ì¶œì‹œ ì‹œê¸°ì™€ ì„±ëŠ¥ ê´€ê³„ (ì¶”ì„¸ì„  í¬í•¨)' if lang == 'ko' else 'Model Release Date vs Performance (with Trendline)',
                        trendline='ols',
                        labels={'date_numeric': 'ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date'}
                    )
                    use_trendline = True
                except (ImportError, ModuleNotFoundError, Exception):
                    # statsmodelsê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¶”ì„¸ì„  ì—†ì´ ê·¸ë¦¬ê¸°
                    fig = px.scatter(
                        table3_plot,
                        x='date_numeric',
                        y='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                        text='ëª¨ë¸ëª…' if lang == 'ko' else 'Model',
                        title='ëª¨ë¸ ì¶œì‹œ ì‹œê¸°ì™€ ì„±ëŠ¥ ê´€ê³„' if lang == 'ko' else 'Model Release Date vs Performance',
                        labels={'date_numeric': 'ì¶œì‹œ ì‹œê¸°' if lang == 'ko' else 'Release Date'}
                    )
                    
                    # ìˆ˜ë™ìœ¼ë¡œ ê°„ë‹¨í•œ ì¶”ì„¸ì„  ì¶”ê°€
                    x_numeric = table3_plot['date_numeric'].values
                    y_values = table3_plot['í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)'].values
                    
                    # ì„ í˜• íšŒê·€ ê³„ì‚°
                    z = np.polyfit(x_numeric, y_values, 1)
                    p = np.poly1d(z)
                    
                    # ì¶”ì„¸ì„  ì¶”ê°€
                    fig.add_scatter(
                        x=x_numeric,
                        y=p(x_numeric),
                        mode='lines',
                        name='ì¶”ì„¸ì„ ' if lang == 'ko' else 'Trend',
                        line=dict(color='red', dash='dash')
                    )
                    use_trendline = False
                
                # Xì¶• ë ˆì´ë¸”ì„ ì›ë˜ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
                tickvals = sorted(table3_plot['date_numeric'].unique())
                ticktext = [f"{str(val)[:4]}-{str(val)[4:]}" for val in tickvals]
                
                fig.update_traces(textposition='top center', marker=dict(size=10, line=dict(width=2, color='black')), selector=dict(mode='markers'))
                fig.update_layout(
                    height=500,
                    xaxis=dict(
                        tickmode='array',
                        tickvals=tickvals,
                        ticktext=ticktext,
                        tickangle=45
                    ),
                    yaxis_title='í‰ê·  ì •ë‹µë¥  (%)' if lang == 'ko' else 'Avg Accuracy (%)',
                    yaxis=dict(range=[0, 100])
                )
                
                st.plotly_chart(fig, width='stretch')
                
                st.markdown("---")
        
        # Figure 8: ë‚œì´ë„ë³„ ë ˆì´ë” ì°¨íŠ¸
        if 'ì •ë‹µì—¬ë¶€' in filtered_df.columns:
            st.subheader("ğŸ¯ " + ("Figure 8: ë‚œì´ë„ êµ¬ê°„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸" if lang == 'ko' else "Figure 8: Model Performance Radar by Difficulty"))
            
            # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚°
            difficulty = filtered_df.groupby('Question')['ì •ë‹µì—¬ë¶€'].mean() * 100
            
            # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
            def classify_difficulty_simple(score):
                if score < 20:
                    return 'ë§¤ìš° ì–´ë ¤ì›€' if lang == 'ko' else 'Very Hard'
                elif score < 40:
                    return 'ì–´ë ¤ì›€' if lang == 'ko' else 'Hard'
                elif score < 60:
                    return 'ë³´í†µ' if lang == 'ko' else 'Medium'
                elif score < 80:
                    return 'ì‰¬ì›€' if lang == 'ko' else 'Easy'
                else:
                    return 'ë§¤ìš° ì‰¬ì›€' if lang == 'ko' else 'Very Easy'
            
            # ìƒìœ„ 5ê°œ ëª¨ë¸ ì„ íƒ
            top_models = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().nlargest(5).index.tolist()
            top_models_mask = filtered_df['ëª¨ë¸'].isin(top_models)
            
            # ë‚œì´ë„ ë ˆë²¨ ê³„ì‚° (í•„ìš”í•œ ë°ì´í„°ë§Œ)
            difficulty_levels = filtered_df.loc[top_models_mask, 'Question'].map(
                lambda q: classify_difficulty_simple(difficulty.get(q, 50))
            )
            
            # í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ë¶„ì„
            radar_df = pd.DataFrame({
                'ëª¨ë¸': filtered_df.loc[top_models_mask, 'ëª¨ë¸'],
                'difficulty_level': difficulty_levels,
                'ì •ë‹µì—¬ë¶€': filtered_df.loc[top_models_mask, 'ì •ë‹µì—¬ë¶€']
            })
            
            # ëª¨ë¸ë³„ ë‚œì´ë„ë³„ ì„±ëŠ¥
            radar_data = radar_df.groupby(['ëª¨ë¸', 'difficulty_level'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            radar_pivot = radar_data.unstack(fill_value=0)
            
            if len(radar_pivot) > 0 and len(radar_pivot.columns) > 0:
                # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
                fig = go.Figure()
                
                for model in radar_pivot.index:
                    fig.add_trace(go.Scatterpolar(
                        r=radar_pivot.loc[model].values,
                        theta=radar_pivot.columns,
                        fill='toself',
                        name=model
                    ))
                
                fig.update_layout(
                    polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
                    showlegend=True,
                    title='ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ' if lang == 'ko' else 'Model Performance by Difficulty',
                    height=600
                )
                
                st.plotly_chart(fig, width='stretch')
            else:
                st.info("ë ˆì´ë” ì°¨íŠ¸ë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤." if lang == 'ko' else "Insufficient data for radar chart.")
        
        st.markdown("---")
        
        # Figure 9: ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ì‚°ì ë„ (NEW!)
        if table9 is not None and len(table9) > 0:
            st.subheader("ğŸ’° " + ("Figure 9: ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ì‚°ì ë„" if lang == 'ko' else "Figure 9: Cost vs Performance Scatter"))
            
            fig = px.scatter(
                table9,
                x='ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ($)' if lang == 'ko' else 'Cost per 1K Correct ($)',
                y='ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)',
                text='ëª¨ë¸ëª…' if lang == 'ko' else 'Model',
                title='ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„ (ë¹„ìš© vs ì •í™•ë„)' if lang == 'ko' else 'Cost Efficiency Analysis (Cost vs Accuracy)',
                labels={
                    'ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ($)' if lang == 'ko' else 'Cost per 1K Correct ($)': 'ì •ë‹µ 1000ê°œë‹¹ ë¹„ìš© ($)' if lang == 'ko' else 'Cost per 1K Correct ($)',
                    'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)': 'ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)'
                }
            )
            
            fig.update_traces(
                textposition='top center',
                marker=dict(size=12, line=dict(width=2, color='black'))
            )
            fig.update_layout(height=500)
            st.plotly_chart(fig, width='stretch')
            
            st.info("ğŸ’¡ " + ("ì™¼ìª½ ìœ„(ë‚®ì€ ë¹„ìš© + ë†’ì€ ì •í™•ë„)ê°€ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤." if lang == 'ko' else "Top left (low cost + high accuracy) is most efficient."))
            
            st.markdown("---")
        
        # Figure 10: ì˜¤ë‹µ íŒ¨í„´ ì›í˜• ì°¨íŠ¸
        if table10 is not None and len(table10) > 0:
            st.subheader("ğŸ¥§ " + ("Figure 10: ì˜¤ë‹µ íŒ¨í„´ ë¹ˆë„ ì›í˜• ì°¨íŠ¸" if lang == 'ko' else "Figure 10: Error Pattern Distribution"))
            
            fig = px.pie(
                table10,
                values='ë¬¸í•­ ìˆ˜' if lang == 'ko' else 'Problem Count',
                names='ì˜¤ë‹µ íŒ¨í„´ ìœ í˜•' if lang == 'ko' else 'Error Pattern Type',
                title='ì˜¤ë‹µ íŒ¨í„´ë³„ ë¹„ìœ¨' if lang == 'ko' else 'Distribution of Error Patterns'
            )
            fig.update_traces(textposition='inside', textinfo='percent+label')
            fig.update_layout(height=500)
            st.plotly_chart(fig, width='stretch')
        
        # Figure 11: ëª¨ë¸ë³„ ì˜¤ë‹µ ì¼ì¹˜ë„ íˆíŠ¸ë§µ
        st.subheader("ğŸ”¥ " + ("Figure 11: ëª¨ë¸ë³„ ì˜¤ë‹µ ì¼ì¹˜ë„ íˆíŠ¸ë§µ" if lang == 'ko' else "Figure 11: Model Error Agreement Heatmap"))
        
        models_list = filtered_df['ëª¨ë¸'].unique()
        
        if len(models_list) >= 2:
            # ìµœì í™”: ëª¨ë¸ë³„ ì˜¤ë‹µ ì •ë³´ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
            model_wrong_dict = {}
            for model in models_list:
                model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
                # Questionë³„ ì •ë‹µì—¬ë¶€ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
                wrong_questions = set(model_df[~model_df['ì •ë‹µì—¬ë¶€']]['Question'].values)
                all_questions = set(model_df['Question'].values)
                model_wrong_dict[model] = {
                    'wrong': wrong_questions,
                    'all': all_questions
                }
            
            # ëª¨ë¸ ìŒë³„ ì˜¤ë‹µ ì¼ì¹˜ë„ ê³„ì‚° (ìµœì í™”ëœ ë²„ì „)
            agreement_matrix = []
            
            for model1 in models_list:
                row = []
                m1_data = model_wrong_dict[model1]
                
                for model2 in models_list:
                    if model1 == model2:
                        row.append(100.0)
                    else:
                        m2_data = model_wrong_dict[model2]
                        
                        # ê³µí†µ ë¬¸ì œ
                        common_questions = m1_data['all'] & m2_data['all']
                        
                        if len(common_questions) > 0:
                            # ë‘ ëª¨ë¸ì´ ëª¨ë‘ í‹€ë¦° ë¬¸ì œ (ê³µí†µ ë¬¸ì œ ì¤‘)
                            both_wrong = len(m1_data['wrong'] & m2_data['wrong'] & common_questions)
                            
                            # ê° ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ ìˆ˜ (ê³µí†µ ë¬¸ì œ ì¤‘)
                            model1_wrong_count = len(m1_data['wrong'] & common_questions)
                            model2_wrong_count = len(m2_data['wrong'] & common_questions)
                            
                            total_wrong = model1_wrong_count + model2_wrong_count - both_wrong
                            
                            if total_wrong > 0:
                                agreement = (both_wrong / total_wrong) * 100
                            else:
                                agreement = 0
                        else:
                            agreement = 0
                        
                        row.append(round(agreement, 1))
                
                agreement_matrix.append(row)
            
            # íˆíŠ¸ë§µ ìƒì„±
            fig = go.Figure(data=go.Heatmap(
                z=agreement_matrix,
                x=models_list,
                y=models_list,
                colorscale='Reds',
                text=agreement_matrix,
                texttemplate='%{text:.1f}',
                textfont={"size": int(10 * chart_text_size)},
                colorbar=dict(title="ì¼ì¹˜ë„ (%)" if lang == 'ko' else "Agreement (%)")
            ))
            
            fig.update_layout(
                title='ëª¨ë¸ ê°„ ì˜¤ë‹µ ì¼ì¹˜ë„' if lang == 'ko' else 'Error Agreement Between Models',
                height=600,
                xaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model',
                yaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model'
            )
            fig.update_xaxes(tickangle=45)
            
            st.plotly_chart(fig, width='stretch')
        else:
            st.info("ëª¨ë¸ì´ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤." if lang == 'ko' else "At least 2 models required.")
        
        st.markdown("---")
        
        # Figure 12 & 13: ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ì‹œê°í™” (NEW!)
        if table11 is not None and len(table11) > 0:
            st.subheader("ğŸ“Š " + ("Figure 12: SafetyQ&A vs ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ì‚°ì ë„ í–‰ë ¬" if lang == 'ko' else "Figure 12: Benchmark Scatter Plot Matrix"))
            
            # ë²¤ì¹˜ë§ˆí¬ ì»¬ëŸ¼ë§Œ ì„ íƒ
            benchmark_cols = ['SafetyQ&A', 'MMLU', 'GPQA', 'MMLU-Pro']
            model_col = 'ëª¨ë¸ëª…' if lang == 'ko' else 'Model'
            
            # ì‚°ì ë„ í–‰ë ¬ ìƒì„±
            from plotly.subplots import make_subplots
            
            n = len(benchmark_cols)
            fig = make_subplots(
                rows=n,
                cols=n,
                subplot_titles=[f"{b1} vs {b2}" for b1 in benchmark_cols for b2 in benchmark_cols],
                horizontal_spacing=0.05,
                vertical_spacing=0.05
            )
            
            for i, bench1 in enumerate(benchmark_cols, 1):
                for j, bench2 in enumerate(benchmark_cols, 1):
                    if i == j:
                        # ëŒ€ê°ì„ : íˆìŠ¤í† ê·¸ë¨
                        fig.add_trace(
                            go.Histogram(
                                x=table11[bench1],
                                name=bench1,
                                showlegend=False,
                                marker_color='lightblue'
                            ),
                            row=i,
                            col=j
                        )
                    else:
                        # ë¹„ëŒ€ê°ì„ : ì‚°ì ë„
                        fig.add_trace(
                            go.Scatter(
                                x=table11[bench2],
                                y=table11[bench1],
                                mode='markers',
                                name=f"{bench1} vs {bench2}",
                                showlegend=False,
                                marker=dict(size=8, color='blue', opacity=0.6),
                                text=table11[model_col],
                                hovertemplate=f"<b>%{{text}}</b><br>{bench2}: %{{x:.1f}}<br>{bench1}: %{{y:.1f}}<extra></extra>"
                            ),
                            row=i,
                            col=j
                        )
            
            fig.update_layout(
                title='ë²¤ì¹˜ë§ˆí¬ ê°„ ìƒê´€ê´€ê³„ í–‰ë ¬' if lang == 'ko' else 'Benchmark Correlation Matrix',
                height=800,
                showlegend=False
            )
            
            st.plotly_chart(fig, width='stretch')
            
            st.markdown("---")
            
            # Figure 13: ë²¤ì¹˜ë§ˆí¬ íˆíŠ¸ë§µ
            st.subheader("ğŸ”¥ " + ("Figure 13: ë²¤ì¹˜ë§ˆí¬ ìœ í˜•ë³„ ëª¨ë¸ ì„±ëŠ¥ í”„ë¡œíŒŒì¼" if lang == 'ko' else "Figure 13: Model Performance Profile by Benchmark"))
            
            # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
            heatmap_data = table11.set_index(model_col)[benchmark_cols]
            
            fig = go.Figure(data=go.Heatmap(
                z=heatmap_data.values,
                x=heatmap_data.columns,
                y=heatmap_data.index,
                colorscale='RdYlGn',
                text=np.round(heatmap_data.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": int(12 * chart_text_size)},
                colorbar=dict(title="ì ìˆ˜" if lang == 'ko' else "Score"),
                zmin=0,
                zmax=100
            ))
            
            fig.update_layout(
                title='ëª¨ë¸ë³„ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ íˆíŠ¸ë§µ' if lang == 'ko' else 'Model Performance Heatmap by Benchmark',
                height=600,
                xaxis_title='ë²¤ì¹˜ë§ˆí¬' if lang == 'ko' else 'Benchmark',
                yaxis_title='ëª¨ë¸' if lang == 'ko' else 'Model'
            )
            
            st.plotly_chart(fig, width='stretch')
            
            # ì¸ì‚¬ì´íŠ¸
            st.success("ğŸ’¡ " + ("SafetyQ&AëŠ” ì „ë¬¸ ì˜ì—­(ì•ˆì „/ë²•ë ¹) ë²¤ì¹˜ë§ˆí¬ë¡œ, ë²”ìš© ë²¤ì¹˜ë§ˆí¬(MMLU, GPQA)ì™€ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì…ë‹ˆë‹¤." if lang == 'ko' else "SafetyQ&A is a specialized benchmark (safety/law) showing different patterns from general benchmarks (MMLU, GPQA)."))
    
    st.sidebar.info(f"ğŸ“Š {t['current_data']}: {len(filtered_df):,}{t['problems']}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í›„)
    gc.collect()

if __name__ == "__main__":
    main()